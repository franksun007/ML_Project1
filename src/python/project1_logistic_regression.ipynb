{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Useful starting lines\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the training data into feature matrix, class labels, and event ids:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# load the training data\n",
    "from proj1_helpers import *\n",
    "DATA_TRAIN_PATH = '../../data/train.csv'\n",
    "y, tX, ids = load_csv_data(DATA_TRAIN_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Do your thing crazy machine learning thing here :) ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Constant to indicate +1 and 0 for classification\n",
    "BINARY_CLASSIFICATOIN_0 = -1\n",
    "BINARY_CLASSIFICATOIN_1 = 1\n",
    "\n",
    "\n",
    "def sigmoid(t):\n",
    "    \"\"\"apply sigmoid function on t.\"\"\"\n",
    "    return 1.0 / (1.0 + np.exp(-t))\n",
    "\n",
    "\n",
    "def calculate_loss_logistic_regression(y, tx, w):\n",
    "    \"\"\"compute the cost by negative log likelihood.\"\"\"\n",
    "    prediction = tx @ w\n",
    "    \n",
    "    y1 = np.where(y == BINARY_CLASSIFICATOIN_1)\n",
    "\n",
    "    # Prevent loss to be inf or nan, so that if the prediction is\n",
    "    # over 700, we keep the prediction as it is, instead of \n",
    "    # taking the exponent of it. \n",
    "    # As the result is only used to be an indication of the current\n",
    "    # function, this approximation is considered as appropriate\n",
    "    over_700 = np.where(prediction >= 700)\n",
    "\n",
    "    prediction_result = np.log(1 + np.exp(prediction))\n",
    "    prediction_result[over_700] = prediction[over_700]\n",
    "    # only -y when classification result is 1\n",
    "    prediction_result[y1] -= prediction[y1]\n",
    "    \n",
    "    result = np.sum(prediction_result)\n",
    "    return result\n",
    "\n",
    "\n",
    "def calculate_gradient_logistic_regression(y, tx, w):\n",
    "    \"\"\"compute the gradient of loss.\"\"\"\n",
    "    \n",
    "    y1 = np.where(y == BINARY_CLASSIFICATOIN_1)\n",
    "    sig = sigmoid(tx @ w).reshape(len(y))\n",
    "    # only -y when classification result is 1\n",
    "    sig[y1] -= y[y1]\n",
    "\n",
    "    return (tx.T @ sig).reshape((tx.shape[1], 1))\n",
    "    \n",
    "    \n",
    "def line_search_gamma(loss, loss_prev, gamma):\n",
    "    \"\"\"\n",
    "    A function that will adjust the step size naively\n",
    "    according to the previous loss function value and \n",
    "    the current loss function value\n",
    "    \"\"\"\n",
    "    if (loss > loss_prev):\n",
    "        gamma = gamma / 1.5\n",
    "    return gamma\n",
    "    \n",
    "\n",
    "def logistic_regression_helper(y, tx, gamma, max_iters, lambda_):\n",
    "    \"\"\"\n",
    "    Helper function that will perform the core logistic regression\n",
    "    algorithm with ** Gradient Descent **. \n",
    "    \"\"\"\n",
    "    w = np.zeros((tx.shape[1], 1))    # init guess for w\n",
    "    threshold = 1e-8    # Threshold for converge\n",
    "    loss_prev = 0       # the previous loss\n",
    "    \n",
    "    for iter in range(max_iters):\n",
    "        # lambda_ = 0 if performing pure logistic regression\n",
    "        loss = calculate_loss_logistic_regression(y, tx, w) + lambda_ * np.linalg.norm(w, 2)\n",
    "        gradient = calculate_gradient_logistic_regression(y, tx, w)\n",
    "\n",
    "        w -= gradient * gamma\n",
    "\n",
    "        # If converge\n",
    "        if (loss_prev != 0) and np.abs(loss_prev - loss) < threshold:\n",
    "            print(\"Reached Theshold, exit\")\n",
    "            break\n",
    "            \n",
    "        # Update gamma\n",
    "        gamma = line_search_gamma(loss, loss_prev, gamma)\n",
    "        loss_prev = loss\n",
    "        if (iter % 100) == 0:\n",
    "            print(\"Gamma: \", gamma)\n",
    "            print(\"Current iteration={i}, the loss={l}\".format(i=iter, l=loss))\n",
    "\n",
    "    return w\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "    Logistic Regression and Regularized Logisitic Regression\n",
    "    share the same core code. The only difference is that \n",
    "    for logistic regression, lambda_, the regularization term\n",
    "    is set to 0. \n",
    "\"\"\"\n",
    "def logistic_regression(y, tx, gamma, max_iters):\n",
    "    \"\"\" return the final w from the logistic regression \"\"\"\n",
    "    return logistic_regression_helper(y, tx, gamma, max_iters, lambda_=0)\n",
    "\n",
    "\n",
    "def reg_logistic_regression(y, tx, lambda_, gamma, max_iters):\n",
    "    \"\"\" return the final w from the penalized logistic regression, with lambda_ as a non 0 value\"\"\"\n",
    "    return logistic_regression_helper(y, tx, gamma, max_iters, lambda_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def performance(weights, y, xT):\n",
    "    \"\"\"Returns the percentage of successful classifications for the weights,\n",
    "    given the expected results (y) and data (xT)\"\"\"\n",
    "    from proj1_helpers import predict_labels\n",
    "    compare_pred = predict_labels(weights, xT)\n",
    "    compare_pred -= y.reshape((len(y), 1))\n",
    "        \n",
    "    non_zero = 0\n",
    "    for i in range(len(compare_pred)):\n",
    "        if compare_pred[i] != 0:\n",
    "            non_zero += 1\n",
    "            \n",
    "    return 1 - non_zero / compare_pred.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def standardize_0123_helper(x):\n",
    "    \"\"\"\n",
    "    Helper function that standardize the input data to mean 0 stddev 1. \n",
    "    The function replace all the -999 entries with the mean of all non -999\n",
    "    entries. \n",
    "    \"\"\"\n",
    "    for i in range(x.shape[1]):\n",
    "        mean = np.mean(x[np.where(x[:, i] != -999), i])\n",
    "        x[np.where(x[:, i] == -999), i] = mean \n",
    "        x[np.where(x[:, i] != -999), i] = x[np.where(x[:, i] != -999), i] - mean\n",
    "    \n",
    "    std_x = np.std(x, axis=0)\n",
    "    x[:, std_x > 0] = x[:, std_x > 0] / std_x[std_x > 0]\n",
    "    \n",
    "    return x\n",
    "\n",
    "\n",
    "def standardize_0(x):\n",
    "    \"\"\"\n",
    "    Standardize function for PRI_jet_num is 0\n",
    "    Return a standardize version of the original feature, with\n",
    "    uselessful thrown away\n",
    "    \"\"\"\n",
    "    # the features left that are meaningful and useful for training\n",
    "    feature_left = np.array([0, 1, 2, 3, 7, 8, 9, 10, 11, 13, 14, 15, 16, 17, 18, 19, 20, 21])\n",
    "    left_x = np.zeros((x.shape[0], len(feature_left)))\n",
    "    left_x[:, :] = x[:, feature_left]\n",
    "    return standardize_0123_helper(left_x)\n",
    "    \n",
    "\n",
    "def standardize_1(x):\n",
    "    \"\"\"\n",
    "    Standardize function for PRI_jet_num is 1\n",
    "    Return a standardize version of the original feature, with\n",
    "    uselessful thrown away\n",
    "    \"\"\"\n",
    "    # the features left that are meaningful and useful for training\n",
    "    feature_left = np.array([0, 1, 2, 3, 7, 8, 9, 10, 11, 13, 14, 15, 16, 17, 18, 19, 20, 21, 23, 24, 25, 29])\n",
    "    left_x = np.zeros((x.shape[0], len(feature_left)))\n",
    "    left_x[:, :] = x[:, feature_left]\n",
    "    return standardize_0123_helper(left_x)\n",
    "    \n",
    "    \n",
    "def standardize_23(x):\n",
    "    \"\"\"\n",
    "    Standardize function for PRI_jet_num is 2 or 3\n",
    "    Return a standardize version of the original feature, with\n",
    "    uselessful thrown away\n",
    "    \"\"\"\n",
    "    # the features left that are meaningful and useful for training\n",
    "    feature_left = np.delete(np.arange(30), 22)\n",
    "    left_x = np.zeros((x.shape[0], len(feature_left)))\n",
    "    left_x[:, :] = x[:, feature_left]\n",
    "    return standardize_0123_helper(left_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# The column index for PRI_jet_num\n",
    "jet_num_col = 22\n",
    "\n",
    "def split_dataset_wrt22(x):\n",
    "    \"\"\"\n",
    "    Return three tuples of indices that splits x with respect to\n",
    "    feature 22 - PRI_jet_num.\n",
    "    First  Tuple of indicies: index in x where PRI_jet_num is 0\n",
    "    Second Tuple of indicies: index in x where PRI_jet_num is 1\n",
    "    Third  Tuple of indicies: index in x where PRI_jet_num is 2 or 3\n",
    "    \"\"\"\n",
    "    x_22_0 = np.where(x[:, jet_num_col] == 0)\n",
    "    x_22_1 = np.where(x[:, jet_num_col] == 1)\n",
    "    x_22_23 = np.where(x[:, jet_num_col] >= 2)\n",
    "    return x_22_0, x_22_1, x_22_23"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_poly(x, degree):\n",
    "    \"\"\"\n",
    "    Build the polynomial rising to the pass in parameter degree. \n",
    "    Return a matrix that has the same entry as pass in x, while \n",
    "    more features added accroding to degree. \n",
    "    Each individual feature is a some power of the original feature.\n",
    "    \"\"\"\n",
    "    matrix = np.zeros((x.shape[0], x.shape[1] * (degree + 1)))\n",
    "    for i in range(degree + 1):\n",
    "        matrix[:, (i * x.shape[1]) : ((i + 1) * x.shape[1])] = (x ** i)[:]\n",
    "        \n",
    "    return matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def add_feature_helper(x, op, ori_shape):\n",
    "    \"\"\"\n",
    "    Helper function that takes in x, an operator op, and the\n",
    "    original shape of x. \n",
    "    Return a matrix that is expanded with the feature added.\n",
    "    The matrix will have the same entries as x, but additional\n",
    "    ori_shape columns of feature added. \n",
    "    \"\"\"\n",
    "    matrix = np.zeros((x.shape[0], x.shape[1] + ori_shape))\n",
    "    matrix[:, : x.shape[1]] = x[:, :]\n",
    "    matrix[:, x.shape[1] : ] = op(x[:, : ori_shape])\n",
    "    return matrix\n",
    "\n",
    "\n",
    "def add_feature(x):\n",
    "    \"\"\"\n",
    "    Add some features that we consider as useful and meaningful\n",
    "    to the data and good for training. \n",
    "    Return a modified x with features added. \n",
    "    \"\"\"\n",
    "    original_d = x.shape[1]\n",
    "    x = add_feature_helper(x, np.sin, original_d)\n",
    "    x = add_feature_helper(x, np.tanh, original_d)\n",
    "#     x = add_feature_helper(x, np.sin, original_d)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "max_iter = 40000\n",
    "lambdas = np.array([0.1])\n",
    "gammas = np.array([0.005])\n",
    "# polynomial degree\n",
    "degree = 2\n",
    "\n",
    "# split the data\n",
    "i_0, i_1, i_23 = split_dataset_wrt22(tX)\n",
    "tx_0 =  tX[i_0]\n",
    "y_0 =   y[i_0]\n",
    "tx_1 =  tX[i_1]\n",
    "y_1 =   y[i_1]\n",
    "tx_23 = tX[i_23]\n",
    "y_23 =  y[i_23]\n",
    "\n",
    "# Standardize the data\n",
    "std_tx_0 = standardize_0(tx_0)\n",
    "std_tx_1 = standardize_1(tx_1)\n",
    "std_tx_23 = standardize_23(tx_23)\n",
    "\n",
    "# Add the feature\n",
    "# std_tx_0 = add_feature(std_tx_0)\n",
    "# std_tx_1 = add_feature(std_tx_1)\n",
    "# std_tx_23 = add_feature(std_tx_23)\n",
    "\n",
    "# Build the polynomial \n",
    "matrix_std_tx_0 = build_poly(std_tx_0, degree)\n",
    "matrix_std_tx_1 = build_poly(std_tx_1, degree)\n",
    "matrix_std_tx_23 = build_poly(std_tx_23, degree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Perform Regularized logistic regression dataset where PRI_jet_num is 0\n",
    "weights_0 = reg_logistic_regression(y_0, matrix_std_tx_0, lambdas[0], gammas[0], max_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Perform Regularized logistic regression dataset where PRI_jet_num is 1\n",
    "weights_1 = reg_logistic_regression(y_1, matrix_std_tx_1, lambdas[0], gammas[0], max_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Perform Regularized logistic regression dataset where PRI_jet_num is 2 or 3\n",
    "weights_23 = reg_logistic_regression(y_23, matrix_std_tx_23, lambdas[0], gammas[0], max_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# invoke the performance function to get a rough estimate on how well we are doing \n",
    "# on the data that we just trained. \n",
    "# \n",
    "# We suppose to use cross-validation for this step. \n",
    "# However, due to the characteristics of the data, we think that evaluate on the original\n",
    "# training dataset will give us a reference on how well we are doing\n",
    "# This step is only an indication on whether we did anything REALLY wrong or not.\n",
    "print(\"0  Size: \", len(y_0), \"\\tPerformance: \", performance(weights_0, y_0, matrix_std_tx_0))\n",
    "print(\"1  Size: \", len(y_1), \"\\tPerformance: \", performance(weights_1, y_1, matrix_std_tx_1))\n",
    "print(\"23 Size: \", len(y_23), \"\\tPerformance: \", performance(weights_23, y_23, matrix_std_tx_23))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate predictions and save ouput in csv format for submission:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# load test data\n",
    "DATA_TEST_PATH = '../../data/test.csv' \n",
    "_, tX_test, ids_test = load_csv_data(DATA_TEST_PATH)\n",
    "i_0_test, i_1_test, i_23_test = split_dataset_wrt22(tX_test)\n",
    "\n",
    "# split tx into 3 set \n",
    "tx_0_test = tX_test[i_0_test]\n",
    "tx_1_test = tX_test[i_1_test]\n",
    "tx_23_test = tX_test[i_23_test]\n",
    "\n",
    "# standardize\n",
    "std_tx_0_test = standardize_0(tx_0_test)\n",
    "std_tx_1_test = standardize_1(tx_1_test)\n",
    "std_tx_23_test = standardize_23(tx_23_test)\n",
    "\n",
    "# add feature\n",
    "# std_tx_0_test = add_feature(std_tx_0_test)\n",
    "# std_tx_1_test = add_feature(std_tx_1_test)\n",
    "# std_tx_23_test = add_feature(std_tx_23_test)\n",
    "\n",
    "# split index into 3 features\n",
    "ids_0_test = ids_test[i_0_test]\n",
    "ids_1_test = ids_test[i_1_test]\n",
    "ids_23_test = ids_test[i_23_test]\n",
    "\n",
    "# Make prediction\n",
    "y_pred_0 = predict_labels(weights_0, build_poly(std_tx_0_test, degree))\n",
    "y_pred_1 = predict_labels(weights_1, build_poly(std_tx_1_test, degree))\n",
    "y_pred_23 = predict_labels(weights_23, build_poly(std_tx_23_test, degree))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# concatenate everything into one\n",
    "y_pred = np.concatenate((y_pred_0, y_pred_1, y_pred_23), axis=0)\n",
    "ids_test = np.concatenate((ids_0_test, ids_1_test, ids_23_test), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# output to file\n",
    "OUTPUT_PATH = '../../data/output.csv' # TODO: fill in desired name of output file for submission\n",
    "create_csv_submission(ids_test, y_pred, OUTPUT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    ""
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3.0
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}