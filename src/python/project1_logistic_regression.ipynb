{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# Useful starting lines\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the training data into feature matrix, class labels, and event ids:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from proj1_helpers import *\n",
    "DATA_TRAIN_PATH = '../../data/train.csv'\n",
    "y, tX, ids = load_csv_data(DATA_TRAIN_PATH)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Do your thing crazy machine learning thing here :) ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def build_poly(x, degree):\n",
    "    \n",
    "    matrix = np.zeros((x.shape[0], x.shape[1] * (degree + 1)))\n",
    "    \n",
    "    for i in range(degree + 1):\n",
    "        matrix[:, (i * x.shape[1]) : ((i + 1) * x.shape[1])] = (x ** i)[:]\n",
    "    return matrix\n",
    "\n",
    "\n",
    "def build_k_indices(y, k_fold, seed=62):\n",
    "    \"\"\"build k indices for k-fold.\"\"\"\n",
    "    num_row = y.shape[0]\n",
    "    interval = int(num_row / k_fold)\n",
    "    np.random.seed(seed)\n",
    "    indices = np.random.permutation(num_row)\n",
    "    k_indices = [indices[k * interval: (k + 1) * interval]\n",
    "                 for k in range(k_fold)]\n",
    "    return np.array(k_indices)\n",
    "\n",
    "def batch_iter(y, tx, batch_size, num_batches=None, shuffle=True):\n",
    "    \"\"\"\n",
    "    Generate a minibatch iterator for a dataset.\n",
    "    Takes as input two iterables (here the output desired values 'y' and the input data 'tx')\n",
    "    Outputs an iterator which gives mini-batches of `batch_size` matching elements from `y` and `tx`.\n",
    "    Data can be randomly shuffled to avoid ordering in the original data messing with the randomness of the minibatches.\n",
    "    Example of use :\n",
    "    for minibatch_y, minibatch_tx in batch_iter(y, tx, 32):\n",
    "        <DO-SOMETHING>\n",
    "    \"\"\"\n",
    "\n",
    "    data_size = len(y)\n",
    "    num_batches_max = int(np.ceil(data_size/batch_size))\n",
    "    if num_batches is None:\n",
    "        num_batches = num_batches_max\n",
    "    else:\n",
    "        num_batches = min(num_batches, num_batches_max)\n",
    "\n",
    "    if shuffle:\n",
    "        shuffle_indices = np.random.permutation(np.arange(data_size))\n",
    "        shuffled_y = y[shuffle_indices]\n",
    "        shuffled_tx = tx[shuffle_indices]\n",
    "    else:\n",
    "        shuffled_y = y\n",
    "        shuffled_tx = tx\n",
    "    for batch_num in range(num_batches):\n",
    "        start_index = batch_num * batch_size\n",
    "        end_index = min((batch_num + 1) * batch_size, data_size)\n",
    "        if start_index != end_index:\n",
    "            yield shuffled_y[start_index:end_index], shuffled_tx[start_index:end_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "BINARY_CLASSIFICATOIN_0 = -1\n",
    "BINARY_CLASSIFICATOIN_1 = 1\n",
    "\n",
    "\n",
    "def sigmoid(t):\n",
    "    \"\"\"apply sigmoid function on t.\"\"\"\n",
    "    return 1.0 / (1.0 + np.exp(-t))\n",
    "\n",
    "\n",
    "def calculate_loss_logistic_regression(y, tx, w):\n",
    "    \"\"\"compute the cost by negative log likelihood.\"\"\"\n",
    "    prediction = tx @ w\n",
    "    \n",
    "    y1 = np.where(y == BINARY_CLASSIFICATOIN_1)\n",
    "\n",
    "    over_700 = np.where(prediction >= 700)\n",
    "\n",
    "    prediction_result = np.log(1 + np.exp(prediction))\n",
    "    prediction_result[over_700] = prediction[over_700]\n",
    "    prediction_result[y1] -= prediction[y1]\n",
    "    \n",
    "    result = np.sum(prediction_result)\n",
    "    return result\n",
    "\n",
    "\n",
    "def calculate_gradient_logistic_regression(y, tx, w):\n",
    "    \"\"\"compute the gradient of loss.\"\"\"\n",
    "\n",
    "    y1 = np.where(y == BINARY_CLASSIFICATOIN_1)\n",
    "    sig = sigmoid(tx @ w).reshape(len(y))\n",
    "    sig[y1] -= y[y1]\n",
    "\n",
    "    return (tx.T @ sig).reshape((tx.shape[1], 1))\n",
    "    \n",
    "    \n",
    "def line_search_gamma(y, tx, w, loss, loss_prev, loss_prev_prev, gamma, gradient, gamma_counter, iter):\n",
    "    if (loss > loss_prev * (1 + 1.0 / np.log(loss))):\n",
    "        gamma = gamma / 1.5 if np.random.randint(2) == 1 else gamma\n",
    "    elif (loss_prev_prev - loss_prev > loss_prev - loss) or gamma_counter >= 250:\n",
    "        gamma = gamma * (1 + 1.0 / loss)\n",
    "    return gamma\n",
    "    \n",
    "\n",
    "def logistic_regression_helper(y, tx, gamma, max_iters, lambda_):\n",
    "\n",
    "    w = np.zeros((tx.shape[1], 1))\n",
    "    threshold = 1e-8\n",
    "    loss_prev_prev = 0\n",
    "    loss_prev = 0\n",
    "    w_max = w\n",
    "    perf = 0\n",
    "    i = 0\n",
    "    gamma_prev = 0\n",
    "    gamma_counter = 0\n",
    "    \n",
    "    batch_size = 100\n",
    "\n",
    "    for iter in range(max_iters):\n",
    "                \n",
    "        loss = calculate_loss_logistic_regression(y, tx, w) + lambda_ * np.linalg.norm(w, 2)\n",
    "        gradient = calculate_gradient_logistic_regression(y, tx, w)\n",
    "        w = w - gradient * gamma\n",
    "\n",
    "        if (loss_prev != 0 and loss_prev_prev != 0) and np.abs(loss_prev - loss) < threshold:\n",
    "            print(\"Reached Theshold, exit\")\n",
    "            break\n",
    "            \n",
    "        gamma_prev = gamma\n",
    "    \n",
    "        gamma = line_search_gamma(y, tx, w, (loss - lambda_ * np.linalg.norm(w, 2)), loss_prev, loss_prev_prev, gamma, gradient, gamma_counter, iter)\n",
    "        \n",
    "        if gamma == gamma_prev:\n",
    "            gamma_counter += 1\n",
    "        else:\n",
    "            gamma_counter = 0\n",
    "        \n",
    "        loss_prev_prev = loss_prev\n",
    "        loss_prev = loss\n",
    "\n",
    "        if (iter % 100) == 0:\n",
    "            print(\"Gamma: \", gamma)\n",
    "            print(\"Current iteration={i}, the loss={l}\".format(i=iter, l=loss))\n",
    "\n",
    "    return w\n",
    "\n",
    "\n",
    "def logistic_regression(y, tx, gamma, max_iters):\n",
    "    \"\"\" return the final w from the logistic regression \"\"\"\n",
    "    return logistic_regression_helper(y, tx, gamma, max_iters, lambda_=0)\n",
    "\n",
    "\n",
    "def reg_logistic_regression(y, tx, lambda_, gamma, max_iters):\n",
    "    \"\"\" return the final w from the penalized logistic regression, with lambda_ as a non 0 value\"\"\"\n",
    "    return logistic_regression_helper(y, tx, gamma, max_iters, lambda_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def performance(weights, y, xT):\n",
    "    \"\"\"Returns the percentage of successful classifications for the weights,\n",
    "    given the expected results (y) and data (xT)\"\"\"\n",
    "    from proj1_helpers import predict_labels\n",
    "    compare_pred = predict_labels(weights, xT)\n",
    "    compare_pred -= y.reshape((len(y), 1))\n",
    "        \n",
    "    non_zero = 0\n",
    "    for i in range(len(compare_pred)):\n",
    "        if compare_pred[i] != 0:\n",
    "            non_zero += 1\n",
    "            \n",
    "    return 1 - non_zero / compare_pred.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "def plot_feature_relationship(y, tX):\n",
    "\n",
    "    std_tx = standardize(tX)\n",
    "\n",
    "    row = np.zeros(std_tx.shape[0])\n",
    "\n",
    "    # for i in range(len(tX)):\n",
    "    #     row[i] = tX[i][0]\n",
    "\n",
    "    plt.figure(figsize=(20, 20))\n",
    "\n",
    "    for j in range(std_tx.shape[1]):\n",
    "        for i in range(len(std_tx)):\n",
    "            row[i] = std_tx[i][j]\n",
    "        plt.subplot(5, 6, j + 1)\n",
    "        plt.title(j)\n",
    "        plt.plot(row[np.where(y == 1)], y[np.where(y == 1)], 'ro')\n",
    "        plt.plot(row[np.where(y == -1)], y[np.where(y == -1)], 'bo')\n",
    "  \n",
    "\n",
    "    plt.tight_layout()\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "# plot_feature_relationship(y, tX)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "# def standardize(x):\n",
    "# #     cols = [4, 5, 6, 12, 26, 28]\n",
    "# #     for i in range(len(x)):\n",
    "# #         x[i][np.where(x[i] == -999)] = 0\n",
    "\n",
    "#     # Combine feature, or do poly expansion\n",
    "    \n",
    "#     # Do not normalize the jet num\n",
    "#     jet_num = x[:, jet_num_col]\n",
    "    \n",
    "#     # Replace -999 with some value that is the mean/median of the represent dataset \n",
    "#     for i in range(x.shape[1]):\n",
    "#         median = np.median(x[np.where(x[:, i] != -999), i])\n",
    "#         x[np.where(x[:, i] == -999), i] = median \n",
    "#         x[np.where(x[:, i] != -999), i] = x[np.where(x[:, i] != -999), i] - np.mean(x[np.where(x[:, i] != -999), i])\n",
    "    \n",
    "#     mean_x = np.mean(x, axis=0)\n",
    "#     x = x - mean_x\n",
    "    \n",
    "#     std_x = np.std(x, axis=0)\n",
    "#     x[:, std_x > 0] = x[:, std_x > 0] / std_x[std_x > 0]\n",
    "\n",
    "#     x[:, jet_num_col] = jet_num\n",
    "    \n",
    "#     return x\n",
    "\n",
    "# print(standardize(tX) - tX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# def standardize_01(x):\n",
    "# #     cols = [4, 5, 6, 12, 26, 28]\n",
    "# #     for i in range(len(x)):\n",
    "# #         x[i][np.where(x[i] == -999)] = 0\n",
    "\n",
    "#     # Combine feature, or do poly expansion\n",
    "    \n",
    "#     # Do not normalize the jet num\n",
    "    \n",
    "#     # Replace -999 with some value that is the mean/median of the represent dataset \n",
    "#     for i in range(x.shape[1]):\n",
    "#         x[np.where(x[:, i] == -999), i] = 1\n",
    "    \n",
    "#     mean_x = np.mean(x, axis=0)\n",
    "#     x = x - mean_x\n",
    "    \n",
    "#     std_x = np.std(x, axis=0)\n",
    "#     x[:, std_x > 0] = x[:, std_x > 0] / std_x[std_x > 0]\n",
    "\n",
    "#     x[:, jet_num_col] = 1\n",
    "    \n",
    "#     return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def standardize_0123_helper(x):\n",
    "    for i in range(x.shape[1]):\n",
    "        mean = np.mean(x[np.where(x[:, i] != -999), i])\n",
    "        x[np.where(x[:, i] == -999), i] = mean \n",
    "        x[np.where(x[:, i] != -999), i] = x[np.where(x[:, i] != -999), i] - mean\n",
    "    \n",
    "    std_x = np.std(x, axis=0)\n",
    "    x[:, std_x > 0] = x[:, std_x > 0] / std_x[std_x > 0]\n",
    "    \n",
    "    return x\n",
    "\n",
    "\n",
    "def standardize_0(x):\n",
    "    feature_left = np.array([0, 1, 2, 3, 7, 8, 9, 10, 11, 13, 14, 15, 16, 17, 18, 19, 20, 21])\n",
    "    left_x = np.zeros((x.shape[0], len(feature_left)))\n",
    "    left_x[:, :] = x[:, feature_left]\n",
    "    return standardize_0123_helper(left_x)\n",
    "    \n",
    "    \n",
    "\n",
    "def standardize_1(x):\n",
    "    feature_left = np.array([0, 1, 2, 3, 7, 8, 9, 10, 11, 13, 14, 15, 16, 17, 18, 19, 20, 21, 23, 24, 25, 29])\n",
    "    left_x = np.zeros((x.shape[0], len(feature_left)))\n",
    "    left_x[:, :] = x[:, feature_left]\n",
    "    return standardize_0123_helper(left_x)\n",
    "    \n",
    "    \n",
    "def standardize_23(x):\n",
    "    feature_left = np.delete(np.arange(30), 22)\n",
    "    left_x = np.zeros((x.shape[0], len(feature_left)))\n",
    "    left_x[:, :] = x[:, feature_left]\n",
    "    return standardize_0123_helper(left_x)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "jet_num_col = 22\n",
    "\n",
    "\n",
    "def split_dataset_wrt22(x):\n",
    "    x_22_0 = np.where(x[:, jet_num_col] == 0)\n",
    "    x_22_1 = np.where(x[:, jet_num_col] == 1)\n",
    "    x_22_23 = np.where(x[:, jet_num_col] >= 2)\n",
    "    return x_22_0, x_22_1, x_22_23\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# There are two parameters, lambda_ and gamma, where gamma is the step size \n",
    "\n",
    "# max_iter = 15000\n",
    "# lambdas = np.arange(0.1, 0.4, 0.1)\n",
    "# gammas = np.array([0.004])\n",
    "\n",
    "# degree = 3\n",
    "\n",
    "# i_0, i_1, i_23 = split_dataset_wrt22(tX)\n",
    "\n",
    "# tx_0 =  tX[i_0]\n",
    "# y_0 =   y[i_0]\n",
    "# tx_1 =  tX[i_1]\n",
    "# y_1 =   y[i_1]\n",
    "# tx_23 = tX[i_23]\n",
    "# y_23 =  y[i_23]\n",
    "\n",
    "# std_tx_0 = standardize_01(tx_0)\n",
    "# std_tx_1 = standardize_01(tx_1)\n",
    "# std_tx_23 = standardize(tx_23)\n",
    "\n",
    "# matrix_std_tx_0 = build_poly(std_tx_0, degree)\n",
    "# matrix_std_tx_1 = build_poly(std_tx_1, degree)\n",
    "# matrix_std_tx_23 = build_poly(std_tx_23, degree)\n",
    "\n",
    "\n",
    "# print(std_tx)\n",
    "# print(tX)\n",
    "\n",
    "# k_fold = 10\n",
    "# k_indices = build_k_indices(y, k_fold)\n",
    "    \n",
    "# k = 2\n",
    "\n",
    "# y_test = y[k_indices[k]]\n",
    "# x_test = std_tx[k_indices[k]]\n",
    "# train_indices = k_indices[~(np.arange(k_indices.shape[0]) == k)]\n",
    "# y_train = np.concatenate(y[train_indices])\n",
    "# x_train = np.concatenate(std_tx[train_indices])\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "max_iter = 40000\n",
    "lambdas = np.arange(0.1, 0.4, 0.1)\n",
    "gammas = np.array([0.005])\n",
    "\n",
    "degree = 2\n",
    "\n",
    "i_0, i_1, i_23 = split_dataset_wrt22(tX)\n",
    "\n",
    "tx_0 =  tX[i_0]\n",
    "y_0 =   y[i_0]\n",
    "tx_1 =  tX[i_1]\n",
    "y_1 =   y[i_1]\n",
    "tx_23 = tX[i_23]\n",
    "y_23 =  y[i_23]\n",
    "\n",
    "std_tx_0 = standardize_0(tx_0)\n",
    "std_tx_1 = standardize_1(tx_1)\n",
    "std_tx_23 = standardize_23(tx_23)\n",
    "\n",
    "matrix_std_tx_0 = build_poly(std_tx_0, degree)\n",
    "matrix_std_tx_1 = build_poly(std_tx_1, degree)\n",
    "matrix_std_tx_23 = build_poly(std_tx_23, degree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# weights_0 = reg_logistic_regression(y_0, matrix_std_tx_0, lambdas[0], gammas[0], max_iter)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gamma:  0.00333333333333\n",
      "Current iteration=0, the loss=53749.4049693404\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chenfs/anaconda/lib/python3.5/site-packages/ipykernel/__main__.py:7: RuntimeWarning: overflow encountered in exp\n",
      "/Users/chenfs/anaconda/lib/python3.5/site-packages/ipykernel/__main__.py:18: RuntimeWarning: overflow encountered in exp\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gamma:  0.000195097861399\n",
      "Current iteration=100, the loss=1359218.7276774226\n",
      "Gamma:  8.67149620046e-05\n",
      "Current iteration=200, the loss=662801.644624778\n",
      "Gamma:  5.78156841351e-05\n",
      "Current iteration=300, the loss=527609.7674706967\n",
      "Gamma:  2.57011484078e-05\n",
      "Current iteration=400, the loss=193036.88032858213\n",
      "Gamma:  1.7140793166e-05\n",
      "Current iteration=500, the loss=137056.07312432767\n",
      "Gamma:  1.14319590774e-05\n",
      "Current iteration=600, the loss=99399.35791908401\n",
      "Gamma:  7.62571829482e-06\n",
      "Current iteration=700, the loss=73931.20458814682\n",
      "Gamma:  7.63095519202e-06\n",
      "Current iteration=800, the loss=73408.39816457803\n",
      "Gamma:  7.63608992692e-06\n",
      "Current iteration=900, the loss=75117.59709950353\n",
      "Gamma:  5.09507649757e-06\n",
      "Current iteration=1000, the loss=55504.41763473445\n",
      "Gamma:  5.0997100108e-06\n",
      "Current iteration=1100, the loss=54644.04935962662\n",
      "Gamma:  5.10440046093e-06\n",
      "Current iteration=1200, the loss=54085.156074195205\n",
      "Gamma:  5.10914351047e-06\n",
      "Current iteration=1300, the loss=53569.30637312257\n",
      "Gamma:  5.11393803052e-06\n",
      "Current iteration=1400, the loss=53086.74891270409\n",
      "Gamma:  5.11877459195e-06\n",
      "Current iteration=1500, the loss=52713.84260737818\n",
      "Gamma:  5.12364625299e-06\n",
      "Current iteration=1600, the loss=52423.93171849004\n",
      "Gamma:  5.12854826459e-06\n",
      "Current iteration=1700, the loss=52164.00235315026\n",
      "Gamma:  5.13347686175e-06\n",
      "Current iteration=1800, the loss=51956.967598507756\n",
      "Gamma:  5.13842747649e-06\n",
      "Current iteration=1900, the loss=51803.12616845688\n",
      "Gamma:  5.14339425046e-06\n",
      "Current iteration=2000, the loss=51711.566759130714\n",
      "Gamma:  5.14837374095e-06\n",
      "Current iteration=2100, the loss=51636.52931356353\n",
      "Gamma:  5.15336424512e-06\n",
      "Current iteration=2200, the loss=51144.74556101746\n",
      "Gamma:  3.43938653261e-06\n",
      "Current iteration=2300, the loss=42997.644416691255\n",
      "Gamma:  3.44339716358e-06\n",
      "Current iteration=2400, the loss=42787.01742852122\n",
      "Gamma:  3.44743613862e-06\n",
      "Current iteration=2500, the loss=42526.452417598346\n",
      "Gamma:  3.45150388901e-06\n",
      "Current iteration=2600, the loss=42281.65470024373\n",
      "Gamma:  3.45559983904e-06\n",
      "Current iteration=2700, the loss=42041.72518109606\n",
      "Gamma:  3.45972369179e-06\n",
      "Current iteration=2800, the loss=41811.773336185164\n",
      "Gamma:  3.4638744932e-06\n",
      "Current iteration=2900, the loss=41597.359248818095\n",
      "Gamma:  3.46805079267e-06\n",
      "Current iteration=3000, the loss=41402.32477006754\n",
      "Gamma:  3.47225073373e-06\n",
      "Current iteration=3100, the loss=41230.09479771096\n",
      "Gamma:  3.47647196302e-06\n",
      "Current iteration=3200, the loss=41085.23455849595\n",
      "Gamma:  3.48071196576e-06\n",
      "Current iteration=3300, the loss=40962.649142023154\n",
      "Gamma:  3.48496932738e-06\n",
      "Current iteration=3400, the loss=40848.68737221415\n",
      "Gamma:  3.48924352064e-06\n",
      "Current iteration=3500, the loss=40740.492485911236\n",
      "Gamma:  3.49353389364e-06\n",
      "Current iteration=3600, the loss=40641.08644585225\n",
      "Gamma:  3.49783949003e-06\n",
      "Current iteration=3700, the loss=40552.49154258542\n",
      "Gamma:  3.50215922053e-06\n",
      "Current iteration=3800, the loss=40474.890774170795\n",
      "Gamma:  3.50649203832e-06\n",
      "Current iteration=3900, the loss=40407.09711507232\n",
      "Gamma:  3.51083703626e-06\n",
      "Current iteration=4000, the loss=40347.60507857837\n",
      "Gamma:  3.51519340089e-06\n",
      "Current iteration=4100, the loss=40296.394612471755\n",
      "Gamma:  3.51956003972e-06\n",
      "Current iteration=4200, the loss=40258.10240075643\n",
      "Gamma:  3.52393567941e-06\n",
      "Current iteration=4300, the loss=40229.11944205676\n",
      "Gamma:  3.52831957912e-06\n",
      "Current iteration=4400, the loss=40206.18135853892\n",
      "Gamma:  3.53271108968e-06\n",
      "Current iteration=4500, the loss=40189.51458114795\n",
      "Gamma:  3.53710953768e-06\n",
      "Current iteration=4600, the loss=40179.08420316015\n",
      "Gamma:  3.54151428265e-06\n",
      "Current iteration=4700, the loss=40174.25967297974\n",
      "Gamma:  3.5459247744e-06\n",
      "Current iteration=4800, the loss=40174.014610845246\n",
      "Gamma:  3.55034056893e-06\n",
      "Current iteration=4900, the loss=40177.59866299189\n",
      "Gamma:  3.55476118111e-06\n",
      "Current iteration=5000, the loss=40186.90180739383\n",
      "Gamma:  3.55918582335e-06\n",
      "Current iteration=5100, the loss=40203.86953238326\n",
      "Gamma:  3.56361384608e-06\n",
      "Current iteration=5200, the loss=40224.78000481898\n",
      "Gamma:  3.56804496701e-06\n",
      "Current iteration=5300, the loss=40247.2935559029\n",
      "Gamma:  3.57247909334e-06\n",
      "Current iteration=5400, the loss=40269.927895061264\n",
      "Gamma:  3.57691628582e-06\n",
      "Current iteration=5500, the loss=40291.52212799407\n",
      "Gamma:  3.58135665834e-06\n",
      "Current iteration=5600, the loss=40312.366542609874\n",
      "Gamma:  3.58580023058e-06\n",
      "Current iteration=5700, the loss=40333.575047033846\n",
      "Gamma:  3.59024692532e-06\n",
      "Current iteration=5800, the loss=40355.70014734614\n",
      "Gamma:  3.59469663691e-06\n",
      "Current iteration=5900, the loss=40378.78867058975\n",
      "Gamma:  3.5991492598e-06\n",
      "Current iteration=6000, the loss=40402.9032730141\n",
      "Gamma:  3.60360462541e-06\n",
      "Current iteration=6100, the loss=40429.28801809258\n",
      "Gamma:  3.60806243472e-06\n",
      "Current iteration=6200, the loss=40458.209578634756\n",
      "Gamma:  3.61252251401e-06\n",
      "Current iteration=6300, the loss=40487.86922418255\n",
      "Gamma:  3.61698482143e-06\n",
      "Current iteration=6400, the loss=40517.728925720694\n",
      "Gamma:  3.6214493386e-06\n",
      "Current iteration=6500, the loss=40547.728537765084\n",
      "Gamma:  3.6259160524e-06\n",
      "Current iteration=6600, the loss=40577.81334720208\n",
      "Gamma:  3.63038495722e-06\n",
      "Current iteration=6700, the loss=40607.90426646499\n",
      "Gamma:  3.63485605682e-06\n",
      "Current iteration=6800, the loss=40637.91305480483\n",
      "Gamma:  3.63932936428e-06\n",
      "Current iteration=6900, the loss=40667.76304899107\n",
      "Gamma:  3.6438048989e-06\n",
      "Current iteration=7000, the loss=40697.42315795426\n",
      "Gamma:  3.64828267961e-06\n",
      "Current iteration=7100, the loss=40726.929967438315\n",
      "Gamma:  3.65276271867e-06\n",
      "Current iteration=7200, the loss=40756.35899043627\n",
      "Gamma:  3.65724502033e-06\n",
      "Current iteration=7300, the loss=40785.77346440596\n",
      "Gamma:  3.66172958375e-06\n",
      "Current iteration=7400, the loss=40815.20372499426\n",
      "Gamma:  3.66621640622e-06\n",
      "Current iteration=7500, the loss=40844.655726377096\n",
      "Gamma:  3.6707054851e-06\n",
      "Current iteration=7600, the loss=40874.12452330946\n",
      "Gamma:  3.67519681851e-06\n",
      "Current iteration=7700, the loss=40903.602485258925\n",
      "Gamma:  3.67969040536e-06\n",
      "Current iteration=7800, the loss=40933.08273186034\n",
      "Gamma:  3.68418624523e-06\n",
      "Current iteration=7900, the loss=40962.56014397963\n",
      "Gamma:  3.68868433815e-06\n",
      "Current iteration=8000, the loss=40992.03142823946\n",
      "Gamma:  3.69318468442e-06\n",
      "Current iteration=8100, the loss=41021.494888249836\n",
      "Gamma:  3.69768728447e-06\n",
      "Current iteration=8200, the loss=41050.95014385812\n",
      "Gamma:  3.70219213867e-06\n",
      "Current iteration=8300, the loss=41080.39787404502\n",
      "Gamma:  3.70669924732e-06\n",
      "Current iteration=8400, the loss=41109.83960022522\n",
      "Gamma:  3.71120861047e-06\n",
      "Current iteration=8500, the loss=41139.27750826868\n",
      "Gamma:  3.71572022791e-06\n",
      "Current iteration=8600, the loss=41168.71430310256\n",
      "Gamma:  3.72023409914e-06\n",
      "Current iteration=8700, the loss=41198.15308967738\n",
      "Gamma:  3.72475022325e-06\n",
      "Current iteration=8800, the loss=41227.597275109125\n",
      "Gamma:  3.72926859899e-06\n",
      "Current iteration=8900, the loss=41257.05048791507\n",
      "Gamma:  3.73378922468e-06\n",
      "Current iteration=9000, the loss=41286.516511161986\n",
      "Gamma:  3.73831209822e-06\n",
      "Current iteration=9100, the loss=41315.99922706036\n",
      "Gamma:  3.74283721708e-06\n",
      "Current iteration=9200, the loss=41345.50257103749\n",
      "Gamma:  3.74736457831e-06\n",
      "Current iteration=9300, the loss=41375.03049374007\n",
      "Gamma:  3.75189417851e-06\n",
      "Current iteration=9400, the loss=41404.58692968449\n",
      "Gamma:  3.75642601385e-06\n",
      "Current iteration=9500, the loss=41434.175771523776\n",
      "Gamma:  3.76096008009e-06\n",
      "Current iteration=9600, the loss=41463.800849060455\n",
      "Gamma:  3.76549637257e-06\n",
      "Current iteration=9700, the loss=41493.4659122907\n",
      "Gamma:  3.7700348862e-06\n",
      "Current iteration=9800, the loss=41523.174617867015\n",
      "Gamma:  3.77457561553e-06\n",
      "Current iteration=9900, the loss=41552.930518466426\n",
      "Gamma:  3.77911855469e-06\n",
      "Current iteration=10000, the loss=41582.73705462909\n",
      "Gamma:  3.78366369747e-06\n",
      "Current iteration=10100, the loss=41612.59754869063\n",
      "Gamma:  3.78821103727e-06\n",
      "Current iteration=10200, the loss=41642.515200491864\n",
      "Gamma:  3.79276056717e-06\n",
      "Current iteration=10300, the loss=41672.4930845901\n",
      "Gamma:  3.79731227991e-06\n",
      "Current iteration=10400, the loss=41702.53414873817\n",
      "Gamma:  3.8018661679e-06\n",
      "Current iteration=10500, the loss=41732.64121342426\n",
      "Gamma:  3.80642222327e-06\n",
      "Current iteration=10600, the loss=41762.81697230353\n",
      "Gamma:  3.81098043783e-06\n",
      "Current iteration=10700, the loss=41793.06399336735\n",
      "Gamma:  3.81554080315e-06\n",
      "Current iteration=10800, the loss=41823.3847207202\n",
      "Gamma:  3.82010331049e-06\n",
      "Current iteration=10900, the loss=41853.78147685287\n",
      "Gamma:  3.82466795091e-06\n",
      "Current iteration=11000, the loss=41884.25646531901\n",
      "Gamma:  3.82923471521e-06\n",
      "Current iteration=11100, the loss=41914.81177372349\n",
      "Gamma:  3.83380359395e-06\n",
      "Current iteration=11200, the loss=41945.44937696008\n",
      "Gamma:  3.8383745775e-06\n",
      "Current iteration=11300, the loss=41976.17114063539\n",
      "Gamma:  3.84294765602e-06\n",
      "Current iteration=11400, the loss=42006.97882462086\n",
      "Gamma:  3.8475228195e-06\n",
      "Current iteration=11500, the loss=42037.8740866966\n",
      "Gamma:  3.85210005771e-06\n",
      "Current iteration=11600, the loss=42068.85848624385\n",
      "Gamma:  3.85667936031e-06\n",
      "Current iteration=11700, the loss=42099.93348795767\n",
      "Gamma:  3.86126071674e-06\n",
      "Current iteration=11800, the loss=42131.100465547584\n",
      "Gamma:  3.86584411635e-06\n",
      "Current iteration=11900, the loss=42162.3607054105\n",
      "Gamma:  3.87042954831e-06\n",
      "Current iteration=12000, the loss=42193.71541025372\n",
      "Gamma:  3.87501700169e-06\n",
      "Current iteration=12100, the loss=42225.165702653656\n",
      "Gamma:  3.87960646542e-06\n",
      "Current iteration=12200, the loss=42256.712628533576\n",
      "Gamma:  3.88419792833e-06\n",
      "Current iteration=12300, the loss=42288.357160560605\n",
      "Gamma:  3.88879137914e-06\n",
      "Current iteration=12400, the loss=42320.10020144463\n",
      "Gamma:  3.89338680648e-06\n",
      "Current iteration=12500, the loss=42351.94258712913\n",
      "Gamma:  3.89798419889e-06\n",
      "Current iteration=12600, the loss=42383.88508987\n",
      "Gamma:  3.90258354481e-06\n",
      "Current iteration=12700, the loss=42415.92842112137\n",
      "Gamma:  3.90718483264e-06\n",
      "Current iteration=12800, the loss=42448.07323393414\n",
      "Gamma:  3.91178805068e-06\n",
      "Current iteration=12900, the loss=42480.320123073194\n",
      "Gamma:  3.91639318718e-06\n",
      "Current iteration=13000, the loss=42512.66961145527\n",
      "Gamma:  3.92100023032e-06\n",
      "Current iteration=13100, the loss=42545.12204126305\n",
      "Gamma:  3.92560916826e-06\n",
      "Current iteration=13200, the loss=42577.67671482551\n",
      "Gamma:  3.93021998909e-06\n",
      "Current iteration=13300, the loss=42610.32438449517\n",
      "Gamma:  3.93483268097e-06\n",
      "Current iteration=13400, the loss=42642.97323950649\n",
      "Gamma:  3.93944723301e-06\n",
      "Current iteration=13500, the loss=42674.62363859093\n",
      "Gamma:  3.9440636474e-06\n",
      "Current iteration=13600, the loss=42692.93469008548\n",
      "Gamma:  3.94868217452e-06\n",
      "Current iteration=13700, the loss=42536.44094462207\n",
      "Gamma:  3.95330447234e-06\n",
      "Current iteration=13800, the loss=42208.1404592791\n",
      "Gamma:  3.95792908674e-06\n",
      "Current iteration=13900, the loss=42227.86169934582\n",
      "Gamma:  3.96255561112e-06\n",
      "Current iteration=14000, the loss=42248.2204880068\n",
      "Gamma:  3.96718404682e-06\n",
      "Current iteration=14100, the loss=42268.349499314776\n",
      "Gamma:  3.97181437832e-06\n",
      "Current iteration=14200, the loss=42288.71593284722\n",
      "Gamma:  3.9764465908e-06\n",
      "Current iteration=14300, the loss=42309.41965469572\n",
      "Gamma:  3.98108067013e-06\n",
      "Current iteration=14400, the loss=42330.49012781307\n",
      "Gamma:  3.98571660237e-06\n",
      "Current iteration=14500, the loss=42351.9369796345\n",
      "Gamma:  3.99035437353e-06\n",
      "Current iteration=14600, the loss=42373.76283824572\n",
      "Gamma:  3.99499396955e-06\n",
      "Current iteration=14700, the loss=42395.96760327726\n",
      "Gamma:  3.99963537618e-06\n",
      "Current iteration=14800, the loss=42418.55008962881\n",
      "Gamma:  4.00427857905e-06\n",
      "Current iteration=14900, the loss=42441.5086854211\n",
      "Gamma:  4.00892356363e-06\n",
      "Current iteration=15000, the loss=42464.84159477651\n",
      "Gamma:  4.01357031524e-06\n",
      "Current iteration=15100, the loss=42488.54689456894\n",
      "Gamma:  4.01821881908e-06\n",
      "Current iteration=15200, the loss=42512.6225058917\n",
      "Gamma:  4.02286906023e-06\n",
      "Current iteration=15300, the loss=42537.06612766549\n",
      "Gamma:  4.02752102369e-06\n",
      "Current iteration=15400, the loss=42561.87515615421\n",
      "Gamma:  4.03217469435e-06\n",
      "Current iteration=15500, the loss=42587.04660313975\n",
      "Gamma:  2.69147938721e-06\n",
      "Current iteration=15600, the loss=37125.767222345574\n",
      "Gamma:  2.69509899419e-06\n",
      "Current iteration=15700, the loss=37254.61109733076\n",
      "Gamma:  2.69871593833e-06\n",
      "Current iteration=15800, the loss=37300.58979015683\n",
      "Gamma:  2.70233467535e-06\n",
      "Current iteration=15900, the loss=37322.63988248922\n",
      "Gamma:  2.70595657851e-06\n",
      "Current iteration=16000, the loss=37336.81082184505\n",
      "Gamma:  2.70958212572e-06\n",
      "Current iteration=16100, the loss=37348.04876103873\n",
      "Gamma:  2.71321150624e-06\n",
      "Current iteration=16200, the loss=37358.063974669654\n",
      "Gamma:  2.71684480445e-06\n",
      "Current iteration=16300, the loss=37367.51788775414\n",
      "Gamma:  2.72048206249e-06\n",
      "Current iteration=16400, the loss=37376.69066365613\n",
      "Gamma:  2.72412330393e-06\n",
      "Current iteration=16500, the loss=37385.71062968997\n",
      "Gamma:  2.7277685436e-06\n",
      "Current iteration=16600, the loss=37394.641274825706\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-297-80c2aab47ff8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mweights_1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreg_logistic_regression\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmatrix_std_tx_1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlambdas\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgammas\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_iter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-296-57729145ef54>\u001b[0m in \u001b[0;36mreg_logistic_regression\u001b[0;34m(y, tx, lambda_, gamma, max_iters)\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mreg_logistic_regression\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlambda_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_iters\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m     \u001b[0;34m\"\"\" return the final w from the penalized logistic regression, with lambda_ as a non 0 value\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 94\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mlogistic_regression_helper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_iters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlambda_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-296-57729145ef54>\u001b[0m in \u001b[0;36mlogistic_regression_helper\u001b[0;34m(y, tx, gamma, max_iters, lambda_)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0miter\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_iters\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcalculate_loss_logistic_regression\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mlambda_\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinalg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m         \u001b[0mgradient\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcalculate_gradient_logistic_regression\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0mw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mw\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mgradient\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mgamma\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-296-57729145ef54>\u001b[0m in \u001b[0;36mcalculate_loss_logistic_regression\u001b[0;34m(y, tx, w)\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mprediction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtx\u001b[0m \u001b[0;34m@\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m     \u001b[0my1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwhere\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mBINARY_CLASSIFICATOIN_1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0mover_700\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwhere\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprediction\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0;36m700\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "weights_1 = reg_logistic_regression(y_1, matrix_std_tx_1, lambdas[0], gammas[0], max_iter)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gamma:  0.00333333333333\n",
      "Current iteration=0, the loss=50282.97591936011\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chenfs/anaconda/lib/python3.5/site-packages/ipykernel/__main__.py:18: RuntimeWarning: overflow encountered in exp\n",
      "/Users/chenfs/anaconda/lib/python3.5/site-packages/ipykernel/__main__.py:7: RuntimeWarning: overflow encountered in exp\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gamma:  0.000130061474369\n",
      "Current iteration=100, the loss=1299813.3253270006\n",
      "Gamma:  5.78050997194e-05\n",
      "Current iteration=200, the loss=821260.9207168942\n",
      "Gamma:  5.78050997194e-05\n",
      "Current iteration=300, the loss=616401.3876636017\n",
      "Gamma:  5.78050997194e-05\n",
      "Current iteration=400, the loss=420942.4316300268\n",
      "Gamma:  3.85367331463e-05\n",
      "Current iteration=500, the loss=286249.6861910225\n",
      "Gamma:  1.71274369539e-05\n",
      "Current iteration=600, the loss=206946.38208725894\n",
      "Gamma:  1.71274369539e-05\n",
      "Current iteration=700, the loss=157597.89235992503\n",
      "Gamma:  1.71274369539e-05\n",
      "Current iteration=800, the loss=113620.56760026883\n",
      "Gamma:  7.61219420174e-06\n",
      "Current iteration=900, the loss=85422.16557873185\n",
      "Gamma:  7.61219420174e-06\n",
      "Current iteration=1000, the loss=73176.41080177324\n",
      "Gamma:  7.61219420174e-06\n",
      "Current iteration=1100, the loss=63774.83872950742\n",
      "Gamma:  7.61219420174e-06\n",
      "Current iteration=1200, the loss=56729.12816050771\n",
      "Gamma:  7.61219420174e-06\n",
      "Current iteration=1300, the loss=51293.49741852834\n",
      "Gamma:  3.38319742299e-06\n",
      "Current iteration=1400, the loss=49109.23301273872\n",
      "Gamma:  3.38319742299e-06\n",
      "Current iteration=1500, the loss=47248.57507621507\n",
      "Gamma:  3.38319742299e-06\n",
      "Current iteration=1600, the loss=45569.537310308166\n",
      "Gamma:  3.38319742299e-06\n",
      "Current iteration=1700, the loss=44062.022920128664\n",
      "Gamma:  3.38319742299e-06\n",
      "Current iteration=1800, the loss=42716.388307849375\n",
      "Gamma:  3.38319742299e-06\n",
      "Current iteration=1900, the loss=41521.78043637182\n",
      "Gamma:  3.38319742299e-06\n",
      "Current iteration=2000, the loss=40465.933006595675\n",
      "Gamma:  3.38319742299e-06\n",
      "Current iteration=2100, the loss=39553.07332390531\n",
      "Gamma:  3.38319742299e-06\n",
      "Current iteration=2200, the loss=38789.68198913543\n",
      "Gamma:  3.38319742299e-06\n",
      "Current iteration=2300, the loss=38118.18550464419\n",
      "Gamma:  3.38319742299e-06\n",
      "Current iteration=2400, the loss=37524.45676838455\n",
      "Gamma:  3.38319742299e-06\n",
      "Current iteration=2500, the loss=36998.342307944564\n",
      "Gamma:  3.38319742299e-06\n",
      "Current iteration=2600, the loss=36531.17975667957\n",
      "Gamma:  3.38319742299e-06\n",
      "Current iteration=2700, the loss=36115.37479329673\n",
      "Gamma:  3.38319742299e-06\n",
      "Current iteration=2800, the loss=35744.308262246326\n",
      "Gamma:  3.38319742299e-06\n",
      "Current iteration=2900, the loss=35412.25693319279\n",
      "Gamma:  3.38319742299e-06\n",
      "Current iteration=3000, the loss=35114.30082721614\n",
      "Gamma:  3.38319742299e-06\n",
      "Current iteration=3100, the loss=34846.22409284855\n",
      "Gamma:  3.38319742299e-06\n",
      "Current iteration=3200, the loss=34604.41798340645\n",
      "Gamma:  3.38319742299e-06\n",
      "Current iteration=3300, the loss=34385.79146859835\n",
      "Gamma:  3.38319742299e-06\n",
      "Current iteration=3400, the loss=34187.69205504357\n",
      "Gamma:  3.38319742299e-06\n",
      "Current iteration=3500, the loss=34007.83738330592\n",
      "Gamma:  3.38319742299e-06\n",
      "Current iteration=3600, the loss=33844.25706221113\n",
      "Gamma:  3.38319742299e-06\n",
      "Current iteration=3700, the loss=33695.243701486404\n",
      "Gamma:  3.38319742299e-06\n",
      "Current iteration=3800, the loss=33559.311940792715\n",
      "Gamma:  3.38319742299e-06\n",
      "Current iteration=3900, the loss=33435.16428056208\n",
      "Gamma:  3.38319742299e-06\n",
      "Current iteration=4000, the loss=33321.66261386397\n",
      "Gamma:  3.38319742299e-06\n",
      "Current iteration=4100, the loss=33217.80449360042\n",
      "Gamma:  3.38319742299e-06\n",
      "Current iteration=4200, the loss=33122.70331585436\n",
      "Gamma:  3.38319742299e-06\n",
      "Current iteration=4300, the loss=33035.57173891668\n",
      "Gamma:  3.38319742299e-06\n",
      "Current iteration=4400, the loss=32955.70777876487\n",
      "Gamma:  3.38319742299e-06\n",
      "Current iteration=4500, the loss=32882.48312303732\n",
      "Gamma:  3.38319742299e-06\n",
      "Current iteration=4600, the loss=32815.333288212154\n",
      "Gamma:  3.38319742299e-06\n",
      "Current iteration=4700, the loss=32753.74931160785\n",
      "Gamma:  3.38319742299e-06\n",
      "Current iteration=4800, the loss=32697.270723975686\n",
      "Gamma:  3.38319742299e-06\n",
      "Current iteration=4900, the loss=32645.479592433574\n",
      "Gamma:  3.38319742299e-06\n",
      "Current iteration=5000, the loss=32597.99552992245\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=5100, the loss=32558.096121580053\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=5200, the loss=32539.80280690363\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=5300, the loss=32522.20722038179\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=5400, the loss=32505.283968516564\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=5500, the loss=32489.00860850243\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=5600, the loss=32473.357593047203\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=5700, the loss=32458.30822964253\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=5800, the loss=32443.838642438717\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=5900, the loss=32429.927736458663\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=6000, the loss=32416.555163962796\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=6100, the loss=32403.701292802274\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=6200, the loss=32391.347176613323\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=6300, the loss=32379.474526717473\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=6400, the loss=32368.065685601854\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=6500, the loss=32357.103601858915\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=6600, the loss=32346.57180646794\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=6700, the loss=32336.454390299677\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=6800, the loss=32326.7359827201\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=6900, the loss=32317.40173115985\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=7000, the loss=32308.437281499504\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=7100, the loss=32299.828759095763\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=7200, the loss=32291.562750235185\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=7300, the loss=32283.626283741913\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=7400, the loss=32276.006812369425\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=7500, the loss=32268.69219344974\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=7600, the loss=32261.670668008035\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=7700, the loss=32254.930837092423\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=7800, the loss=32248.4616332478\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=7900, the loss=32242.25228355246\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=8000, the loss=32236.29225778589\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=8100, the loss=32230.571189889208\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=8200, the loss=32225.078751091416\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=8300, the loss=32219.804438879844\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=8400, the loss=32214.737243959236\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=8500, the loss=32209.865247387206\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=8600, the loss=32205.175523262467\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=8700, the loss=32200.654989244253\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=8800, the loss=32196.29197912804\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=8900, the loss=32192.077108997804\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=9000, the loss=32188.00296224044\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=9100, the loss=32184.063421081733\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=9200, the loss=32180.25318470564\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=9300, the loss=32176.567492531343\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=9400, the loss=32173.001965014984\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=9500, the loss=32169.55250723191\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=9600, the loss=32166.215247961543\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=9700, the loss=32162.986499932584\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=9800, the loss=32159.862733098376\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=9900, the loss=32156.840556112293\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=10000, the loss=32153.916703049937\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=10100, the loss=32151.088023537668\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=10200, the loss=32148.351475125302\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=10300, the loss=32145.70411716158\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=10400, the loss=32143.143105695883\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=10500, the loss=32140.665689097492\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=10600, the loss=32138.269204190346\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=10700, the loss=32135.951072769996\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=10800, the loss=32133.708798413423\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=10900, the loss=32131.539963520787\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=11000, the loss=32129.442226547137\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=11100, the loss=32127.413319393676\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=11200, the loss=32125.451044936963\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=11300, the loss=32123.553274679387\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=11400, the loss=32121.717946508114\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=11500, the loss=32119.943062552436\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=11600, the loss=32118.226687131068\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=11700, the loss=32116.566944782444\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=11800, the loss=32114.962018372127\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=11900, the loss=32113.410147271992\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=12000, the loss=32111.90962560679\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=12100, the loss=32110.458800563913\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=12200, the loss=32109.056070762796\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=12300, the loss=32107.699884680696\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=12400, the loss=32106.388739131788\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=12500, the loss=32105.121177796977\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=12600, the loss=32103.895789801925\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=12700, the loss=32102.711208340894\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=12800, the loss=32101.56610934455\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=12900, the loss=32100.45921018963\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=13000, the loss=32099.3892684487\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=13100, the loss=32098.355080678397\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=13200, the loss=32097.35548124469\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=13300, the loss=32096.389341183494\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=13400, the loss=32095.455567095614\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=13500, the loss=32094.553100074478\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=13600, the loss=32093.680914665714\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=13700, the loss=32092.8380178573\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=13800, the loss=32092.023448099375\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=13900, the loss=32091.23627435264\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=14000, the loss=32090.47559516449\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=14100, the loss=32089.740537771937\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=14200, the loss=32089.03025723051\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=14300, the loss=32088.34393556842\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=14400, the loss=32087.680780965038\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=14500, the loss=32087.04002695322\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=14600, the loss=32086.42093164448\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=14700, the loss=32085.82277697667\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=14800, the loss=32085.24486798329\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=14900, the loss=32084.686532083862\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=15000, the loss=32084.14711839492\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=15100, the loss=32083.62599706079\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=15200, the loss=32083.122558603856\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=15300, the loss=32082.636213293656\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=15400, the loss=32082.166390534236\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=15500, the loss=32081.71253826947\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=15600, the loss=32081.274122405597\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=15700, the loss=32080.850626250693\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=15800, the loss=32080.44154997055\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=15900, the loss=32080.04641006056\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=16000, the loss=32079.6647388331\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=16100, the loss=32079.29608392005\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=16200, the loss=32078.940007790025\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=16300, the loss=32078.596087279933\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=16400, the loss=32078.26391314046\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=16500, the loss=32077.943089595054\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=16600, the loss=32077.633233912158\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=16700, the loss=32077.333975990205\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=16800, the loss=32077.044957955102\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=16900, the loss=32076.76583376984\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=17000, the loss=32076.4962688559\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=17100, the loss=32076.2359397261\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=17200, the loss=32075.984533628613\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=17300, the loss=32075.741748201803\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=17400, the loss=32075.507291139613\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=17500, the loss=32075.280879867216\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=17600, the loss=32075.062241226515\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=17700, the loss=32074.85111117146\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=17800, the loss=32074.64723447266\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=17900, the loss=32074.45036443114\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=18000, the loss=32074.26026260101\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=18100, the loss=32074.076698520716\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=18200, the loss=32073.899449452638\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=18300, the loss=32073.728300130893\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=18400, the loss=32073.56304251693\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=18500, the loss=32073.403475562856\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=18600, the loss=32073.2494049822\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=18700, the loss=32073.100643027832\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=18800, the loss=32072.95700827698\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=18900, the loss=32072.81832542297\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=19000, the loss=32072.684425073632\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=19100, the loss=32072.555143556052\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=19200, the loss=32072.43032272759\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=19300, the loss=32072.30980979287\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=19400, the loss=32072.193457126697\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=19500, the loss=32072.081122102543\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=19600, the loss=32071.972666926602\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=19700, the loss=32071.867958477185\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=19800, the loss=32071.766868149236\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=19900, the loss=32071.6692717039\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=20000, the loss=32071.575049122934\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=20100, the loss=32071.48408446784\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=20200, the loss=32071.396265743548\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=20300, the loss=32071.31148476656\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=20400, the loss=32071.22963703732\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=20500, the loss=32071.150621616824\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=20600, the loss=32071.074341007166\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=20700, the loss=32071.00070103606\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=20800, the loss=32070.929610745094\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=20900, the loss=32070.860982281672\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=21000, the loss=32070.794730794492\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=21100, the loss=32070.73077433245\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=21200, the loss=32070.669033746875\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=21300, the loss=32070.609432597015\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=21400, the loss=32070.551897058565\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=21500, the loss=32070.496355835297\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=21600, the loss=32070.442740073617\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=21700, the loss=32070.390983279805\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=21800, the loss=32070.341021240227\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=21900, the loss=32070.292791944008\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=22000, the loss=32070.24623550835\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=22100, the loss=32070.201294106366\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=22200, the loss=32070.157911897197\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=22300, the loss=32070.116034958617\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=22400, the loss=32070.075611221797\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=22500, the loss=32070.036590408214\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=22600, the loss=32069.99892396875\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=22700, the loss=32069.962565024794\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=22800, the loss=32069.927468311318\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=22900, the loss=32069.893590121857\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=23000, the loss=32069.860888255338\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=23100, the loss=32069.82932196469\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=23200, the loss=32069.79885190724\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=23300, the loss=32069.769440096632\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=23400, the loss=32069.741049856584\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=23500, the loss=32069.713645775973\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=23600, the loss=32069.687193665595\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=23700, the loss=32069.66166051634\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=23800, the loss=32069.637014458753\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=23900, the loss=32069.61322472399\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=24000, the loss=32069.590261606092\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=24100, the loss=32069.56809642551\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=24200, the loss=32069.54670149395\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=24300, the loss=32069.52605008027\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=24400, the loss=32069.506116377663\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=24500, the loss=32069.486875471892\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=24600, the loss=32069.468303310612\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=24700, the loss=32069.450376673765\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=24800, the loss=32069.433073144904\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=24900, the loss=32069.416371083615\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=25000, the loss=32069.400249598733\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=25100, the loss=32069.384688522623\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=25200, the loss=32069.3696683862\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=25300, the loss=32069.355170394894\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=25400, the loss=32069.34117640542\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=25500, the loss=32069.327668903297\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=25600, the loss=32069.31463098116\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=25700, the loss=32069.30204631784\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=25800, the loss=32069.289899158135\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=25900, the loss=32069.278174293264\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=26000, the loss=32069.266857042025\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=26100, the loss=32069.255933232533\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=26200, the loss=32069.245389184674\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=26300, the loss=32069.235211693085\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=26400, the loss=32069.225388010753\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=26500, the loss=32069.21590583317\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=26600, the loss=32069.206753283037\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=26700, the loss=32069.197918895457\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=26800, the loss=32069.18939160373\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=26900, the loss=32069.181160725515\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=27000, the loss=32069.173215949548\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=27100, the loss=32069.165547322806\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=27200, the loss=32069.15814523809\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=27300, the loss=32069.15100042203\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=27400, the loss=32069.144103923587\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=27500, the loss=32069.13744710278\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=27600, the loss=32069.13102162002\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=27700, the loss=32069.124819425608\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=27800, the loss=32069.11883274973\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=27900, the loss=32069.11305409274\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=28000, the loss=32069.1074762158\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=28100, the loss=32069.102092131823\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=28200, the loss=32069.096895096725\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=28300, the loss=32069.09187860103\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=28400, the loss=32069.087036361714\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=28500, the loss=32069.082362314308\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=28600, the loss=32069.077850605372\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=28700, the loss=32069.07349558512\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=28800, the loss=32069.069291800373\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=28900, the loss=32069.065233987712\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=29000, the loss=32069.061317066913\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=29100, the loss=32069.057536134555\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=29200, the loss=32069.053886457896\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=29300, the loss=32069.050363468934\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=29400, the loss=32069.046962758686\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=29500, the loss=32069.04368007164\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=29600, the loss=32069.040511300467\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=29700, the loss=32069.03745248081\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=29800, the loss=32069.034499786365\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=29900, the loss=32069.031649524048\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=30000, the loss=32069.028898129352\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=30100, the loss=32069.026242161945\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=30200, the loss=32069.02367830124\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=30300, the loss=32069.021203342345\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=30400, the loss=32069.018814191924\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=30500, the loss=32069.016507864424\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=30600, the loss=32069.01428147825\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=30700, the loss=32069.01213225217\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=30800, the loss=32069.010057501822\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=30900, the loss=32069.008054636346\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=31000, the loss=32069.00612115512\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=31100, the loss=32069.00425464465\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=31200, the loss=32069.00245277546\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=31300, the loss=32069.00071329927\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=31400, the loss=32068.999034046068\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=31500, the loss=32068.99741292148\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=31600, the loss=32068.99584790405\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=31700, the loss=32068.994337042765\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=31800, the loss=32068.99287845455\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=31900, the loss=32068.99147032194\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=32000, the loss=32068.990110890787\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=32100, the loss=32068.98879846804\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=32200, the loss=32068.987531419625\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=32300, the loss=32068.986308168398\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=32400, the loss=32068.98512719214\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=32500, the loss=32068.98398702166\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=32600, the loss=32068.982886238955\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=32700, the loss=32068.981823475406\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=32800, the loss=32068.980797410077\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=32900, the loss=32068.979806768017\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=33000, the loss=32068.978850318716\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=33100, the loss=32068.977926874508\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=33200, the loss=32068.977035289096\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=33300, the loss=32068.97617445608\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=33400, the loss=32068.975343307644\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=33500, the loss=32068.974540813128\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=33600, the loss=32068.97376597777\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=33700, the loss=32068.97301784146\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=33800, the loss=32068.972295477506\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=33900, the loss=32068.971597991494\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=34000, the loss=32068.970924520178\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=34100, the loss=32068.970274230323\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=34200, the loss=32068.969646317728\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=34300, the loss=32068.969040006217\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=34400, the loss=32068.968454546597\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=34500, the loss=32068.967889215775\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=34600, the loss=32068.967343315828\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=34700, the loss=32068.96681617314\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=34800, the loss=32068.96630713751\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=34900, the loss=32068.9658155814\n"
     ]
    }
   ],
   "source": [
    "weights_23 = reg_logistic_regression(y_23, matrix_std_tx_23, lambdas[0], gammas[0], max_iter)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1  Size:  77544 \tPerformance:  0.7770427112349118\n",
      "0  Size:  99913 \tPerformance:  0.8369781710087776\n",
      "23 Size:  72543 \tPerformance:  0.8077002605351309\n"
     ]
    }
   ],
   "source": [
    "print(\"1  Size: \", len(y_1), \"\\tPerformance: \", performance(weights_1, y_1, matrix_std_tx_1))\n",
    "print(\"0  Size: \", len(y_0), \"\\tPerformance: \", performance(weights_0, y_0, matrix_std_tx_0))\n",
    "print(\"23 Size: \", len(y_23), \"\\tPerformance: \", performance(weights_23, y_23, matrix_std_tx_23))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate predictions and save ouput in csv format for submission:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "shapes (227458,72) and (54,1) not aligned: 72 (dim 1) != 54 (dim 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-198-791892fc3558>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0moutput_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'../../data/output0.csv'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m \u001b[0my_pred_0\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpredict_labels\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweights_0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuild_poly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstd_tx_0_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdegree\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0mcreate_csv_submission\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mids_0_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred_0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/chenfs/Google Drive/EPFL/Courses/PC and ML/ML_course_origin/ML_course/projects/project1/src/python/proj1_helpers.py\u001b[0m in \u001b[0;36mpredict_labels\u001b[0;34m(weights, data)\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mpredict_labels\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0;34m\"\"\"Generates class predictions given weights, and a test data matrix\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m     \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m     \u001b[0my_pred\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwhere\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0my_pred\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwhere\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: shapes (227458,72) and (54,1) not aligned: 72 (dim 1) != 54 (dim 0)"
     ]
    }
   ],
   "source": [
    "DATA_TEST_PATH = '../../data/test.csv' \n",
    "_, tX_test, ids_test = load_csv_data(DATA_TEST_PATH)\n",
    "i_0_test, i_1_test, i_23_test = split_dataset_wrt22(tX_test)\n",
    "\n",
    "tx_0_test = tX_test[i_0_test]\n",
    "tx_1_test = tX_test[i_1_test]\n",
    "tx_23_test = tX_test[i_23_test]\n",
    "\n",
    "std_tx_0_test = standardize_0(tx_0_test)\n",
    "std_tx_1_test = standardize_1(tx_1_test)\n",
    "std_tx_23_test = standardize_23(tx_23_test)\n",
    "\n",
    "ids_0_test = ids_test[i_0_test]\n",
    "ids_1_test = ids_test[i_1_test]\n",
    "ids_23_test = ids_test[i_23_test]\n",
    "\n",
    "output_path = '../../data/output0.csv'\n",
    "y_pred_0 = predict_labels(weights_0, build_poly(std_tx_0_test, degree))\n",
    "create_csv_submission(ids_0_test, y_pred_0, output_path)\n",
    "\n",
    "\n",
    "output_path = '../../data/output1_79.csv'\n",
    "y_pred_1 = predict_labels(weights_1, build_poly(std_tx_1_test, degree))\n",
    "create_csv_submission(ids_1_test, y_pred_1, output_path)\n",
    "\n",
    "\n",
    "output_path = '../../data/output23.csv'\n",
    "y_pred_23 = predict_labels(weights_23, build_poly(std_tx_23_test, degree))\n",
    "create_csv_submission(ids_23_test, y_pred_23, output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "OUTPUT_PATH = '../../data/output.csv' # TODO: fill in desired name of output file for submission\n",
    "y_pred = predict_labels(weights, build_poly(standardize(tX_test), degree))\n",
    "create_csv_submission(ids_test, y_pred, OUTPUT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
