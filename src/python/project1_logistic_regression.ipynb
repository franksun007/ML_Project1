{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# Useful starting lines\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the training data into feature matrix, class labels, and event ids:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from proj1_helpers import *\n",
    "DATA_TRAIN_PATH = '../../data/train.csv'\n",
    "y, tX, ids = load_csv_data(DATA_TRAIN_PATH)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Do your thing crazy machine learning thing here :) ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def build_poly(x, degree):\n",
    "    \n",
    "    matrix = np.zeros((x.shape[0], x.shape[1] * (degree + 1)))\n",
    "    \n",
    "    for i in range(degree + 1):\n",
    "        matrix[:, (i * x.shape[1]) : ((i + 1) * x.shape[1])] = (x ** i)[:]\n",
    "    return matrix\n",
    "\n",
    "\n",
    "def build_k_indices(y, k_fold, seed=62):\n",
    "    \"\"\"build k indices for k-fold.\"\"\"\n",
    "    num_row = y.shape[0]\n",
    "    interval = int(num_row / k_fold)\n",
    "    np.random.seed(seed)\n",
    "    indices = np.random.permutation(num_row)\n",
    "    k_indices = [indices[k * interval: (k + 1) * interval]\n",
    "                 for k in range(k_fold)]\n",
    "    return np.array(k_indices)\n",
    "\n",
    "def batch_iter(y, tx, batch_size, num_batches=None, shuffle=True):\n",
    "    \"\"\"\n",
    "    Generate a minibatch iterator for a dataset.\n",
    "    Takes as input two iterables (here the output desired values 'y' and the input data 'tx')\n",
    "    Outputs an iterator which gives mini-batches of `batch_size` matching elements from `y` and `tx`.\n",
    "    Data can be randomly shuffled to avoid ordering in the original data messing with the randomness of the minibatches.\n",
    "    Example of use :\n",
    "    for minibatch_y, minibatch_tx in batch_iter(y, tx, 32):\n",
    "        <DO-SOMETHING>\n",
    "    \"\"\"\n",
    "\n",
    "    data_size = len(y)\n",
    "    num_batches_max = int(np.ceil(data_size/batch_size))\n",
    "    if num_batches is None:\n",
    "        num_batches = num_batches_max\n",
    "    else:\n",
    "        num_batches = min(num_batches, num_batches_max)\n",
    "\n",
    "    if shuffle:\n",
    "        shuffle_indices = np.random.permutation(np.arange(data_size))\n",
    "        shuffled_y = y[shuffle_indices]\n",
    "        shuffled_tx = tx[shuffle_indices]\n",
    "    else:\n",
    "        shuffled_y = y\n",
    "        shuffled_tx = tx\n",
    "    for batch_num in range(num_batches):\n",
    "        start_index = batch_num * batch_size\n",
    "        end_index = min((batch_num + 1) * batch_size, data_size)\n",
    "        if start_index != end_index:\n",
    "            yield shuffled_y[start_index:end_index], shuffled_tx[start_index:end_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "BINARY_CLASSIFICATOIN_0 = -1\n",
    "BINARY_CLASSIFICATOIN_1 = 1\n",
    "\n",
    "\n",
    "def sigmoid(t):\n",
    "    \"\"\"apply sigmoid function on t.\"\"\"\n",
    "    return 1.0 / (1.0 + np.exp(-t))\n",
    "\n",
    "\n",
    "def calculate_loss_logistic_regression(y, tx, w):\n",
    "    \"\"\"compute the cost by negative log likelihood.\"\"\"\n",
    "    prediction = tx @ w\n",
    "    \n",
    "    y1 = np.where(y == BINARY_CLASSIFICATOIN_1)\n",
    "\n",
    "    over_700 = np.where(prediction >= 700)\n",
    "\n",
    "    prediction_result = np.log(1 + np.exp(prediction))\n",
    "    prediction_result[over_700] = prediction[over_700]\n",
    "    prediction_result[y1] -= prediction[y1]\n",
    "    \n",
    "    result = np.sum(prediction_result)\n",
    "    return result\n",
    "\n",
    "\n",
    "def calculate_gradient_logistic_regression(y, tx, w):\n",
    "    \"\"\"compute the gradient of loss.\"\"\"\n",
    "\n",
    "    y1 = np.where(y == BINARY_CLASSIFICATOIN_1)\n",
    "    sig = sigmoid(tx @ w).reshape(len(y))\n",
    "    sig[y1] -= y[y1]\n",
    "\n",
    "    return tx.T @ sig\n",
    "\n",
    "\n",
    "def logistic_regression_helper(y, tx, gamma, max_iters, lambda_):\n",
    "\n",
    "    w = np.zeros((tx.shape[1], 1))\n",
    "    threshold = 1e-8\n",
    "    loss_prev = 0\n",
    "    w_max = w\n",
    "    perf = 0\n",
    "    i = 0\n",
    "    \n",
    "    batch_size = len(y) / 5\n",
    "\n",
    "    for iter in range(max_iters):\n",
    "        \n",
    "        for mini_y, mini_x in batch_iter(y, tx, batch_size):\n",
    "        \n",
    "            loss = calculate_loss_logistic_regression(mini_y, mini_x, w) + lambda_ * np.linalg.norm(w, 2)\n",
    "            gradient = calculate_gradient_logistic_regression(mini_y, mini_x, w)\n",
    "            w -= (gradient * gamma).reshape(w.shape)\n",
    "\n",
    "            if (loss_prev != 0) and np.abs(loss_prev - loss) < threshold:\n",
    "                print(\"Reached Theshold, exit\")\n",
    "                break\n",
    "\n",
    "            loss_prev = loss\n",
    "            \n",
    "        \n",
    "#         if (iter % 10) == 0:\n",
    "#             cur_perf = performance(w, y, tx)\n",
    "#             if cur_perf >= perf:\n",
    "#                 w_max = w\n",
    "#                 perf = cur_perf\n",
    "#                 i = iter\n",
    "\n",
    "\n",
    "        if (iter % 100) == 0:\n",
    "            print(\"Current iteration={i}, the loss={l}\".format(i=iter, l=loss))\n",
    "\n",
    "#         if (iter % 300) == 0:\n",
    "#             print(w_max)\n",
    "#             print(\"Performance: \", perf)\n",
    "#             print(\"Iteration: \", i)\n",
    "            \n",
    "\n",
    "    return w\n",
    "\n",
    "\n",
    "def logistic_regression(y, tx, gamma, max_iters):\n",
    "    \"\"\" return the final w from the logistic regression \"\"\"\n",
    "    return logistic_regression_helper(y, tx, gamma, max_iters, lambda_=0)\n",
    "\n",
    "\n",
    "def reg_logistic_regression(y, tx, lambda_, gamma, max_iters):\n",
    "    \"\"\" return the final w from the penalized logistic regression, with lambda_ as a non 0 value\"\"\"\n",
    "    return logistic_regression_helper(y, tx, gamma, max_iters, lambda_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def performance(weights, y, xT):\n",
    "    \"\"\"Returns the percentage of successful classifications for the weights,\n",
    "    given the expected results (y) and data (xT)\"\"\"\n",
    "    from proj1_helpers import predict_labels\n",
    "    compare_pred = predict_labels(weights, xT)\n",
    "    compare_pred -= y.reshape((len(y), 1))\n",
    "        \n",
    "    non_zero = 0\n",
    "    for i in range(len(compare_pred)):\n",
    "        if compare_pred[i] != 0:\n",
    "            non_zero += 1\n",
    "            \n",
    "    return 1 - non_zero / compare_pred.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "def plot_feature_relationship(y, tX):\n",
    "\n",
    "    std_tx = standardize(tX)\n",
    "\n",
    "    row = np.zeros(std_tx.shape[0])\n",
    "\n",
    "    # for i in range(len(tX)):\n",
    "    #     row[i] = tX[i][0]\n",
    "\n",
    "    plt.figure(figsize=(20, 20))\n",
    "\n",
    "    for j in range(std_tx.shape[1]):\n",
    "        for i in range(len(std_tx)):\n",
    "            row[i] = std_tx[i][j]\n",
    "        plt.subplot(5, 6, j + 1)\n",
    "        plt.title(j)\n",
    "        plt.plot(row[np.where(y == 1)], y[np.where(y == 1)], 'ro')\n",
    "        plt.plot(row[np.where(y == -1)], y[np.where(y == -1)], 'bo')\n",
    "  \n",
    "\n",
    "    plt.tight_layout()\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "# plot_feature_relationship(y, tX)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def standardize(x):\n",
    "#     cols = [4, 5, 6, 12, 26, 28]\n",
    "#     for i in range(len(x)):\n",
    "#         x[i][np.where(x[i] == -999)] = 0\n",
    "\n",
    "    # Combine feature, or do poly expansion\n",
    "    \n",
    "    # Do not normalize the jet num\n",
    "    jet_num_col = 22\n",
    "    jet_num = x[:, jet_num_col]\n",
    "    \n",
    "    # Replace -999 with some value that is the mean/median of the represent dataset \n",
    "    for i in range(x.shape[1]):\n",
    "        median = np.median(x[np.where(x[:, i] != -999), i])\n",
    "        x[np.where(x[:, i] == -999), i] = median \n",
    "        x[np.where(x[:, i] != -999), i] = x[np.where(x[:, i] != -999), i] - np.mean(x[np.where(x[:, i] != -999), i])\n",
    "    \n",
    "    mean_x = np.mean(x, axis=0)\n",
    "    x = x - mean_x\n",
    "    \n",
    "    std_x = np.std(x, axis=0)\n",
    "    x[:, std_x > 0] = x[:, std_x > 0] / std_x[std_x > 0]\n",
    "\n",
    "    x[:, jet_num_col] = jet_num\n",
    "    \n",
    "    return x\n",
    "\n",
    "# print(standardize(tX) - tX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# def add_feature(tX, cols, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chenfs/anaconda/lib/python3.5/site-packages/ipykernel/__main__.py:49: VisibleDeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n",
      "/Users/chenfs/anaconda/lib/python3.5/site-packages/ipykernel/__main__.py:18: RuntimeWarning: overflow encountered in exp\n",
      "/Users/chenfs/anaconda/lib/python3.5/site-packages/ipykernel/__main__.py:7: RuntimeWarning: overflow encountered in exp\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current iteration=0, the loss=3180308687.6000843\n",
      "Current iteration=100, the loss=889313007.2500957\n",
      "Current iteration=200, the loss=1411474542.2793546\n",
      "Current iteration=300, the loss=5656608215.101741\n",
      "Current iteration=400, the loss=913416419.7535772\n",
      "Current iteration=500, the loss=1208454568.5202713\n",
      "Current iteration=600, the loss=1135909556.7617693\n",
      "Current iteration=700, the loss=2628409051.558204\n",
      "Current iteration=800, the loss=1588890502.000391\n",
      "Current iteration=900, the loss=720905312.1548002\n",
      "Current iteration=1000, the loss=898052316.7573014\n",
      "Current iteration=1100, the loss=935839693.9204004\n",
      "Current iteration=1200, the loss=793017926.9597172\n",
      "Current iteration=1300, the loss=1736994288.4637105\n",
      "Current iteration=1400, the loss=936578964.9458123\n",
      "Current iteration=1500, the loss=519054400.51307905\n",
      "Current iteration=1600, the loss=634680928.4236592\n",
      "Current iteration=1700, the loss=1235288395.0958323\n",
      "Current iteration=1800, the loss=579119340.2990378\n",
      "Current iteration=1900, the loss=532723906.6465236\n",
      "Current iteration=2000, the loss=543717093.2001415\n",
      "Current iteration=2100, the loss=897537463.6476489\n",
      "Current iteration=2200, the loss=706030348.7886039\n",
      "Current iteration=2300, the loss=848559534.2119421\n",
      "Current iteration=2400, the loss=602145439.1597593\n",
      "Current iteration=2500, the loss=954846317.3448597\n",
      "Current iteration=2600, the loss=762903774.5638444\n",
      "Current iteration=2700, the loss=1024379823.5225298\n",
      "Current iteration=2800, the loss=892127018.6012658\n",
      "Current iteration=2900, the loss=725817333.3387464\n",
      "Current iteration=3000, the loss=593690307.9302597\n",
      "Current iteration=3100, the loss=470269744.9924732\n",
      "Current iteration=3200, the loss=847632742.2789559\n",
      "Current iteration=3300, the loss=1104605039.7470253\n",
      "Current iteration=3400, the loss=2132193138.919975\n",
      "Current iteration=3500, the loss=890933899.404817\n",
      "Current iteration=3600, the loss=1104156356.9860299\n",
      "Current iteration=3700, the loss=1024231654.8635452\n",
      "Current iteration=3800, the loss=943799637.9850329\n",
      "Current iteration=3900, the loss=555690611.4768227\n",
      "Current iteration=4000, the loss=1202733242.6445892\n",
      "Current iteration=4100, the loss=3670501764.6219683\n",
      "Current iteration=4200, the loss=494277900.69225633\n",
      "Current iteration=4300, the loss=1266724132.2065208\n",
      "Current iteration=4400, the loss=617281951.3102163\n",
      "Current iteration=4500, the loss=1029097232.332898\n",
      "Current iteration=4600, the loss=1622476121.6383858\n",
      "Current iteration=4700, the loss=805869959.2339537\n",
      "Current iteration=4800, the loss=1324017554.238652\n",
      "Current iteration=4900, the loss=611946103.2447605\n",
      "Current iteration=5000, the loss=736591547.960285\n",
      "Current iteration=5100, the loss=1370770429.7005856\n",
      "Current iteration=5200, the loss=509468019.56635505\n",
      "Current iteration=5300, the loss=5470298778.11404\n",
      "Current iteration=5400, the loss=614666810.3759712\n",
      "Current iteration=5500, the loss=1107375383.2401605\n",
      "Current iteration=5600, the loss=472656551.6975746\n",
      "Current iteration=5700, the loss=895913366.1658835\n",
      "Current iteration=5800, the loss=871847287.0189217\n",
      "Current iteration=5900, the loss=1399306647.1027822\n",
      "Current iteration=6000, the loss=3147400522.518423\n",
      "Current iteration=6100, the loss=1718459520.4601\n",
      "Current iteration=6200, the loss=1483595538.7272608\n",
      "Current iteration=6300, the loss=804309657.4032544\n",
      "Current iteration=6400, the loss=718713113.5365132\n",
      "Current iteration=6500, the loss=823130157.5743271\n",
      "Current iteration=6600, the loss=1281100833.2724361\n",
      "Current iteration=6700, the loss=491220721.20611686\n",
      "Current iteration=6800, the loss=671143298.8229259\n",
      "Current iteration=6900, the loss=715808435.1764796\n",
      "Current iteration=7000, the loss=633665029.2171847\n",
      "Current iteration=7100, the loss=775211731.8965634\n",
      "Current iteration=7200, the loss=10305235973.15888\n",
      "Current iteration=7300, the loss=517637013.6396385\n",
      "Current iteration=7400, the loss=6859718060.361584\n",
      "Current iteration=7500, the loss=8113399457.8708935\n",
      "Current iteration=7600, the loss=519573067.9310458\n",
      "Current iteration=7700, the loss=483363120.1650845\n",
      "Current iteration=7800, the loss=773066105.9235753\n",
      "Current iteration=7900, the loss=10708817110.824451\n",
      "Current iteration=8000, the loss=674618758.5411788\n",
      "Current iteration=8100, the loss=11202945604.181995\n",
      "Current iteration=8200, the loss=589728804.8605915\n",
      "Current iteration=8300, the loss=1541582221.9196966\n",
      "Current iteration=8400, the loss=5033113462.557975\n",
      "Current iteration=8500, the loss=1214066080.0933092\n",
      "Current iteration=8600, the loss=1148110988.8936698\n",
      "Current iteration=8700, the loss=1073267111.9967076\n",
      "Current iteration=8800, the loss=1481498612.1032212\n",
      "Current iteration=8900, the loss=982626394.9286864\n",
      "Current iteration=9000, the loss=799845599.5412402\n",
      "Current iteration=9100, the loss=866916438.9880617\n",
      "Current iteration=9200, the loss=1000500685.1906316\n",
      "Current iteration=9300, the loss=3958805089.4109454\n",
      "Current iteration=9400, the loss=707608382.4446377\n",
      "Current iteration=9500, the loss=1123949588.3227165\n",
      "Current iteration=9600, the loss=498219143.1858381\n",
      "Current iteration=9700, the loss=5705967688.463991\n",
      "Current iteration=9800, the loss=569603226.1634406\n",
      "Current iteration=9900, the loss=750499189.3022296\n",
      "Current iteration=10000, the loss=1162562424.8649745\n",
      "Current iteration=10100, the loss=915921320.5102094\n",
      "Current iteration=10200, the loss=922006352.8377424\n",
      "Current iteration=10300, the loss=432779302.91355973\n",
      "Current iteration=10400, the loss=1172067790.8666975\n",
      "Current iteration=10500, the loss=851100319.9146495\n",
      "Current iteration=10600, the loss=1126040851.048324\n",
      "Current iteration=10700, the loss=3731504404.0039344\n",
      "Current iteration=10800, the loss=703058985.6172681\n",
      "Current iteration=10900, the loss=685754891.78382\n",
      "Current iteration=11000, the loss=1178286233.2189157\n",
      "Current iteration=11100, the loss=1337622927.581825\n",
      "Current iteration=11200, the loss=975124657.708036\n",
      "Current iteration=11300, the loss=1172840571.458587\n",
      "Current iteration=11400, the loss=1077266331.14194\n",
      "Current iteration=11500, the loss=527886684.33537287\n",
      "Current iteration=11600, the loss=829245561.7762011\n",
      "Current iteration=11700, the loss=864594972.6731787\n",
      "Current iteration=11800, the loss=733112590.5417147\n",
      "Current iteration=11900, the loss=605670901.6975907\n",
      "Current iteration=12000, the loss=488464913.5257992\n",
      "Current iteration=12100, the loss=1156567018.896982\n",
      "Current iteration=12200, the loss=920291339.9579711\n",
      "Current iteration=12300, the loss=1298030380.9798777\n",
      "Current iteration=12400, the loss=920014269.798521\n",
      "Current iteration=12500, the loss=1135103160.5739644\n",
      "Current iteration=12600, the loss=826390533.9569871\n",
      "Current iteration=12700, the loss=801525079.9312878\n",
      "Current iteration=12800, the loss=615416482.2208157\n",
      "Current iteration=12900, the loss=10670575287.63329\n",
      "Current iteration=13000, the loss=739355472.4859273\n",
      "Current iteration=13100, the loss=6507309621.660165\n",
      "Current iteration=13200, the loss=1418407925.8996093\n",
      "Current iteration=13300, the loss=930160259.0778488\n",
      "Current iteration=13400, the loss=903608562.828743\n",
      "Current iteration=13500, the loss=603261431.3320817\n",
      "Current iteration=13600, the loss=413870849.23087883\n",
      "Current iteration=13700, the loss=5463888070.183166\n",
      "Current iteration=13800, the loss=763749073.1115847\n",
      "Current iteration=13900, the loss=3712391049.4407234\n",
      "Current iteration=14000, the loss=706093808.499409\n",
      "Current iteration=14100, the loss=1431728835.432653\n",
      "Current iteration=14200, the loss=844150183.1637812\n",
      "Current iteration=14300, the loss=635021525.0023861\n",
      "Current iteration=14400, the loss=3351946761.4048057\n",
      "Current iteration=14500, the loss=2269268144.2604065\n",
      "Current iteration=14600, the loss=1141276453.81569\n",
      "Current iteration=14700, the loss=1124570002.378135\n",
      "Current iteration=14800, the loss=455814434.848787\n",
      "Current iteration=14900, the loss=884777278.9347041\n",
      "[[ -4.39421493e+01]\n",
      " [ -4.39421493e+01]\n",
      " [ -4.39421493e+01]\n",
      " [ -4.39421493e+01]\n",
      " [ -4.39421493e+01]\n",
      " [ -4.39421493e+01]\n",
      " [ -4.39421493e+01]\n",
      " [ -4.39421493e+01]\n",
      " [ -4.39421493e+01]\n",
      " [ -4.39421493e+01]\n",
      " [ -4.39421493e+01]\n",
      " [ -4.39421493e+01]\n",
      " [ -4.39421493e+01]\n",
      " [ -4.39421493e+01]\n",
      " [ -4.39421493e+01]\n",
      " [ -4.39421493e+01]\n",
      " [ -4.39421493e+01]\n",
      " [ -4.39421493e+01]\n",
      " [ -4.39421493e+01]\n",
      " [ -4.39421493e+01]\n",
      " [ -4.39421493e+01]\n",
      " [ -4.39421493e+01]\n",
      " [ -4.39421493e+01]\n",
      " [ -4.39421493e+01]\n",
      " [ -4.39421493e+01]\n",
      " [ -4.39421493e+01]\n",
      " [ -4.39421493e+01]\n",
      " [ -4.39421493e+01]\n",
      " [ -4.39421493e+01]\n",
      " [ -4.39421493e+01]\n",
      " [  1.63634007e+04]\n",
      " [ -6.22130830e+03]\n",
      " [ -1.82900883e+04]\n",
      " [  1.03748716e+04]\n",
      " [  3.03069231e+03]\n",
      " [ -2.84147243e+03]\n",
      " [ -1.80878844e+03]\n",
      " [  3.15588379e+03]\n",
      " [ -6.38060006e+03]\n",
      " [ -3.57484158e+01]\n",
      " [ -2.46347858e+04]\n",
      " [ -9.68556774e+02]\n",
      " [  4.75517385e+03]\n",
      " [ -1.38050918e+04]\n",
      " [ -1.41509938e+02]\n",
      " [  1.67148805e+02]\n",
      " [  2.41958340e+04]\n",
      " [  2.09365451e+02]\n",
      " [  2.86832410e+02]\n",
      " [ -6.86666967e+03]\n",
      " [ -4.66281645e+01]\n",
      " [  1.44645170e+02]\n",
      " [ -9.27692828e+03]\n",
      " [  3.35219304e+03]\n",
      " [ -2.05591013e+02]\n",
      " [ -2.53922396e+02]\n",
      " [  9.62592848e+03]\n",
      " [  7.38787581e+01]\n",
      " [ -3.44758446e+02]\n",
      " [ -2.33193058e+03]\n",
      " [ -1.41953130e+04]\n",
      " [ -3.26845018e+02]\n",
      " [ -9.31317719e+03]\n",
      " [  1.20431868e+04]\n",
      " [  4.19809629e+02]\n",
      " [  6.71902115e+03]\n",
      " [  1.28120295e+03]\n",
      " [ -1.73630187e+04]\n",
      " [ -1.67072797e+04]\n",
      " [ -3.01424624e+03]\n",
      " [  3.94840646e+03]\n",
      " [  3.32667281e+03]\n",
      " [  9.57231089e+02]\n",
      " [  1.02471097e+04]\n",
      " [ -1.08125791e+03]\n",
      " [ -9.90522221e+01]\n",
      " [ -1.70062301e+02]\n",
      " [ -1.42807713e+03]\n",
      " [ -9.01753301e+01]\n",
      " [ -3.81103389e+03]\n",
      " [ -1.60667010e+01]\n",
      " [  8.30892147e+01]\n",
      " [ -7.54562185e+03]\n",
      " [ -2.63419560e+03]\n",
      " [  1.52102713e+03]\n",
      " [ -1.79994014e+02]\n",
      " [  1.25287302e+04]\n",
      " [  7.28094990e+02]\n",
      " [ -3.37950583e+02]\n",
      " [  5.31065375e+03]\n",
      " [ -7.11036792e+01]\n",
      " [  1.06571157e+03]\n",
      " [ -1.77659992e+03]\n",
      " [  3.69182506e+03]\n",
      " [ -2.38283595e+03]\n",
      " [  3.12815871e+03]\n",
      " [  4.25160673e+02]\n",
      " [  1.09096490e+04]\n",
      " [  3.19734251e+03]\n",
      " [  8.39127752e+03]\n",
      " [ -1.44720009e+03]\n",
      " [ -6.57708011e+01]\n",
      " [ -2.70604535e+02]\n",
      " [ -2.59202495e+03]\n",
      " [  4.50529812e+01]\n",
      " [ -6.05167332e+01]\n",
      " [ -1.16076926e+03]\n",
      " [ -1.00672247e+02]\n",
      " [ -1.02657597e+02]\n",
      " [ -1.25068224e+03]\n",
      " [ -2.22243298e+01]\n",
      " [ -1.09689698e+03]\n",
      " [  5.44660705e+03]\n",
      " [ -7.99004348e+02]\n",
      " [  1.08577417e+02]\n",
      " [  3.89610676e+01]\n",
      " [ -2.01443270e+03]\n",
      " [ -3.97550315e+01]\n",
      " [  1.17638690e+02]\n",
      " [ -1.39367139e+04]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# There are two parameters, lambda_ and gamma, where gamma is the step size \n",
    "\n",
    "max_iter = 15000\n",
    "lambdas = np.arange(0.1, 0.4, 0.1)\n",
    "gammas = [0.05]\n",
    "\n",
    "degree = 3\n",
    "\n",
    "std_tx = standardize(tX)\n",
    "matrix_std = build_poly(std_tx, degree)\n",
    "# print(std_tx)\n",
    "# print(tX)\n",
    "\n",
    "# k_fold = 10\n",
    "# k_indices = build_k_indices(y, k_fold)\n",
    "    \n",
    "# k = 2\n",
    "\n",
    "# y_test = y[k_indices[k]]\n",
    "# x_test = std_tx[k_indices[k]]\n",
    "# train_indices = k_indices[~(np.arange(k_indices.shape[0]) == k)]\n",
    "# y_train = np.concatenate(y[train_indices])\n",
    "# x_train = np.concatenate(std_tx[train_indices])\n",
    "    \n",
    "weights = reg_logistic_regression(y, matrix_std, lambdas[0], gammas[0], max_iter)\n",
    "\n",
    "print(weights)\n",
    "\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.779196\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "print(performance(weights, y, matrix_std))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate predictions and save ouput in csv format for submission:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "DATA_TEST_PATH = '../../data/test.csv' \n",
    "_, tX_test, ids_test = load_csv_data(DATA_TEST_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "OUTPUT_PATH = '../../data/output.csv' # TODO: fill in desired name of output file for submission\n",
    "y_pred = predict_labels(weights, build_poly(standardize(tX_test), degree))\n",
    "create_csv_submission(ids_test, y_pred, OUTPUT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
