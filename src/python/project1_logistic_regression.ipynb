{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Useful starting lines\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the training data into feature matrix, class labels, and event ids:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from proj1_helpers import *\n",
    "DATA_TRAIN_PATH = '../../data/train.csv'\n",
    "y, tX, ids = load_csv_data(DATA_TRAIN_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Do your thing crazy machine learning thing here :) ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.          0.46141372  0.06833197 ...,  1.5668      1.55858439\n",
      "   0.4125105 ]\n",
      " [ 1.          0.51670419  0.55250482 ..., -0.63936657 -0.63936694\n",
      "  -0.27381996]\n",
      " [ 1.         -2.33785898  3.19515553 ..., -0.63936657 -0.63936694\n",
      "  -0.29396985]\n",
      " ..., \n",
      " [ 1.          0.38016991  0.31931645 ..., -0.63936657 -0.63936694\n",
      "  -0.31701723]\n",
      " [ 1.          0.35431502 -0.84532397 ..., -0.63936657 -0.63936694\n",
      "  -0.74543941]\n",
      " [ 1.         -2.33785898  0.66533608 ..., -0.63936657 -0.63936694\n",
      "  -0.74543941]]\n",
      "Current iteration=0, the loss=173286.79513998624\n",
      "Current iteration=10, the loss=32032804.779582806\n",
      "Current iteration=20, the loss=30323363.851633057\n",
      "Current iteration=30, the loss=29544933.862421125\n",
      "Current iteration=40, the loss=28437933.309825484\n",
      "Current iteration=50, the loss=27505748.35188141\n",
      "Current iteration=60, the loss=26803698.848540463\n",
      "Current iteration=70, the loss=26328492.708785374\n",
      "Current iteration=80, the loss=25916050.5867202\n",
      "Current iteration=90, the loss=25598566.232025757\n",
      "Current iteration=100, the loss=25354894.080155335\n",
      "Current iteration=110, the loss=25158708.8789194\n",
      "Current iteration=120, the loss=24981886.702070583\n",
      "Current iteration=130, the loss=24845349.323754087\n",
      "Current iteration=140, the loss=24715819.606126238\n",
      "Current iteration=150, the loss=24620441.080800906\n",
      "Current iteration=160, the loss=24543622.133036073\n",
      "Current iteration=170, the loss=24475744.650118105\n",
      "Current iteration=180, the loss=24398497.915031455\n",
      "Current iteration=190, the loss=24326587.49829631\n",
      "Current iteration=200, the loss=24269976.469693992\n",
      "Current iteration=210, the loss=24218310.701643303\n",
      "Current iteration=220, the loss=24171574.119007774\n",
      "Current iteration=230, the loss=24137525.15482782\n",
      "Current iteration=240, the loss=24107810.179167114\n",
      "Current iteration=250, the loss=24074637.551290426\n",
      "Current iteration=260, the loss=24047045.44198942\n",
      "Current iteration=270, the loss=24019412.8944301\n",
      "Current iteration=280, the loss=23994822.866394855\n",
      "Current iteration=290, the loss=23973786.33985123\n",
      "Current iteration=300, the loss=23958115.75682683\n",
      "Current iteration=310, the loss=23943221.730328895\n",
      "Current iteration=320, the loss=23928698.119146686\n",
      "Current iteration=330, the loss=23914485.620387815\n",
      "Current iteration=340, the loss=23899869.677099757\n",
      "Current iteration=350, the loss=23888482.386434957\n",
      "Current iteration=360, the loss=23879552.390360214\n",
      "Current iteration=370, the loss=23871644.535011724\n",
      "Current iteration=380, the loss=23864045.584531453\n",
      "Current iteration=390, the loss=23856762.676962942\n",
      "Current iteration=400, the loss=23849169.515850443\n",
      "Current iteration=410, the loss=23841665.32703746\n",
      "Current iteration=420, the loss=23835066.034075785\n",
      "Current iteration=430, the loss=23830574.6710084\n",
      "Current iteration=440, the loss=23826586.0988651\n",
      "Current iteration=450, the loss=23823281.365738444\n",
      "Current iteration=460, the loss=23820458.884593565\n",
      "Current iteration=470, the loss=23817465.659290064\n",
      "Current iteration=480, the loss=23814275.339349616\n",
      "Current iteration=490, the loss=23811335.63464813\n",
      "Current iteration=500, the loss=23808594.15553833\n",
      "Current iteration=510, the loss=23805980.927091446\n",
      "Current iteration=520, the loss=23803656.457955472\n",
      "Current iteration=530, the loss=23801807.22496938\n",
      "Current iteration=540, the loss=23800459.24146964\n",
      "Current iteration=550, the loss=23799314.505889308\n",
      "Current iteration=560, the loss=23798221.939849474\n",
      "Current iteration=570, the loss=23797311.378421344\n",
      "Current iteration=580, the loss=23796553.869361553\n",
      "Current iteration=590, the loss=23795898.505538207\n",
      "Current iteration=600, the loss=23795323.80603438\n",
      "Current iteration=610, the loss=23794825.560999177\n",
      "Current iteration=620, the loss=23794394.243888214\n",
      "Current iteration=630, the loss=23793979.141696006\n",
      "Current iteration=640, the loss=23793514.265821602\n",
      "Current iteration=650, the loss=23792980.018492818\n",
      "Current iteration=660, the loss=23792427.828452855\n",
      "Current iteration=670, the loss=23791931.403734818\n",
      "Current iteration=680, the loss=23791519.474152233\n",
      "Current iteration=690, the loss=23791187.96691411\n",
      "Current iteration=700, the loss=23790917.261644445\n",
      "Current iteration=710, the loss=23790679.663870912\n",
      "Current iteration=720, the loss=23790449.705115072\n",
      "Current iteration=730, the loss=23790212.81847127\n",
      "Current iteration=740, the loss=23789965.62450646\n",
      "Current iteration=750, the loss=23789709.592670534\n",
      "Current iteration=760, the loss=23789445.089539345\n",
      "Current iteration=770, the loss=23789170.366153233\n",
      "Current iteration=780, the loss=23788883.95293976\n",
      "Current iteration=790, the loss=23788586.885680743\n",
      "Current iteration=800, the loss=23788283.593160324\n",
      "Current iteration=810, the loss=23787981.79526597\n",
      "Current iteration=820, the loss=23787691.640173275\n",
      "Current iteration=830, the loss=23787424.13619004\n",
      "Current iteration=840, the loss=23787189.01560683\n",
      "Current iteration=850, the loss=23786992.525925208\n",
      "Current iteration=860, the loss=23786836.132987842\n",
      "Current iteration=870, the loss=23786716.85405181\n",
      "Current iteration=880, the loss=23786628.873037428\n",
      "Current iteration=890, the loss=23786565.43251191\n",
      "Current iteration=900, the loss=23786520.243205734\n",
      "Current iteration=910, the loss=23786488.208152603\n",
      "Current iteration=920, the loss=23786465.598911922\n",
      "Current iteration=930, the loss=23786449.900772493\n",
      "Current iteration=940, the loss=23786439.505911168\n",
      "Current iteration=950, the loss=23786433.379042763\n",
      "Current iteration=960, the loss=23786430.77571425\n",
      "Current iteration=970, the loss=23786431.053704236\n",
      "Current iteration=980, the loss=23786433.580865256\n",
      "Current iteration=990, the loss=23786437.71593904\n",
      "Current iteration=1000, the loss=23786442.829109076\n",
      "Current iteration=1010, the loss=23786448.33407407\n",
      "Current iteration=1020, the loss=23786453.71496096\n",
      "Current iteration=1030, the loss=23786458.541978925\n",
      "Current iteration=1040, the loss=23786462.476194657\n",
      "Current iteration=1050, the loss=23786465.26638939\n",
      "Current iteration=1060, the loss=23786466.741203204\n",
      "Current iteration=1070, the loss=23786466.79904888\n",
      "Current iteration=1080, the loss=23786465.397415984\n",
      "Current iteration=1090, the loss=23786462.54249679\n",
      "Current iteration=1100, the loss=23786458.279608823\n",
      "Current iteration=1110, the loss=23786452.684623376\n",
      "Current iteration=1120, the loss=23786445.856455393\n",
      "Current iteration=1130, the loss=23786437.91059791\n",
      "Current iteration=1140, the loss=23786428.97363758\n",
      "Current iteration=1150, the loss=23786419.178675577\n",
      "Current iteration=1160, the loss=23786408.661562793\n",
      "Current iteration=1170, the loss=23786397.557860345\n",
      "Current iteration=1180, the loss=23786386.000434346\n",
      "Current iteration=1190, the loss=23786374.117599882\n",
      "Current iteration=1200, the loss=23786362.03173376\n",
      "Current iteration=1210, the loss=23786349.858282093\n",
      "Current iteration=1220, the loss=23786337.705096442\n",
      "Current iteration=1230, the loss=23786325.67203938\n",
      "Current iteration=1240, the loss=23786313.850808587\n",
      "Current iteration=1250, the loss=23786302.32493258\n",
      "Current iteration=1260, the loss=23786291.169903204\n",
      "Current iteration=1270, the loss=23786280.453409776\n",
      "Current iteration=1280, the loss=23786270.23564932\n",
      "Current iteration=1290, the loss=23786260.569690518\n",
      "Current iteration=1300, the loss=23786251.501873232\n",
      "Current iteration=1310, the loss=23786243.072228387\n",
      "Current iteration=1320, the loss=23786235.314907994\n",
      "Current iteration=1330, the loss=23786228.258614667\n",
      "Current iteration=1340, the loss=23786221.927025203\n",
      "Current iteration=1350, the loss=23786216.33920117\n",
      "Current iteration=1360, the loss=23786211.509985037\n",
      "Current iteration=1370, the loss=23786207.45037752\n",
      "Current iteration=1380, the loss=23786204.16789492\n",
      "Current iteration=1390, the loss=23786201.66690528\n",
      "Current iteration=1400, the loss=23786199.94894431\n",
      "Current iteration=1410, the loss=23786199.013008792\n",
      "Current iteration=1420, the loss=23786198.85583035\n",
      "Current iteration=1430, the loss=23786199.472127527\n",
      "Current iteration=1440, the loss=23786200.854840115\n",
      "Current iteration=1450, the loss=23786202.99534326\n",
      "Current iteration=1460, the loss=23786205.883645058\n",
      "Current iteration=1470, the loss=23786209.508567378\n",
      "Current iteration=1480, the loss=23786213.857910294\n",
      "Current iteration=1490, the loss=23786218.918602813\n",
      "Current iteration=1500, the loss=23786224.67683983\n",
      "Current iteration=1510, the loss=23786231.11820539\n",
      "Current iteration=1520, the loss=23786238.227785688\n",
      "Current iteration=1530, the loss=23786245.990270052\n",
      "Current iteration=1540, the loss=23786254.390042163\n",
      "Current iteration=1550, the loss=23786263.411261708\n",
      "Current iteration=1560, the loss=23786273.037938155\n",
      "Current iteration=1570, the loss=23786283.25399557\n",
      "Current iteration=1580, the loss=23786294.04333086\n",
      "Current iteration=1590, the loss=23786305.389865495\n",
      "Current iteration=1600, the loss=23786317.27759022\n",
      "Current iteration=1610, the loss=23786329.690605436\n",
      "Current iteration=1620, the loss=23786342.61315548\n",
      "Current iteration=1630, the loss=23786356.029658448\n",
      "Current iteration=1640, the loss=23786369.92473259\n",
      "Current iteration=1650, the loss=23786384.283217605\n",
      "Current iteration=1660, the loss=23786399.090193696\n",
      "Current iteration=1670, the loss=23786414.330996744\n",
      "Current iteration=1680, the loss=23786429.99123071\n",
      "Current iteration=1690, the loss=23786446.05677812\n",
      "Current iteration=1700, the loss=23786462.51380734\n",
      "Current iteration=1710, the loss=23786479.348778836\n",
      "Current iteration=1720, the loss=23786496.54844861\n",
      "Current iteration=1730, the loss=23786514.09987096\n",
      "Current iteration=1740, the loss=23786531.99039925\n",
      "Current iteration=1750, the loss=23786550.207685634\n",
      "Current iteration=1760, the loss=23786568.739679754\n",
      "Current iteration=1770, the loss=23786587.57462598\n",
      "Current iteration=1780, the loss=23786606.70106106\n",
      "Current iteration=1790, the loss=23786626.1078095\n",
      "Current iteration=1800, the loss=23786645.783979397\n",
      "Current iteration=1810, the loss=23786665.71895768\n",
      "Current iteration=1820, the loss=23786685.90240445\n",
      "Current iteration=1830, the loss=23786706.32424767\n",
      "Current iteration=1840, the loss=23786726.97467677\n",
      "Current iteration=1850, the loss=23786747.844136912\n",
      "Current iteration=1860, the loss=23786768.92332255\n",
      "Current iteration=1870, the loss=23786790.203171194\n",
      "Current iteration=1880, the loss=23786811.674856793\n",
      "Current iteration=1890, the loss=23786833.329783555\n",
      "Current iteration=1900, the loss=23786855.159579724\n",
      "Current iteration=1910, the loss=23786877.156090967\n",
      "Current iteration=1920, the loss=23786899.31137449\n",
      "Current iteration=1930, the loss=23786921.617692687\n",
      "Current iteration=1940, the loss=23786944.06750778\n",
      "Current iteration=1950, the loss=23786966.653475624\n",
      "Current iteration=1960, the loss=23786989.368440334\n",
      "Current iteration=1970, the loss=23787012.205428988\n",
      "Current iteration=1980, the loss=23787035.157646485\n",
      "Current iteration=1990, the loss=23787058.218470756\n",
      "[[ -1.88716794e+02]\n",
      " [  7.67063825e+00]\n",
      " [ -1.14046365e+02]\n",
      " [ -1.64866105e+02]\n",
      " [  4.57208511e+01]\n",
      " [ -1.02855699e+02]\n",
      " [  6.61042128e+02]\n",
      " [ -1.26645542e+02]\n",
      " [  1.52967168e+02]\n",
      " [ -2.44962869e+01]\n",
      " [ -9.71023561e+01]\n",
      " [ -9.72906674e+01]\n",
      " [  3.43424268e+01]\n",
      " [ -1.05812696e+02]\n",
      " [  1.31733670e+02]\n",
      " [ -1.01510714e+00]\n",
      " [ -7.98199601e-01]\n",
      " [  1.60093839e+02]\n",
      " [  7.43879388e-02]\n",
      " [  1.20759434e+00]\n",
      " [  5.50049738e+01]\n",
      " [  8.58092238e-01]\n",
      " [ -4.46280197e+01]\n",
      " [ -2.40283796e+02]\n",
      " [  6.87370158e+01]\n",
      " [  5.07432323e+01]\n",
      " [  4.97718210e+01]\n",
      " [ -1.17206797e+02]\n",
      " [ -1.12183792e+02]\n",
      " [ -1.14869019e+02]\n",
      " [ -1.80788397e+02]]\n"
     ]
    }
   ],
   "source": [
    "from logistic_regression import *\n",
    "from helpers import standardize\n",
    "from costs import *\n",
    "\n",
    "# There are two parameters, lambda_ and gamma, where gamma is the step size \n",
    "\n",
    "max_iter = 2000\n",
    "lambdas = np.arange(0.1, 0.4, 0.1)\n",
    "gammas = np.arange(0.001, 0.01, 0.001)\n",
    "\n",
    "# When lambda is 0, reg_logistic_regression is naive non-penalized logistic_regression.\n",
    "\n",
    "std_tx, _, _ = standardize(tX)\n",
    "print(std_tx)\n",
    "\n",
    "# struct = dict()\n",
    "# for lambda_ in lambdas:\n",
    "#     for gamma in gammas:\n",
    "weights = reg_logistic_regression(y, std_tx, lambdas[0], gammas[0], max_iter)\n",
    "# err = compute_loss(y, tX, w)\n",
    "# struct[(gamma, lambda_)] = (w, err)\n",
    "# #         break\n",
    "print(weights)\n",
    "# for (gamma, lambda_), (w, err) in struct.items():\n",
    "#     print(\"Gamma: \", gamma, \" Lamdba: \", lambda_, \" w: \", w, \"error: \", err)\n",
    "        \n",
    "        \n",
    "\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a\n",
    "## Generate predictions and save ouput in csv format for submission:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "DATA_TEST_PATH = '../../data/test.csv' \n",
    "_, tX_test, ids_test = load_csv_data(DATA_TEST_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "shapes (568238,30) and (31,1) not aligned: 30 (dim 1) != 31 (dim 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-24bb795c4e35>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mOUTPUT_PATH\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'../../data/output.csv'\u001b[0m \u001b[0;31m# TODO: fill in desired name of output file for submission\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpredict_labels\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mcreate_csv_submission\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mids_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mOUTPUT_PATH\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/chenfs/Google Drive/EPFL/Courses/PC and ML/ML_course_origin/ML_course/projects/project1/src/python/proj1_helpers.py\u001b[0m in \u001b[0;36mpredict_labels\u001b[0;34m(weights, data)\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mpredict_labels\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0;34m\"\"\"Generates class predictions given weights, and a test data matrix\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m     \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m     \u001b[0my_pred\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwhere\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0my_pred\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwhere\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: shapes (568238,30) and (31,1) not aligned: 30 (dim 1) != 31 (dim 0)"
     ]
    }
   ],
   "source": [
    "OUTPUT_PATH = '../../data/output.csv' # TODO: fill in desired name of output file for submission\n",
    "y_pred = predict_labels(weights, tX_test)\n",
    "create_csv_submission(ids_test, y_pred, OUTPUT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
