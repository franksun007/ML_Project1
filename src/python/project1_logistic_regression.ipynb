{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Useful starting lines\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the training data into feature matrix, class labels, and event ids:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from proj1_helpers import *\n",
    "DATA_TRAIN_PATH = '../../data/train.csv'\n",
    "y, tX, ids = load_csv_data(DATA_TRAIN_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Do your thing crazy machine learning thing here :) ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from logistic_regression import *\n",
    "from helpers import *\n",
    "from costs import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current iteration=0, the loss=173286.79513998624\n",
      "Current iteration=10, the loss=17228101405644.5\n",
      "Current iteration=20, the loss=22873190303267.0\n",
      "Current iteration=30, the loss=36290382221133.85\n",
      "Current iteration=40, the loss=12723255054628.037\n",
      "Current iteration=50, the loss=19530506805927.344\n",
      "Current iteration=60, the loss=29551274200062.695\n",
      "Current iteration=70, the loss=22395900757354.58\n",
      "Current iteration=80, the loss=29183024987702.574\n",
      "Current iteration=90, the loss=41832730591862.234\n",
      "Current iteration=100, the loss=17085293750945.066\n",
      "Current iteration=110, the loss=7083068358157.868\n",
      "Current iteration=120, the loss=25053973123375.754\n",
      "Current iteration=130, the loss=39722756244533.945\n",
      "Current iteration=140, the loss=7007459647135.056\n",
      "Current iteration=150, the loss=29487047052114.234\n",
      "Current iteration=160, the loss=5951842617631.351\n",
      "Current iteration=170, the loss=24833077862939.465\n",
      "Current iteration=180, the loss=13950272750864.822\n",
      "Current iteration=190, the loss=13734084810107.46\n",
      "Current iteration=200, the loss=28614709571367.34\n",
      "Current iteration=210, the loss=6627142340844.551\n",
      "Current iteration=220, the loss=8223431301755.377\n",
      "Current iteration=230, the loss=8888681050895.71\n",
      "Current iteration=240, the loss=28517838880355.562\n",
      "Current iteration=250, the loss=5577969565196.15\n",
      "Current iteration=260, the loss=16487734913101.875\n",
      "Current iteration=270, the loss=13566254407237.193\n",
      "Current iteration=280, the loss=26930558475259.3\n",
      "Current iteration=290, the loss=28363352209278.78\n",
      "Current iteration=300, the loss=34823539785094.957\n",
      "Current iteration=310, the loss=28901228948094.605\n",
      "Current iteration=320, the loss=28933744395063.83\n",
      "Current iteration=330, the loss=28333326800952.47\n",
      "Current iteration=340, the loss=34334714335977.965\n",
      "Current iteration=350, the loss=4680655831329.682\n",
      "Current iteration=360, the loss=4426068325321.524\n",
      "Current iteration=370, the loss=8910656579356.312\n",
      "Current iteration=380, the loss=3486297600849.0737\n",
      "Current iteration=390, the loss=11530046025887.027\n",
      "Current iteration=400, the loss=10572038062984.227\n",
      "Current iteration=410, the loss=11225991806950.947\n",
      "Current iteration=420, the loss=29325249338270.62\n",
      "Current iteration=430, the loss=12341723767622.83\n",
      "Current iteration=440, the loss=28956762963107.742\n",
      "Current iteration=450, the loss=28778782446840.457\n",
      "Current iteration=460, the loss=27746836565127.6\n",
      "Current iteration=470, the loss=34698542607355.746\n",
      "Current iteration=480, the loss=33533679006990.113\n",
      "Current iteration=490, the loss=4602898944780.331\n",
      "Current iteration=500, the loss=9165988199519.586\n",
      "Current iteration=510, the loss=17746091459718.156\n",
      "Current iteration=520, the loss=18648020215635.09\n",
      "Current iteration=530, the loss=20277356624978.984\n",
      "Current iteration=540, the loss=34720514480481.42\n",
      "Current iteration=550, the loss=32331842159876.836\n",
      "Current iteration=560, the loss=35808385829310.664\n",
      "Current iteration=570, the loss=24306337081726.164\n",
      "Current iteration=580, the loss=32661995760198.56\n",
      "Current iteration=590, the loss=5608955711628.082\n",
      "Current iteration=600, the loss=4109223671806.2876\n",
      "Current iteration=610, the loss=20806952251737.207\n",
      "Current iteration=620, the loss=30102444042933.004\n",
      "Current iteration=630, the loss=4484324772060.076\n",
      "Current iteration=640, the loss=2677727174988.888\n",
      "Current iteration=650, the loss=14432346935413.086\n",
      "Current iteration=660, the loss=29542048828096.754\n",
      "Current iteration=670, the loss=35448795204812.89\n",
      "Current iteration=680, the loss=5090989367225.476\n",
      "Current iteration=690, the loss=29926114356229.594\n",
      "Current iteration=700, the loss=8937234172055.988\n",
      "Current iteration=710, the loss=30824532928440.473\n",
      "Current iteration=720, the loss=10963025949966.22\n",
      "Current iteration=730, the loss=28287322841733.074\n",
      "Current iteration=740, the loss=21841203362263.527\n",
      "Current iteration=750, the loss=31416745160081.297\n",
      "Current iteration=760, the loss=28538077807805.895\n",
      "Current iteration=770, the loss=27861162888721.336\n",
      "Current iteration=780, the loss=33884309508246.01\n",
      "Current iteration=790, the loss=4627819226932.585\n",
      "Current iteration=800, the loss=5298267864842.776\n",
      "Current iteration=810, the loss=3306957517074.9077\n",
      "Current iteration=820, the loss=1995103157163.0662\n",
      "Current iteration=830, the loss=10962882607271.686\n",
      "Current iteration=840, the loss=4740176808617.795\n",
      "Current iteration=850, the loss=23609086424404.22\n",
      "Current iteration=860, the loss=35499442841933.08\n",
      "Current iteration=870, the loss=23643979154443.4\n",
      "Current iteration=880, the loss=23738984126834.78\n",
      "Current iteration=890, the loss=32701842069970.242\n",
      "Current iteration=900, the loss=6753743887335.934\n",
      "Current iteration=910, the loss=5033228340251.396\n",
      "Current iteration=920, the loss=34516948815005.77\n",
      "Current iteration=930, the loss=5628050829098.125\n",
      "Current iteration=940, the loss=27583634835006.266\n",
      "Current iteration=950, the loss=10963297502388.967\n",
      "Current iteration=960, the loss=25327885713315.89\n",
      "Current iteration=970, the loss=30129436999561.89\n",
      "Current iteration=980, the loss=25676329728886.504\n",
      "Current iteration=990, the loss=6286670430904.606\n",
      "Current iteration=1000, the loss=18226115880266.91\n",
      "Current iteration=1010, the loss=27262309905096.93\n",
      "Current iteration=1020, the loss=4768392696240.076\n",
      "Current iteration=1030, the loss=25670402603706.39\n",
      "Current iteration=1040, the loss=14643230158380.613\n",
      "Current iteration=1050, the loss=22463612787574.895\n",
      "Current iteration=1060, the loss=33371818516662.28\n",
      "Current iteration=1070, the loss=35018204135019.06\n",
      "Current iteration=1080, the loss=8356314421415.627\n",
      "Current iteration=1090, the loss=14891528574306.592\n",
      "Current iteration=1100, the loss=12970608526972.33\n",
      "Current iteration=1110, the loss=27269963099060.184\n",
      "Current iteration=1120, the loss=4398695725439.191\n",
      "Current iteration=1130, the loss=20731757556418.543\n",
      "Current iteration=1140, the loss=34771807878773.617\n",
      "Current iteration=1150, the loss=5206350530209.746\n",
      "Current iteration=1160, the loss=37304293450378.02\n",
      "Current iteration=1170, the loss=9419045820638.451\n",
      "Current iteration=1180, the loss=25448999860284.918\n",
      "Current iteration=1190, the loss=6439975264186.711\n",
      "Current iteration=1200, the loss=3178572613368.779\n",
      "Current iteration=1210, the loss=5544397367595.627\n",
      "Current iteration=1220, the loss=2936797006973.751\n",
      "Current iteration=1230, the loss=12038963847365.564\n",
      "Current iteration=1240, the loss=9025982096570.967\n",
      "Current iteration=1250, the loss=29291265755916.598\n",
      "Current iteration=1260, the loss=29509028527843.12\n",
      "Current iteration=1270, the loss=24864120366930.492\n",
      "Current iteration=1280, the loss=28025867781391.78\n",
      "Current iteration=1290, the loss=34468842077427.047\n",
      "Current iteration=1300, the loss=33760704165372.53\n",
      "Current iteration=1310, the loss=6217666917143.994\n",
      "Current iteration=1320, the loss=28420854281071.15\n",
      "Current iteration=1330, the loss=31368388583883.516\n",
      "Current iteration=1340, the loss=27158403903099.84\n",
      "Current iteration=1350, the loss=7276176245847.446\n",
      "Current iteration=1360, the loss=35638161954493.25\n",
      "Current iteration=1370, the loss=4497591440749.295\n",
      "Current iteration=1380, the loss=16436691471103.533\n",
      "Current iteration=1390, the loss=12510905475023.67\n",
      "Current iteration=1400, the loss=26861688355448.17\n",
      "Current iteration=1410, the loss=12337107276871.75\n",
      "Current iteration=1420, the loss=15370641834443.967\n",
      "Current iteration=1430, the loss=32365816710082.477\n",
      "Current iteration=1440, the loss=7585450761803.799\n",
      "Current iteration=1450, the loss=3326692648312.8516\n",
      "Current iteration=1460, the loss=12670345532268.594\n",
      "Current iteration=1470, the loss=27594716593099.94\n",
      "Current iteration=1480, the loss=8958403622143.23\n",
      "Current iteration=1490, the loss=27422552139167.5\n",
      "Current iteration=1500, the loss=6338770092761.458\n",
      "Current iteration=1510, the loss=34806532634358.844\n",
      "Current iteration=1520, the loss=4422269481650.151\n",
      "Current iteration=1530, the loss=35174132513851.742\n",
      "Current iteration=1540, the loss=9318044887150.012\n",
      "Current iteration=1550, the loss=23782063978789.13\n",
      "Current iteration=1560, the loss=32032613905635.836\n",
      "Current iteration=1570, the loss=4933326459432.534\n",
      "Current iteration=1580, the loss=25211935623596.754\n",
      "Current iteration=1590, the loss=11139872613338.484\n",
      "Current iteration=1600, the loss=23622492058454.52\n",
      "Current iteration=1610, the loss=27971410912737.65\n",
      "Current iteration=1620, the loss=6389800813244.083\n",
      "Current iteration=1630, the loss=20605935997063.22\n",
      "Current iteration=1640, the loss=28082291955765.836\n",
      "Current iteration=1650, the loss=5963452134508.896\n",
      "Current iteration=1660, the loss=26146745047140.254\n",
      "Current iteration=1670, the loss=27565817630573.61\n",
      "Current iteration=1680, the loss=13933764488039.809\n",
      "Current iteration=1690, the loss=31987093411799.508\n",
      "Current iteration=1700, the loss=11931221056670.656\n",
      "Current iteration=1710, the loss=13425007411083.506\n",
      "Current iteration=1720, the loss=28684743618175.812\n",
      "Current iteration=1730, the loss=13561895722244.445\n",
      "Current iteration=1740, the loss=22357956320428.46\n",
      "Current iteration=1750, the loss=29720918821579.625\n",
      "Current iteration=1760, the loss=6297024480927.478\n",
      "Current iteration=1770, the loss=22656978005739.637\n",
      "Current iteration=1780, the loss=30074901333958.37\n",
      "Current iteration=1790, the loss=6971615433665.602\n",
      "Current iteration=1800, the loss=20910779649272.633\n",
      "Current iteration=1810, the loss=30077399334384.77\n",
      "Current iteration=1820, the loss=6229339498521.001\n",
      "Current iteration=1830, the loss=24279655590813.547\n",
      "Current iteration=1840, the loss=33806918291844.453\n",
      "Current iteration=1850, the loss=7253185600431.574\n",
      "Current iteration=1860, the loss=21043652334690.848\n",
      "Current iteration=1870, the loss=31639481390932.85\n",
      "Current iteration=1880, the loss=6429005591999.934\n",
      "Current iteration=1890, the loss=21058853189592.77\n",
      "Current iteration=1900, the loss=28391313197311.914\n",
      "Current iteration=1910, the loss=7081735621124.696\n",
      "Current iteration=1920, the loss=24265318472538.973\n",
      "Current iteration=1930, the loss=29146855402927.145\n",
      "Current iteration=1940, the loss=7097630648161.215\n",
      "Current iteration=1950, the loss=9748810449201.064\n",
      "Current iteration=1960, the loss=34836819681542.93\n",
      "Current iteration=1970, the loss=4159200962433.902\n",
      "Current iteration=1980, the loss=13202479761248.469\n",
      "Current iteration=1990, the loss=8550105367586.954\n",
      "[[  1.92681161e+05]\n",
      " [ -8.23579045e+05]\n",
      " [ -2.56738460e+05]\n",
      " [  7.75098494e+04]\n",
      " [  5.38374541e+04]\n",
      " [  1.10569361e+05]\n",
      " [  6.13003084e+04]\n",
      " [  6.11745225e+03]\n",
      " [  2.55810899e+03]\n",
      " [  1.08578146e+05]\n",
      " [ -2.09476996e+04]\n",
      " [  2.03189447e+04]\n",
      " [  6.07369994e+04]\n",
      " [  4.12697091e+05]\n",
      " [ -4.34530637e+02]\n",
      " [ -1.60832856e+03]\n",
      " [ -1.10311660e+05]\n",
      " [ -5.19469046e+02]\n",
      " [  1.76453128e+03]\n",
      " [ -8.40343459e+04]\n",
      " [  1.23731832e+03]\n",
      " [ -2.99659644e+04]\n",
      " [ -5.00352874e+03]\n",
      " [  6.46784420e+04]\n",
      " [  7.97374241e+04]\n",
      " [  8.00047363e+04]\n",
      " [ -2.57834411e+04]\n",
      " [  5.99462788e+04]\n",
      " [  5.95372315e+04]\n",
      " [ -1.93807111e+05]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# There are two parameters, lambda_ and gamma, where gamma is the step size \n",
    "\n",
    "max_iter = 2000\n",
    "lambdas = np.arange(0.1, 0.4, 0.1)\n",
    "gammas = np.arange(0.001, 0.01, 0.001)\n",
    "\n",
    "# When lambda is 0, reg_logistic_regression is naive non-penalized logistic_regression.\n",
    "\n",
    "# std_tx, _, _ = standardize(tX)\n",
    "# print(std_tx)\n",
    "\n",
    "# struct = dict()\n",
    "# for lambda_ in lambdas:\n",
    "#     for gamma in gammas:\n",
    "weights = reg_logistic_regression(y, tX, lambdas[0], gammas[0], max_iter)\n",
    "# err = compute_loss(y, tX, w)\n",
    "# struct[(gamma, lambda_)] = (w, err)\n",
    "# #         break\n",
    "print(weights)\n",
    "# for (gamma, lambda_), (w, err) in struct.items():\n",
    "#     print(\"Gamma: \", gamma, \" Lamdba: \", lambda_, \" w: \", w, \"error: \", err)\n",
    "        \n",
    "        \n",
    "\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.68692\n"
     ]
    }
   ],
   "source": [
    "\n",
    "compare_pred = predict_labels(weights, tX) - y\n",
    "\n",
    "nonzero = 0\n",
    "for i in range(len(compare_pred)):\n",
    "    if (compare_pred[i] != 0):\n",
    "        nonzero += 1\n",
    "\n",
    "print(1 - nonzero / compare_pred.size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate predictions and save ouput in csv format for submission:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "DATA_TEST_PATH = '../../data/test.csv' \n",
    "_, tX_test, ids_test = load_csv_data(DATA_TEST_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "OUTPUT_PATH = '../../data/output.csv' # TODO: fill in desired name of output file for submission\n",
    "y_pred = predict_labels(weights, tX_test)\n",
    "create_csv_submission(ids_test, y_pred, OUTPUT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
