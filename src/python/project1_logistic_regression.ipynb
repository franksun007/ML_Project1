{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# Useful starting lines\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the training data into feature matrix, class labels, and event ids:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from proj1_helpers import *\n",
    "DATA_TRAIN_PATH = '../../data/train.csv'\n",
    "y, tX, ids = load_csv_data(DATA_TRAIN_PATH)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Do your thing crazy machine learning thing here :) ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def build_poly(x, degree):\n",
    "    \n",
    "    matrix = np.zeros((x.shape[0], x.shape[1] * (degree + 1)))\n",
    "    \n",
    "    for i in range(degree + 1):\n",
    "        matrix[:, (i * x.shape[1]) : ((i + 1) * x.shape[1])] = (x ** i)[:]\n",
    "    return matrix\n",
    "\n",
    "\n",
    "def build_k_indices(y, k_fold, seed=62):\n",
    "    \"\"\"build k indices for k-fold.\"\"\"\n",
    "    num_row = y.shape[0]\n",
    "    interval = int(num_row / k_fold)\n",
    "    np.random.seed(seed)\n",
    "    indices = np.random.permutation(num_row)\n",
    "    k_indices = [indices[k * interval: (k + 1) * interval]\n",
    "                 for k in range(k_fold)]\n",
    "    return np.array(k_indices)\n",
    "\n",
    "def batch_iter(y, tx, batch_size, num_batches=None, shuffle=True):\n",
    "    \"\"\"\n",
    "    Generate a minibatch iterator for a dataset.\n",
    "    Takes as input two iterables (here the output desired values 'y' and the input data 'tx')\n",
    "    Outputs an iterator which gives mini-batches of `batch_size` matching elements from `y` and `tx`.\n",
    "    Data can be randomly shuffled to avoid ordering in the original data messing with the randomness of the minibatches.\n",
    "    Example of use :\n",
    "    for minibatch_y, minibatch_tx in batch_iter(y, tx, 32):\n",
    "        <DO-SOMETHING>\n",
    "    \"\"\"\n",
    "\n",
    "    data_size = len(y)\n",
    "    num_batches_max = int(np.ceil(data_size/batch_size))\n",
    "    if num_batches is None:\n",
    "        num_batches = num_batches_max\n",
    "    else:\n",
    "        num_batches = min(num_batches, num_batches_max)\n",
    "\n",
    "    if shuffle:\n",
    "        shuffle_indices = np.random.permutation(np.arange(data_size))\n",
    "        shuffled_y = y[shuffle_indices]\n",
    "        shuffled_tx = tx[shuffle_indices]\n",
    "    else:\n",
    "        shuffled_y = y\n",
    "        shuffled_tx = tx\n",
    "    for batch_num in range(num_batches):\n",
    "        start_index = batch_num * batch_size\n",
    "        end_index = min((batch_num + 1) * batch_size, data_size)\n",
    "        if start_index != end_index:\n",
    "            yield shuffled_y[start_index:end_index], shuffled_tx[start_index:end_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "BINARY_CLASSIFICATOIN_0 = -1\n",
    "BINARY_CLASSIFICATOIN_1 = 1\n",
    "\n",
    "\n",
    "def sigmoid(t):\n",
    "    \"\"\"apply sigmoid function on t.\"\"\"\n",
    "    return 1.0 / (1.0 + np.exp(-t))\n",
    "\n",
    "\n",
    "def calculate_loss_logistic_regression(y, tx, w):\n",
    "    \"\"\"compute the cost by negative log likelihood.\"\"\"\n",
    "    prediction = tx @ w\n",
    "    \n",
    "    y1 = np.where(y == BINARY_CLASSIFICATOIN_1)\n",
    "\n",
    "    over_700 = np.where(prediction >= 700)\n",
    "\n",
    "    prediction_result = np.log(1 + np.exp(prediction))\n",
    "    prediction_result[over_700] = prediction[over_700]\n",
    "    prediction_result[y1] -= prediction[y1]\n",
    "    \n",
    "    result = np.sum(prediction_result)\n",
    "    return result\n",
    "\n",
    "\n",
    "def calculate_gradient_logistic_regression(y, tx, w):\n",
    "    \"\"\"compute the gradient of loss.\"\"\"\n",
    "\n",
    "    y1 = np.where(y == BINARY_CLASSIFICATOIN_1)\n",
    "    sig = sigmoid(tx @ w).reshape(len(y))\n",
    "    sig[y1] -= y[y1]\n",
    "\n",
    "    return (tx.T @ sig).reshape((tx.shape[1], 1))\n",
    "\n",
    "\n",
    "def calculate_hessian_logistic_regression(y, tx, w):\n",
    "    \"\"\"return the hessian of the loss function.\"\"\"\n",
    "    Snn = (sigmoid(tx @ w) * (1 - sigmoid(tx @ w)))\n",
    "    \n",
    "    \n",
    "    Snn = Snn.reshape(Snn.shape[0])\n",
    "    S = np.diag(Snn)\n",
    "    return (tx.T @ S @ tx).reshape((tx.shape[1], tx.shape[1]))\n",
    "    \n",
    "    \n",
    "def line_search_gamma(loss, loss_prev, gamma):\n",
    "    if (loss > loss_prev):\n",
    "        gamma = gamma / 1.5\n",
    "    return gamma\n",
    "    \n",
    "\n",
    "def logistic_regression_helper(y, tx, gamma, max_iters, lambda_):\n",
    "\n",
    "    w = np.zeros((tx.shape[1], 1))\n",
    "    threshold = 1e-8\n",
    "    loss_prev = 0\n",
    "    w_max = w\n",
    "    perf = 0\n",
    "    i = 0\n",
    "    \n",
    "    batch_size = len(y) / 5\n",
    "    counter = 0\n",
    "\n",
    "    for iter in range(max_iters):\n",
    "                \n",
    "#         for mini_y, mini_x in batch_iter(y, tx, batch_size):\n",
    "        loss = calculate_loss_logistic_regression(y, tx, w) + lambda_ * np.linalg.norm(w, 2)\n",
    "        gradient = calculate_gradient_logistic_regression(y, tx, w)\n",
    "#         loss = calculate_loss_logistic_regression(mini_y, mini_x, w) + lambda_ * np.linalg.norm(w, 2)\n",
    "#         gradient = calculate_gradient_logistic_regression(mini_y, mini_x, w)\n",
    "#             hessian = calculate_hessian_logistic_regression(mini_y, mini_x, w)\n",
    "\n",
    "        w -= gradient * gamma\n",
    "\n",
    "        if (loss_prev != 0) and np.abs(loss_prev - loss) < threshold:\n",
    "            print(\"Reached Theshold, exit\")\n",
    "            break\n",
    "        \n",
    "        gamma = line_search_gamma(loss, loss_prev, gamma)\n",
    "            \n",
    "        loss_prev = loss\n",
    "        \n",
    "#         if (iter > 0 and iter < 10000 and (iter % 3000) == 0):\n",
    "#             gamma = gamma / 10\n",
    "#             print(\"Gamma / 10\")\n",
    "        \n",
    "#         if ((iter >= 10000 and iter <= 24000) and (iter % 1000) == 0):\n",
    "#             counter -= 1\n",
    "#             if (np.abs(loss_prev - loss) > 5000):\n",
    "#                 gamma = gamma / 10\n",
    "#                 print(\"Gamma / 10\")\n",
    "\n",
    "#             if (np.abs(loss_prev - loss) < 1000 and counter == 0):\n",
    "#                 counter = 3\n",
    "#                 gamma = gamma * 2\n",
    "#                 print(\"Gamma * 2\")\n",
    "\n",
    "        \n",
    "#         if ((iter >= 25000) and (iter % 100) == 0):\n",
    "#             counter -= 1\n",
    "#             if (np.abs(loss_prev - loss) > 10000):\n",
    "#                 gamma = gamma / 10\n",
    "#                 print(\"Gamma / 10\")\n",
    "\n",
    "#             if (np.abs(loss_prev - loss) < 1000 and counter == 0):\n",
    "#                 counter = 3\n",
    "#                 gamma = gamma * 2\n",
    "#                 print(\"Gamma * 2\")\n",
    "            \n",
    "        \n",
    "#         if (iter % 10) == 0:\n",
    "#             cur_perf = performance(w, y, tx)\n",
    "#             if cur_perf >= perf:\n",
    "#                 w_max = w\n",
    "#                 perf = cur_perf\n",
    "#                 i = iter\n",
    "\n",
    "\n",
    "        if (iter % 100) == 0:\n",
    "            print(\"Gamma: \", gamma)\n",
    "            print(\"Current iteration={i}, the loss={l}\".format(i=iter, l=loss))\n",
    "\n",
    "#         if (iter % 300) == 0:\n",
    "#             print(w_max)\n",
    "#             print(\"Performance: \", perf)\n",
    "#             print(\"Iteration: \", i)\n",
    "            \n",
    "\n",
    "    return w\n",
    "\n",
    "\n",
    "def logistic_regression(y, tx, gamma, max_iters):\n",
    "    \"\"\" return the final w from the logistic regression \"\"\"\n",
    "    return logistic_regression_helper(y, tx, gamma, max_iters, lambda_=0)\n",
    "\n",
    "\n",
    "def reg_logistic_regression(y, tx, lambda_, gamma, max_iters):\n",
    "    \"\"\" return the final w from the penalized logistic regression, with lambda_ as a non 0 value\"\"\"\n",
    "    return logistic_regression_helper(y, tx, gamma, max_iters, lambda_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def performance(weights, y, xT):\n",
    "    \"\"\"Returns the percentage of successful classifications for the weights,\n",
    "    given the expected results (y) and data (xT)\"\"\"\n",
    "    from proj1_helpers import predict_labels\n",
    "    compare_pred = predict_labels(weights, xT)\n",
    "    compare_pred -= y.reshape((len(y), 1))\n",
    "        \n",
    "    non_zero = 0\n",
    "    for i in range(len(compare_pred)):\n",
    "        if compare_pred[i] != 0:\n",
    "            non_zero += 1\n",
    "            \n",
    "    return 1 - non_zero / compare_pred.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "def plot_feature_relationship(y, tX):\n",
    "\n",
    "    std_tx = standardize(tX)\n",
    "\n",
    "    row = np.zeros(std_tx.shape[0])\n",
    "\n",
    "    # for i in range(len(tX)):\n",
    "    #     row[i] = tX[i][0]\n",
    "\n",
    "    plt.figure(figsize=(20, 20))\n",
    "\n",
    "    for j in range(std_tx.shape[1]):\n",
    "        for i in range(len(std_tx)):\n",
    "            row[i] = std_tx[i][j]\n",
    "        plt.subplot(5, 6, j + 1)\n",
    "        plt.title(j)\n",
    "        plt.plot(row[np.where(y == 1)], y[np.where(y == 1)], 'ro')\n",
    "        plt.plot(row[np.where(y == -1)], y[np.where(y == -1)], 'bo')\n",
    "  \n",
    "\n",
    "    plt.tight_layout()\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "# plot_feature_relationship(y, tX)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "# def standardize(x):\n",
    "# #     cols = [4, 5, 6, 12, 26, 28]\n",
    "# #     for i in range(len(x)):\n",
    "# #         x[i][np.where(x[i] == -999)] = 0\n",
    "\n",
    "#     # Combine feature, or do poly expansion\n",
    "    \n",
    "#     # Do not normalize the jet num\n",
    "#     jet_num = x[:, jet_num_col]\n",
    "    \n",
    "#     # Replace -999 with some value that is the mean/median of the represent dataset \n",
    "#     for i in range(x.shape[1]):\n",
    "#         median = np.median(x[np.where(x[:, i] != -999), i])\n",
    "#         x[np.where(x[:, i] == -999), i] = median \n",
    "#         x[np.where(x[:, i] != -999), i] = x[np.where(x[:, i] != -999), i] - np.mean(x[np.where(x[:, i] != -999), i])\n",
    "    \n",
    "#     mean_x = np.mean(x, axis=0)\n",
    "#     x = x - mean_x\n",
    "    \n",
    "#     std_x = np.std(x, axis=0)\n",
    "#     x[:, std_x > 0] = x[:, std_x > 0] / std_x[std_x > 0]\n",
    "\n",
    "#     x[:, jet_num_col] = jet_num\n",
    "    \n",
    "#     return x\n",
    "\n",
    "# print(standardize(tX) - tX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# def standardize_01(x):\n",
    "# #     cols = [4, 5, 6, 12, 26, 28]\n",
    "# #     for i in range(len(x)):\n",
    "# #         x[i][np.where(x[i] == -999)] = 0\n",
    "\n",
    "#     # Combine feature, or do poly expansion\n",
    "    \n",
    "#     # Do not normalize the jet num\n",
    "    \n",
    "#     # Replace -999 with some value that is the mean/median of the represent dataset \n",
    "#     for i in range(x.shape[1]):\n",
    "#         x[np.where(x[:, i] == -999), i] = 1\n",
    "    \n",
    "#     mean_x = np.mean(x, axis=0)\n",
    "#     x = x - mean_x\n",
    "    \n",
    "#     std_x = np.std(x, axis=0)\n",
    "#     x[:, std_x > 0] = x[:, std_x > 0] / std_x[std_x > 0]\n",
    "\n",
    "#     x[:, jet_num_col] = 1\n",
    "    \n",
    "#     return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def standardize_0123_helper(x):\n",
    "    for i in range(x.shape[1]):\n",
    "        mean = np.mean(x[np.where(x[:, i] != -999), i])\n",
    "        x[np.where(x[:, i] == -999), i] = mean \n",
    "        x[np.where(x[:, i] != -999), i] = x[np.where(x[:, i] != -999), i] - mean\n",
    "    \n",
    "    std_x = np.std(x, axis=0)\n",
    "    x[:, std_x > 0] = x[:, std_x > 0] / std_x[std_x > 0]\n",
    "    \n",
    "    return x\n",
    "\n",
    "\n",
    "def standardize_0(x):\n",
    "    feature_left = np.array([0, 1, 2, 3, 7, 8, 9, 10, 11, 13, 14, 15, 16, 17, 18, 19, 20, 21])\n",
    "    left_x = np.zeros((x.shape[0], len(feature_left)))\n",
    "    left_x[:, :] = x[:, feature_left]\n",
    "    return standardize_0123_helper(left_x)\n",
    "    \n",
    "    \n",
    "\n",
    "def standardize_1(x):\n",
    "    feature_left = np.array([0, 1, 2, 3, 7, 8, 9, 10, 11, 13, 14, 15, 16, 17, 18, 19, 20, 21, 23, 24, 25, 29])\n",
    "    left_x = np.zeros((x.shape[0], len(feature_left)))\n",
    "    left_x[:, :] = x[:, feature_left]\n",
    "    return standardize_0123_helper(left_x)\n",
    "    \n",
    "    \n",
    "def standardize_23(x):\n",
    "    feature_left = np.delete(np.arange(30), 22)\n",
    "    left_x = np.zeros((x.shape[0], len(feature_left)))\n",
    "    left_x[:, :] = x[:, feature_left]\n",
    "    return standardize_0123_helper(left_x)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "jet_num_col = 22\n",
    "\n",
    "\n",
    "def split_dataset_wrt22(x):\n",
    "    x_22_0 = np.where(x[:, jet_num_col] == 0)\n",
    "    x_22_1 = np.where(x[:, jet_num_col] == 1)\n",
    "    x_22_23 = np.where(x[:, jet_num_col] >= 2)\n",
    "    return x_22_0, x_22_1, x_22_23\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# There are two parameters, lambda_ and gamma, where gamma is the step size \n",
    "\n",
    "# max_iter = 15000\n",
    "# lambdas = np.arange(0.1, 0.4, 0.1)\n",
    "# gammas = np.array([0.004])\n",
    "\n",
    "# degree = 3\n",
    "\n",
    "# i_0, i_1, i_23 = split_dataset_wrt22(tX)\n",
    "\n",
    "# tx_0 =  tX[i_0]\n",
    "# y_0 =   y[i_0]\n",
    "# tx_1 =  tX[i_1]\n",
    "# y_1 =   y[i_1]\n",
    "# tx_23 = tX[i_23]\n",
    "# y_23 =  y[i_23]\n",
    "\n",
    "# std_tx_0 = standardize_01(tx_0)\n",
    "# std_tx_1 = standardize_01(tx_1)\n",
    "# std_tx_23 = standardize(tx_23)\n",
    "\n",
    "# matrix_std_tx_0 = build_poly(std_tx_0, degree)\n",
    "# matrix_std_tx_1 = build_poly(std_tx_1, degree)\n",
    "# matrix_std_tx_23 = build_poly(std_tx_23, degree)\n",
    "\n",
    "\n",
    "# print(std_tx)\n",
    "# print(tX)\n",
    "\n",
    "# k_fold = 10\n",
    "# k_indices = build_k_indices(y, k_fold)\n",
    "    \n",
    "# k = 2\n",
    "\n",
    "# y_test = y[k_indices[k]]\n",
    "# x_test = std_tx[k_indices[k]]\n",
    "# train_indices = k_indices[~(np.arange(k_indices.shape[0]) == k)]\n",
    "# y_train = np.concatenate(y[train_indices])\n",
    "# x_train = np.concatenate(std_tx[train_indices])\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "max_iter = 35000\n",
    "lambdas = np.arange(0.1, 0.4, 0.1)\n",
    "gammas = np.array([0.005])\n",
    "\n",
    "degree = 2\n",
    "\n",
    "i_0, i_1, i_23 = split_dataset_wrt22(tX)\n",
    "\n",
    "tx_0 =  tX[i_0]\n",
    "y_0 =   y[i_0]\n",
    "tx_1 =  tX[i_1]\n",
    "y_1 =   y[i_1]\n",
    "tx_23 = tX[i_23]\n",
    "y_23 =  y[i_23]\n",
    "\n",
    "std_tx_0 = standardize_0(tx_0)\n",
    "std_tx_1 = standardize_1(tx_1)\n",
    "std_tx_23 = standardize_23(tx_23)\n",
    "\n",
    "matrix_std_tx_0 = build_poly(std_tx_0, degree)\n",
    "matrix_std_tx_1 = build_poly(std_tx_1, degree)\n",
    "matrix_std_tx_23 = build_poly(std_tx_23, degree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gamma:  0.00333333333333\n",
      "Current iteration=0, the loss=69254.41425128581\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chenfs/anaconda/lib/python3.5/site-packages/ipykernel/__main__.py:7: RuntimeWarning: overflow encountered in exp\n",
      "/Users/chenfs/anaconda/lib/python3.5/site-packages/ipykernel/__main__.py:18: RuntimeWarning: overflow encountered in exp\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gamma:  8.67076495792e-05\n",
      "Current iteration=100, the loss=907432.3983842771\n",
      "Gamma:  3.85367331463e-05\n",
      "Current iteration=200, the loss=579572.0798688746\n",
      "Gamma:  3.85367331463e-05\n",
      "Current iteration=300, the loss=444114.63556453993\n",
      "Gamma:  1.71274369539e-05\n",
      "Current iteration=400, the loss=342446.0555998906\n",
      "Gamma:  1.71274369539e-05\n",
      "Current iteration=500, the loss=299793.91725403134\n",
      "Gamma:  1.71274369539e-05\n",
      "Current iteration=600, the loss=260677.89710240118\n",
      "Gamma:  1.71274369539e-05\n",
      "Current iteration=700, the loss=223685.8381724672\n",
      "Gamma:  1.71274369539e-05\n",
      "Current iteration=800, the loss=188971.47363117422\n",
      "Gamma:  1.71274369539e-05\n",
      "Current iteration=900, the loss=157029.85037529917\n",
      "Gamma:  1.71274369539e-05\n",
      "Current iteration=1000, the loss=128488.11067916258\n",
      "Gamma:  7.61219420174e-06\n",
      "Current iteration=1100, the loss=111156.21268467171\n",
      "Gamma:  7.61219420174e-06\n",
      "Current iteration=1200, the loss=101549.91898673339\n",
      "Gamma:  7.61219420174e-06\n",
      "Current iteration=1300, the loss=93020.52449849749\n",
      "Gamma:  7.61219420174e-06\n",
      "Current iteration=1400, the loss=85619.24170982176\n",
      "Gamma:  7.61219420174e-06\n",
      "Current iteration=1500, the loss=79243.64096156158\n",
      "Gamma:  7.61219420174e-06\n",
      "Current iteration=1600, the loss=73618.09743052206\n",
      "Gamma:  7.61219420174e-06\n",
      "Current iteration=1700, the loss=68531.59094564347\n",
      "Gamma:  7.61219420174e-06\n",
      "Current iteration=1800, the loss=63874.26223322391\n",
      "Gamma:  7.61219420174e-06\n",
      "Current iteration=1900, the loss=59611.03865217853\n",
      "Gamma:  7.61219420174e-06\n",
      "Current iteration=2000, the loss=55731.20632139099\n",
      "Gamma:  7.61219420174e-06\n",
      "Current iteration=2100, the loss=52272.903465189054\n",
      "Gamma:  7.61219420174e-06\n",
      "Current iteration=2200, the loss=49220.84699760083\n",
      "Gamma:  3.38319742299e-06\n",
      "Current iteration=2300, the loss=47489.947631765695\n",
      "Gamma:  3.38319742299e-06\n",
      "Current iteration=2400, the loss=46377.2952526765\n",
      "Gamma:  3.38319742299e-06\n",
      "Current iteration=2500, the loss=45345.4090698007\n",
      "Gamma:  3.38319742299e-06\n",
      "Current iteration=2600, the loss=44391.94424378932\n",
      "Gamma:  3.38319742299e-06\n",
      "Current iteration=2700, the loss=43520.91681308662\n",
      "Gamma:  3.38319742299e-06\n",
      "Current iteration=2800, the loss=42738.61856392662\n",
      "Gamma:  3.38319742299e-06\n",
      "Current iteration=2900, the loss=42030.525412179464\n",
      "Gamma:  3.38319742299e-06\n",
      "Current iteration=3000, the loss=41396.91729727979\n",
      "Gamma:  3.38319742299e-06\n",
      "Current iteration=3100, the loss=40837.972673394\n",
      "Gamma:  3.38319742299e-06\n",
      "Current iteration=3200, the loss=40346.94481301265\n",
      "Gamma:  3.38319742299e-06\n",
      "Current iteration=3300, the loss=39925.55268847848\n",
      "Gamma:  3.38319742299e-06\n",
      "Current iteration=3400, the loss=39562.18167081373\n",
      "Gamma:  3.38319742299e-06\n",
      "Current iteration=3500, the loss=39247.94148541099\n",
      "Gamma:  3.38319742299e-06\n",
      "Current iteration=3600, the loss=38976.20351245334\n",
      "Gamma:  3.38319742299e-06\n",
      "Current iteration=3700, the loss=38741.157875095574\n",
      "Gamma:  3.38319742299e-06\n",
      "Current iteration=3800, the loss=38537.76705247927\n",
      "Gamma:  3.38319742299e-06\n",
      "Current iteration=3900, the loss=38361.531620034475\n",
      "Gamma:  3.38319742299e-06\n",
      "Current iteration=4000, the loss=38208.48155332177\n",
      "Gamma:  3.38319742299e-06\n",
      "Current iteration=4100, the loss=38075.146465009144\n",
      "Gamma:  3.38319742299e-06\n",
      "Current iteration=4200, the loss=37958.26825005793\n",
      "Gamma:  3.38319742299e-06\n",
      "Current iteration=4300, the loss=37854.99623749451\n",
      "Gamma:  3.38319742299e-06\n",
      "Current iteration=4400, the loss=37763.2038797037\n",
      "Gamma:  3.38319742299e-06\n",
      "Current iteration=4500, the loss=37681.269821611684\n",
      "Gamma:  3.38319742299e-06\n",
      "Current iteration=4600, the loss=37607.874858925585\n",
      "Gamma:  3.38319742299e-06\n",
      "Current iteration=4700, the loss=37541.916404572905\n",
      "Gamma:  3.38319742299e-06\n",
      "Current iteration=4800, the loss=37482.46411729057\n",
      "Gamma:  3.38319742299e-06\n",
      "Current iteration=4900, the loss=37428.7276893747\n",
      "Gamma:  3.38319742299e-06\n",
      "Current iteration=5000, the loss=37380.0300548384\n",
      "Gamma:  3.38319742299e-06\n",
      "Current iteration=5100, the loss=37335.78396029485\n",
      "Gamma:  3.38319742299e-06\n",
      "Current iteration=5200, the loss=37295.47228420127\n",
      "Gamma:  3.38319742299e-06\n",
      "Current iteration=5300, the loss=37258.63409664756\n",
      "Gamma:  3.38319742299e-06\n",
      "Current iteration=5400, the loss=37224.85673596723\n",
      "Gamma:  3.38319742299e-06\n",
      "Current iteration=5500, the loss=37193.771749637475\n",
      "Gamma:  3.38319742299e-06\n",
      "Current iteration=5600, the loss=37165.052390027566\n",
      "Gamma:  3.38319742299e-06\n",
      "Current iteration=5700, the loss=37138.41136592269\n",
      "Gamma:  3.38319742299e-06\n",
      "Current iteration=5800, the loss=37113.59831800117\n",
      "Gamma:  3.38319742299e-06\n",
      "Current iteration=5900, the loss=37090.39692106312\n",
      "Gamma:  3.38319742299e-06\n",
      "Current iteration=6000, the loss=37068.62173170612\n",
      "Gamma:  3.38319742299e-06\n",
      "Current iteration=6100, the loss=37048.11490718751\n",
      "Gamma:  3.38319742299e-06\n",
      "Current iteration=6200, the loss=37028.74286259644\n",
      "Gamma:  3.38319742299e-06\n",
      "Current iteration=6300, the loss=37010.39294320662\n",
      "Gamma:  3.38319742299e-06\n",
      "Current iteration=6400, the loss=36992.97023339528\n",
      "Gamma:  3.38319742299e-06\n",
      "Current iteration=6500, the loss=36976.394627048016\n",
      "Gamma:  3.38319742299e-06\n",
      "Current iteration=6600, the loss=36960.59824246759\n",
      "Gamma:  3.38319742299e-06\n",
      "Current iteration=6700, the loss=36945.52321219598\n",
      "Gamma:  3.38319742299e-06\n",
      "Current iteration=6800, the loss=36931.11983777167\n",
      "Gamma:  3.38319742299e-06\n",
      "Current iteration=6900, the loss=36917.34507562421\n",
      "Gamma:  3.38319742299e-06\n",
      "Current iteration=7000, the loss=36904.16130961118\n",
      "Gamma:  3.38319742299e-06\n",
      "Current iteration=7100, the loss=36891.53536365402\n",
      "Gamma:  3.38319742299e-06\n",
      "Current iteration=7200, the loss=36879.43771090406\n",
      "Gamma:  3.38319742299e-06\n",
      "Current iteration=7300, the loss=36867.84184132868\n",
      "Gamma:  3.38319742299e-06\n",
      "Current iteration=7400, the loss=36856.72375597728\n",
      "Gamma:  3.38319742299e-06\n",
      "Current iteration=7500, the loss=36846.06156266343\n",
      "Gamma:  3.38319742299e-06\n",
      "Current iteration=7600, the loss=36835.835154145774\n",
      "Gamma:  3.38319742299e-06\n",
      "Current iteration=7700, the loss=36826.02595631937\n",
      "Gamma:  3.38319742299e-06\n",
      "Current iteration=7800, the loss=36816.61674105865\n",
      "Gamma:  3.38319742299e-06\n",
      "Current iteration=7900, the loss=36807.591507200304\n",
      "Gamma:  3.38319742299e-06\n",
      "Current iteration=8000, the loss=36798.93544495065\n",
      "Gamma:  3.38319742299e-06\n",
      "Current iteration=8100, the loss=36790.635014176376\n",
      "Gamma:  3.38319742299e-06\n",
      "Current iteration=8200, the loss=36782.67818159085\n",
      "Gamma:  3.38319742299e-06\n",
      "Current iteration=8300, the loss=36775.05485764367\n",
      "Gamma:  3.38319742299e-06\n",
      "Current iteration=8400, the loss=36767.757502645116\n",
      "Gamma:  3.38319742299e-06\n",
      "Current iteration=8500, the loss=36760.7816622417\n",
      "Gamma:  3.38319742299e-06\n",
      "Current iteration=8600, the loss=36754.125881295295\n",
      "Gamma:  3.38319742299e-06\n",
      "Current iteration=8700, the loss=36747.79042625987\n",
      "Gamma:  3.38319742299e-06\n",
      "Current iteration=8800, the loss=36741.77500271906\n",
      "Gamma:  3.38319742299e-06\n",
      "Current iteration=8900, the loss=36736.076621573855\n",
      "Gamma:  3.38319742299e-06\n",
      "Current iteration=9000, the loss=36730.68864114353\n",
      "Gamma:  3.38319742299e-06\n",
      "Current iteration=9100, the loss=36725.600994415356\n",
      "Gamma:  3.38319742299e-06\n",
      "Current iteration=9200, the loss=36720.801003321976\n",
      "Gamma:  3.38319742299e-06\n",
      "Current iteration=9300, the loss=36716.27427216768\n",
      "Gamma:  3.38319742299e-06\n",
      "Current iteration=9400, the loss=36712.00543793107\n",
      "Gamma:  3.38319742299e-06\n",
      "Current iteration=9500, the loss=36707.97873900834\n",
      "Gamma:  3.38319742299e-06\n",
      "Current iteration=9600, the loss=36704.178435296955\n",
      "Gamma:  3.38319742299e-06\n",
      "Current iteration=9700, the loss=36700.589126930245\n",
      "Gamma:  3.38319742299e-06\n",
      "Current iteration=9800, the loss=36697.19601184566\n",
      "Gamma:  3.38319742299e-06\n",
      "Current iteration=9900, the loss=36693.98510766747\n",
      "Gamma:  3.38319742299e-06\n",
      "Current iteration=10000, the loss=36690.94344583304\n",
      "Gamma:  3.38319742299e-06\n",
      "Current iteration=10100, the loss=36688.05922905019\n",
      "Gamma:  3.38319742299e-06\n",
      "Current iteration=10200, the loss=36685.321932364335\n",
      "Gamma:  3.38319742299e-06\n",
      "Current iteration=10300, the loss=36682.722328506934\n",
      "Gamma:  3.38319742299e-06\n",
      "Current iteration=10400, the loss=36680.25243023053\n",
      "Gamma:  3.38319742299e-06\n",
      "Current iteration=10500, the loss=36677.905359793964\n",
      "Gamma:  3.38319742299e-06\n",
      "Current iteration=10600, the loss=36675.675169735114\n",
      "Gamma:  3.38319742299e-06\n",
      "Current iteration=10700, the loss=36673.556644185206\n",
      "Gamma:  3.38319742299e-06\n",
      "Current iteration=10800, the loss=36671.54510672322\n",
      "Gamma:  3.38319742299e-06\n",
      "Current iteration=10900, the loss=36669.636252974495\n",
      "Gamma:  3.38319742299e-06\n",
      "Current iteration=11000, the loss=36667.82601755143\n",
      "Gamma:  3.38319742299e-06\n",
      "Current iteration=11100, the loss=36666.11047766163\n",
      "Gamma:  3.38319742299e-06\n",
      "Current iteration=11200, the loss=36664.485790558836\n",
      "Gamma:  3.38319742299e-06\n",
      "Current iteration=11300, the loss=36662.94815903636\n",
      "Gamma:  3.38319742299e-06\n",
      "Current iteration=11400, the loss=36661.49381804475\n",
      "Gamma:  3.38319742299e-06\n",
      "Current iteration=11500, the loss=36660.119035741176\n",
      "Gamma:  3.38319742299e-06\n",
      "Current iteration=11600, the loss=36658.82012329022\n",
      "Gamma:  3.38319742299e-06\n",
      "Current iteration=11700, the loss=36657.59344905497\n",
      "Gamma:  3.38319742299e-06\n",
      "Current iteration=11800, the loss=36656.43545411046\n",
      "Gamma:  3.38319742299e-06\n",
      "Current iteration=11900, the loss=36655.34266710682\n",
      "Gamma:  3.38319742299e-06\n",
      "Current iteration=12000, the loss=36654.31171734569\n",
      "Gamma:  3.38319742299e-06\n",
      "Current iteration=12100, the loss=36653.33934552477\n",
      "Gamma:  3.38319742299e-06\n",
      "Current iteration=12200, the loss=36652.42241199406\n",
      "Gamma:  3.38319742299e-06\n",
      "Current iteration=12300, the loss=36651.55790260511\n",
      "Gamma:  3.38319742299e-06\n",
      "Current iteration=12400, the loss=36650.74293236536\n",
      "Gamma:  3.38319742299e-06\n",
      "Current iteration=12500, the loss=36649.97474717167\n",
      "Gamma:  3.38319742299e-06\n",
      "Current iteration=12600, the loss=36649.250723914105\n",
      "Gamma:  3.38319742299e-06\n",
      "Current iteration=12700, the loss=36648.56836923313\n",
      "Gamma:  3.38319742299e-06\n",
      "Current iteration=12800, the loss=36647.92531719079\n",
      "Gamma:  3.38319742299e-06\n",
      "Current iteration=12900, the loss=36647.31932608872\n",
      "Gamma:  3.38319742299e-06\n",
      "Current iteration=13000, the loss=36646.74827463493\n",
      "Gamma:  3.38319742299e-06\n",
      "Current iteration=13100, the loss=36646.210157632275\n",
      "Gamma:  3.38319742299e-06\n",
      "Current iteration=13200, the loss=36645.703081334475\n",
      "Gamma:  3.38319742299e-06\n",
      "Current iteration=13300, the loss=36645.22525859039\n",
      "Gamma:  3.38319742299e-06\n",
      "Current iteration=13400, the loss=36644.77500387638\n",
      "Gamma:  3.38319742299e-06\n",
      "Current iteration=13500, the loss=36644.35072829738\n",
      "Gamma:  3.38319742299e-06\n",
      "Current iteration=13600, the loss=36643.95093462146\n",
      "Gamma:  3.38319742299e-06\n",
      "Current iteration=13700, the loss=36643.5742123985\n",
      "Gamma:  3.38319742299e-06\n",
      "Current iteration=13800, the loss=36643.21923320255\n",
      "Gamma:  3.38319742299e-06\n",
      "Current iteration=13900, the loss=36642.88474602712\n",
      "Gamma:  3.38319742299e-06\n",
      "Current iteration=14000, the loss=36642.56957285474\n",
      "Gamma:  3.38319742299e-06\n",
      "Current iteration=14100, the loss=36642.2726044154\n",
      "Gamma:  3.38319742299e-06\n",
      "Current iteration=14200, the loss=36641.99279614279\n",
      "Gamma:  3.38319742299e-06\n",
      "Current iteration=14300, the loss=36641.729164333075\n",
      "Gamma:  3.38319742299e-06\n",
      "Current iteration=14400, the loss=36641.48078250667\n",
      "Gamma:  3.38319742299e-06\n",
      "Current iteration=14500, the loss=36641.2467779717\n",
      "Gamma:  3.38319742299e-06\n",
      "Current iteration=14600, the loss=36641.02632858441\n",
      "Gamma:  3.38319742299e-06\n",
      "Current iteration=14700, the loss=36640.81865970124\n",
      "Gamma:  3.38319742299e-06\n",
      "Current iteration=14800, the loss=36640.62304131501\n",
      "Gamma:  3.38319742299e-06\n",
      "Current iteration=14900, the loss=36640.43878536788\n",
      "Gamma:  3.38319742299e-06\n",
      "Current iteration=15000, the loss=36640.26524323206\n",
      "Gamma:  3.38319742299e-06\n",
      "Current iteration=15100, the loss=36640.10180334998\n",
      "Gamma:  3.38319742299e-06\n",
      "Current iteration=15200, the loss=36639.94788902467\n",
      "Gamma:  3.38319742299e-06\n",
      "Current iteration=15300, the loss=36639.80295635157\n",
      "Gamma:  3.38319742299e-06\n",
      "Current iteration=15400, the loss=36639.66649228314\n",
      "Gamma:  3.38319742299e-06\n",
      "Current iteration=15500, the loss=36639.53801281766\n",
      "Gamma:  3.38319742299e-06\n",
      "Current iteration=15600, the loss=36639.41706130398\n",
      "Gamma:  3.38319742299e-06\n",
      "Current iteration=15700, the loss=36639.3032068545\n",
      "Gamma:  3.38319742299e-06\n",
      "Current iteration=15800, the loss=36639.19604285898\n",
      "Gamma:  3.38319742299e-06\n",
      "Current iteration=15900, the loss=36639.095185592\n",
      "Gamma:  3.38319742299e-06\n",
      "Current iteration=16000, the loss=36639.000272907586\n",
      "Gamma:  3.38319742299e-06\n",
      "Current iteration=16100, the loss=36638.91096301479\n",
      "Gamma:  3.38319742299e-06\n",
      "Current iteration=16200, the loss=36638.82693332847\n",
      "Gamma:  3.38319742299e-06\n",
      "Current iteration=16300, the loss=36638.74787939002\n",
      "Gamma:  3.38319742299e-06\n",
      "Current iteration=16400, the loss=36638.67351385291\n",
      "Gamma:  3.38319742299e-06\n",
      "Current iteration=16500, the loss=36638.60356552868\n",
      "Gamma:  3.38319742299e-06\n",
      "Current iteration=16600, the loss=36638.537778489066\n",
      "Gamma:  3.38319742299e-06\n",
      "Current iteration=16700, the loss=36638.47591122037\n",
      "Gamma:  3.38319742299e-06\n",
      "Current iteration=16800, the loss=36638.41773582665\n",
      "Gamma:  3.38319742299e-06\n",
      "Current iteration=16900, the loss=36638.36303727828\n",
      "Gamma:  3.38319742299e-06\n",
      "Current iteration=17000, the loss=36638.31161270297\n",
      "Gamma:  3.38319742299e-06\n",
      "Current iteration=17100, the loss=36638.26327071661\n",
      "Gamma:  3.38319742299e-06\n",
      "Current iteration=17200, the loss=36638.217830791094\n",
      "Gamma:  3.38319742299e-06\n",
      "Current iteration=17300, the loss=36638.1751226572\n",
      "Gamma:  3.38319742299e-06\n",
      "Current iteration=17400, the loss=36638.13498574001\n",
      "Gamma:  3.38319742299e-06\n",
      "Current iteration=17500, the loss=36638.09726862523\n",
      "Gamma:  3.38319742299e-06\n",
      "Current iteration=17600, the loss=36638.061828554266\n",
      "Gamma:  3.38319742299e-06\n",
      "Current iteration=17700, the loss=36638.02853094673\n",
      "Gamma:  3.38319742299e-06\n",
      "Current iteration=17800, the loss=36637.99724894852\n",
      "Gamma:  3.38319742299e-06\n",
      "Current iteration=17900, the loss=36637.9678630041\n",
      "Gamma:  3.38319742299e-06\n",
      "Current iteration=18000, the loss=36637.94026045189\n",
      "Gamma:  3.38319742299e-06\n",
      "Current iteration=18100, the loss=36637.91433514105\n",
      "Gamma:  3.38319742299e-06\n",
      "Current iteration=18200, the loss=36637.889987068964\n",
      "Gamma:  3.38319742299e-06\n",
      "Current iteration=18300, the loss=36637.86712203795\n",
      "Gamma:  3.38319742299e-06\n",
      "Current iteration=18400, the loss=36637.84565133044\n",
      "Gamma:  3.38319742299e-06\n",
      "Current iteration=18500, the loss=36637.82549140158\n",
      "Gamma:  3.38319742299e-06\n",
      "Current iteration=18600, the loss=36637.80656358814\n",
      "Gamma:  3.38319742299e-06\n",
      "Current iteration=18700, the loss=36637.7887938333\n",
      "Gamma:  3.38319742299e-06\n",
      "Current iteration=18800, the loss=36637.772112426086\n",
      "Gamma:  3.38319742299e-06\n",
      "Current iteration=18900, the loss=36637.75645375492\n",
      "Gamma:  3.38319742299e-06\n",
      "Current iteration=19000, the loss=36637.7417560746\n",
      "Gamma:  3.38319742299e-06\n",
      "Current iteration=19100, the loss=36637.72796128576\n",
      "Gamma:  3.38319742299e-06\n",
      "Current iteration=19200, the loss=36637.71501472645\n",
      "Gamma:  3.38319742299e-06\n",
      "Current iteration=19300, the loss=36637.7028649751\n",
      "Gamma:  3.38319742299e-06\n",
      "Current iteration=19400, the loss=36637.691463664254\n",
      "Gamma:  3.38319742299e-06\n",
      "Current iteration=19500, the loss=36637.68076530449\n",
      "Gamma:  3.38319742299e-06\n",
      "Current iteration=19600, the loss=36637.67072711821\n",
      "Gamma:  3.38319742299e-06\n",
      "Current iteration=19700, the loss=36637.66130888241\n",
      "Gamma:  3.38319742299e-06\n",
      "Current iteration=19800, the loss=36637.65247278042\n",
      "Gamma:  3.38319742299e-06\n",
      "Current iteration=19900, the loss=36637.64418326167\n",
      "Gamma:  3.38319742299e-06\n",
      "Current iteration=20000, the loss=36637.63640690955\n",
      "Gamma:  3.38319742299e-06\n",
      "Current iteration=20100, the loss=36637.62911231643\n",
      "Gamma:  3.38319742299e-06\n",
      "Current iteration=20200, the loss=36637.6222699659\n",
      "Gamma:  3.38319742299e-06\n",
      "Current iteration=20300, the loss=36637.615852121635\n",
      "Gamma:  3.38319742299e-06\n",
      "Current iteration=20400, the loss=36637.60983272246\n",
      "Gamma:  3.38319742299e-06\n",
      "Current iteration=20500, the loss=36637.60418728349\n",
      "Gamma:  3.38319742299e-06\n",
      "Current iteration=20600, the loss=36637.59889280285\n",
      "Gamma:  3.38319742299e-06\n",
      "Current iteration=20700, the loss=36637.593927673784\n",
      "Gamma:  3.38319742299e-06\n",
      "Current iteration=20800, the loss=36637.589271601726\n",
      "Gamma:  3.38319742299e-06\n",
      "Current iteration=20900, the loss=36637.58490552622\n",
      "Gamma:  3.38319742299e-06\n",
      "Current iteration=21000, the loss=36637.58081154735\n",
      "Gamma:  3.38319742299e-06\n",
      "Current iteration=21100, the loss=36637.576972856354\n",
      "Gamma:  3.38319742299e-06\n",
      "Current iteration=21200, the loss=36637.573373670326\n",
      "Gamma:  3.38319742299e-06\n",
      "Current iteration=21300, the loss=36637.569999170744\n",
      "Gamma:  3.38319742299e-06\n",
      "Current iteration=21400, the loss=36637.56683544552\n",
      "Gamma:  3.38319742299e-06\n",
      "Current iteration=21500, the loss=36637.56386943446\n",
      "Gamma:  3.38319742299e-06\n",
      "Current iteration=21600, the loss=36637.56108887795\n",
      "Gamma:  3.38319742299e-06\n",
      "Current iteration=21700, the loss=36637.55848226865\n",
      "Gamma:  3.38319742299e-06\n",
      "Current iteration=21800, the loss=36637.55603880599\n",
      "Gamma:  3.38319742299e-06\n",
      "Current iteration=21900, the loss=36637.55374835348\n",
      "Gamma:  3.38319742299e-06\n",
      "Current iteration=22000, the loss=36637.55160139833\n",
      "Gamma:  3.38319742299e-06\n",
      "Current iteration=22100, the loss=36637.54958901372\n",
      "Gamma:  3.38319742299e-06\n",
      "Current iteration=22200, the loss=36637.54770282308\n",
      "Gamma:  3.38319742299e-06\n",
      "Current iteration=22300, the loss=36637.545934966736\n",
      "Gamma:  3.38319742299e-06\n",
      "Current iteration=22400, the loss=36637.54427807024\n",
      "Gamma:  3.38319742299e-06\n",
      "Current iteration=22500, the loss=36637.542725214975\n",
      "Gamma:  3.38319742299e-06\n",
      "Current iteration=22600, the loss=36637.541269910114\n",
      "Gamma:  3.38319742299e-06\n",
      "Current iteration=22700, the loss=36637.53990606663\n",
      "Gamma:  3.38319742299e-06\n",
      "Current iteration=22800, the loss=36637.538627972586\n",
      "Gamma:  3.38319742299e-06\n",
      "Current iteration=22900, the loss=36637.53743027011\n",
      "Gamma:  3.38319742299e-06\n",
      "Current iteration=23000, the loss=36637.53630793364\n",
      "Gamma:  3.38319742299e-06\n",
      "Current iteration=23100, the loss=36637.53525624947\n",
      "Gamma:  3.38319742299e-06\n",
      "Current iteration=23200, the loss=36637.53427079673\n",
      "Gamma:  3.38319742299e-06\n",
      "Current iteration=23300, the loss=36637.533347429184\n",
      "Gamma:  3.38319742299e-06\n",
      "Current iteration=23400, the loss=36637.532482258495\n",
      "Gamma:  3.38319742299e-06\n",
      "Current iteration=23500, the loss=36637.53167163822\n",
      "Gamma:  3.38319742299e-06\n",
      "Current iteration=23600, the loss=36637.5309121489\n",
      "Gamma:  3.38319742299e-06\n",
      "Current iteration=23700, the loss=36637.530200584115\n",
      "Gamma:  3.38319742299e-06\n",
      "Current iteration=23800, the loss=36637.52953393724\n",
      "Gamma:  3.38319742299e-06\n",
      "Current iteration=23900, the loss=36637.52890938916\n",
      "Gamma:  3.38319742299e-06\n",
      "Current iteration=24000, the loss=36637.52832429665\n",
      "Gamma:  3.38319742299e-06\n",
      "Current iteration=24100, the loss=36637.52777618142\n",
      "Gamma:  3.38319742299e-06\n",
      "Current iteration=24200, the loss=36637.527262720025\n",
      "Gamma:  3.38319742299e-06\n",
      "Current iteration=24300, the loss=36637.52678173414\n",
      "Gamma:  3.38319742299e-06\n",
      "Current iteration=24400, the loss=36637.526331181616\n",
      "Gamma:  3.38319742299e-06\n",
      "Current iteration=24500, the loss=36637.52590914802\n",
      "Gamma:  3.38319742299e-06\n",
      "Current iteration=24600, the loss=36637.525513838686\n",
      "Gamma:  3.38319742299e-06\n",
      "Current iteration=24700, the loss=36637.52514357129\n",
      "Gamma:  3.38319742299e-06\n",
      "Current iteration=24800, the loss=36637.52479676884\n",
      "Gamma:  3.38319742299e-06\n",
      "Current iteration=24900, the loss=36637.52447195316\n",
      "Gamma:  3.38319742299e-06\n",
      "Current iteration=25000, the loss=36637.52416773869\n",
      "Gamma:  3.38319742299e-06\n",
      "Current iteration=25100, the loss=36637.52388282674\n",
      "Gamma:  3.38319742299e-06\n",
      "Current iteration=25200, the loss=36637.52361600005\n",
      "Gamma:  3.38319742299e-06\n",
      "Current iteration=25300, the loss=36637.52336611776\n",
      "Gamma:  3.38319742299e-06\n",
      "Current iteration=25400, the loss=36637.52313211064\n",
      "Gamma:  3.38319742299e-06\n",
      "Current iteration=25500, the loss=36637.52291297653\n",
      "Gamma:  3.38319742299e-06\n",
      "Current iteration=25600, the loss=36637.52270777626\n",
      "Gamma:  3.38319742299e-06\n",
      "Current iteration=25700, the loss=36637.522515629666\n",
      "Gamma:  3.38319742299e-06\n",
      "Current iteration=25800, the loss=36637.52233571187\n",
      "Gamma:  3.38319742299e-06\n",
      "Current iteration=25900, the loss=36637.52216724992\n",
      "Gamma:  3.38319742299e-06\n",
      "Current iteration=26000, the loss=36637.52200951942\n",
      "Gamma:  3.38319742299e-06\n",
      "Current iteration=26100, the loss=36637.521861841575\n",
      "Gamma:  3.38319742299e-06\n",
      "Current iteration=26200, the loss=36637.52172358033\n",
      "Gamma:  3.38319742299e-06\n",
      "Current iteration=26300, the loss=36637.52159413962\n",
      "Gamma:  3.38319742299e-06\n",
      "Current iteration=26400, the loss=36637.52147296099\n",
      "Gamma:  3.38319742299e-06\n",
      "Current iteration=26500, the loss=36637.52135952108\n",
      "Gamma:  3.38319742299e-06\n",
      "Current iteration=26600, the loss=36637.52125332958\n",
      "Gamma:  3.38319742299e-06\n",
      "Current iteration=26700, the loss=36637.521153927046\n",
      "Gamma:  3.38319742299e-06\n",
      "Current iteration=26800, the loss=36637.521060883024\n",
      "Gamma:  3.38319742299e-06\n",
      "Current iteration=26900, the loss=36637.52097379421\n",
      "Gamma:  3.38319742299e-06\n",
      "Current iteration=27000, the loss=36637.52089228271\n",
      "Gamma:  3.38319742299e-06\n",
      "Current iteration=27100, the loss=36637.520815994525\n",
      "Gamma:  3.38319742299e-06\n",
      "Current iteration=27200, the loss=36637.52074459798\n",
      "Gamma:  3.38319742299e-06\n",
      "Current iteration=27300, the loss=36637.52067778233\n",
      "Gamma:  3.38319742299e-06\n",
      "Current iteration=27400, the loss=36637.52061525649\n",
      "Gamma:  3.38319742299e-06\n",
      "Current iteration=27500, the loss=36637.52055674773\n",
      "Gamma:  3.38319742299e-06\n",
      "Current iteration=27600, the loss=36637.520502000596\n",
      "Gamma:  3.38319742299e-06\n",
      "Current iteration=27700, the loss=36637.52045077575\n",
      "Gamma:  3.38319742299e-06\n",
      "Current iteration=27800, the loss=36637.52040284901\n",
      "Gamma:  3.38319742299e-06\n",
      "Current iteration=27900, the loss=36637.52035801036\n",
      "Gamma:  3.38319742299e-06\n",
      "Current iteration=28000, the loss=36637.52031606308\n",
      "Gamma:  3.38319742299e-06\n",
      "Current iteration=28100, the loss=36637.5202768229\n",
      "Gamma:  3.38319742299e-06\n",
      "Current iteration=28200, the loss=36637.52024011722\n",
      "Gamma:  3.38319742299e-06\n",
      "Current iteration=28300, the loss=36637.52020578436\n",
      "Gamma:  3.38319742299e-06\n",
      "Current iteration=28400, the loss=36637.520173672914\n",
      "Gamma:  3.38319742299e-06\n",
      "Current iteration=28500, the loss=36637.52014364102\n",
      "Gamma:  3.38319742299e-06\n",
      "Current iteration=28600, the loss=36637.520115555846\n",
      "Gamma:  3.38319742299e-06\n",
      "Current iteration=28700, the loss=36637.52008929298\n",
      "Gamma:  3.38319742299e-06\n",
      "Current iteration=28800, the loss=36637.52006473587\n",
      "Gamma:  3.38319742299e-06\n",
      "Current iteration=28900, the loss=36637.5200417754\n",
      "Gamma:  3.38319742299e-06\n",
      "Current iteration=29000, the loss=36637.52002030936\n",
      "Gamma:  3.38319742299e-06\n",
      "Current iteration=29100, the loss=36637.52000024203\n",
      "Gamma:  3.38319742299e-06\n",
      "Current iteration=29200, the loss=36637.51998148378\n",
      "Gamma:  3.38319742299e-06\n",
      "Current iteration=29300, the loss=36637.51996395064\n",
      "Gamma:  3.38319742299e-06\n",
      "Current iteration=29400, the loss=36637.519947564026\n",
      "Gamma:  3.38319742299e-06\n",
      "Current iteration=29500, the loss=36637.51993225031\n",
      "Gamma:  3.38319742299e-06\n",
      "Current iteration=29600, the loss=36637.519917940575\n",
      "Gamma:  3.38319742299e-06\n",
      "Current iteration=29700, the loss=36637.51990457026\n",
      "Gamma:  3.38319742299e-06\n",
      "Current iteration=29800, the loss=36637.519892078955\n",
      "Gamma:  3.38319742299e-06\n",
      "Current iteration=29900, the loss=36637.51988041004\n",
      "Gamma:  3.38319742299e-06\n",
      "Current iteration=30000, the loss=36637.519869510565\n",
      "Gamma:  3.38319742299e-06\n",
      "Current iteration=30100, the loss=36637.51985933089\n",
      "Gamma:  3.38319742299e-06\n",
      "Current iteration=30200, the loss=36637.5198498246\n",
      "Gamma:  3.38319742299e-06\n",
      "Current iteration=30300, the loss=36637.51984094819\n",
      "Gamma:  3.38319742299e-06\n",
      "Current iteration=30400, the loss=36637.519832660975\n",
      "Gamma:  3.38319742299e-06\n",
      "Current iteration=30500, the loss=36637.519824924835\n",
      "Gamma:  3.38319742299e-06\n",
      "Current iteration=30600, the loss=36637.519817704066\n",
      "Gamma:  3.38319742299e-06\n",
      "Current iteration=30700, the loss=36637.51981096529\n",
      "Gamma:  3.38319742299e-06\n",
      "Current iteration=30800, the loss=36637.51980467727\n",
      "Gamma:  3.38319742299e-06\n",
      "Current iteration=30900, the loss=36637.519798810725\n",
      "Gamma:  3.38319742299e-06\n",
      "Current iteration=31000, the loss=36637.51979333825\n",
      "Gamma:  3.38319742299e-06\n",
      "Current iteration=31100, the loss=36637.519788234225\n",
      "Gamma:  3.38319742299e-06\n",
      "Current iteration=31200, the loss=36637.51978347465\n",
      "Gamma:  3.38319742299e-06\n",
      "Current iteration=31300, the loss=36637.51977903707\n",
      "Gamma:  3.38319742299e-06\n",
      "Current iteration=31400, the loss=36637.51977490046\n",
      "Gamma:  3.38319742299e-06\n",
      "Current iteration=31500, the loss=36637.51977104517\n",
      "Gamma:  3.38319742299e-06\n",
      "Current iteration=31600, the loss=36637.51976745277\n",
      "Gamma:  3.38319742299e-06\n",
      "Current iteration=31700, the loss=36637.51976410605\n",
      "Gamma:  3.38319742299e-06\n",
      "Current iteration=31800, the loss=36637.51976098889\n",
      "Gamma:  3.38319742299e-06\n",
      "Current iteration=31900, the loss=36637.51975808622\n",
      "Gamma:  3.38319742299e-06\n",
      "Current iteration=32000, the loss=36637.51975538391\n",
      "Gamma:  3.38319742299e-06\n",
      "Current iteration=32100, the loss=36637.519752868786\n",
      "Gamma:  3.38319742299e-06\n",
      "Current iteration=32200, the loss=36637.51975052848\n",
      "Gamma:  3.38319742299e-06\n",
      "Current iteration=32300, the loss=36637.51974835146\n",
      "Gamma:  3.38319742299e-06\n",
      "Current iteration=32400, the loss=36637.51974632687\n",
      "Gamma:  3.38319742299e-06\n",
      "Current iteration=32500, the loss=36637.519744444646\n",
      "Gamma:  3.38319742299e-06\n",
      "Current iteration=32600, the loss=36637.519742695324\n",
      "Gamma:  3.38319742299e-06\n",
      "Current iteration=32700, the loss=36637.519741070064\n",
      "Gamma:  3.38319742299e-06\n",
      "Current iteration=32800, the loss=36637.519739560594\n",
      "Gamma:  3.38319742299e-06\n",
      "Current iteration=32900, the loss=36637.51973815916\n",
      "Gamma:  3.38319742299e-06\n",
      "Current iteration=33000, the loss=36637.51973685855\n",
      "Gamma:  3.38319742299e-06\n",
      "Current iteration=33100, the loss=36637.519735651986\n",
      "Gamma:  3.38319742299e-06\n",
      "Current iteration=33200, the loss=36637.51973453317\n",
      "Reached Theshold, exit\n"
     ]
    }
   ],
   "source": [
    "weights_0 = reg_logistic_regression(y_0, matrix_std_tx_0, lambdas[0], gammas[0], max_iter)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gamma:  0.00333333333333\n",
      "Current iteration=0, the loss=53749.4049693404\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chenfs/anaconda/lib/python3.5/site-packages/ipykernel/__main__.py:7: RuntimeWarning: overflow encountered in exp\n",
      "/Users/chenfs/anaconda/lib/python3.5/site-packages/ipykernel/__main__.py:18: RuntimeWarning: overflow encountered in exp\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gamma:  0.000195092211553\n",
      "Current iteration=100, the loss=1364469.6244029587\n",
      "Gamma:  8.67076495792e-05\n",
      "Current iteration=200, the loss=686902.4822888984\n",
      "Gamma:  3.85367331463e-05\n",
      "Current iteration=300, the loss=391479.3177926035\n",
      "Gamma:  2.56911554309e-05\n",
      "Current iteration=400, the loss=235798.0257714902\n",
      "Gamma:  1.14182913026e-05\n",
      "Current iteration=500, the loss=153538.7093832145\n",
      "Gamma:  1.14182913026e-05\n",
      "Current iteration=600, the loss=115915.75405120073\n",
      "Gamma:  5.07479613449e-06\n",
      "Current iteration=700, the loss=92898.236657372\n",
      "Gamma:  5.07479613449e-06\n",
      "Current iteration=800, the loss=82753.21929793632\n",
      "Gamma:  5.07479613449e-06\n",
      "Current iteration=900, the loss=74082.76167778182\n",
      "Gamma:  5.07479613449e-06\n",
      "Current iteration=1000, the loss=66702.97037265061\n",
      "Gamma:  5.07479613449e-06\n",
      "Current iteration=1100, the loss=60485.36097399944\n",
      "Gamma:  5.07479613449e-06\n",
      "Current iteration=1200, the loss=55370.46429210676\n",
      "Gamma:  2.25546494866e-06\n",
      "Current iteration=1300, the loss=52124.55261939003\n",
      "Gamma:  2.25546494866e-06\n",
      "Current iteration=1400, the loss=50619.8069138846\n",
      "Gamma:  2.25546494866e-06\n",
      "Current iteration=1500, the loss=49334.75664236526\n",
      "Gamma:  2.25546494866e-06\n",
      "Current iteration=1600, the loss=48261.331697771835\n",
      "Gamma:  2.25546494866e-06\n",
      "Current iteration=1700, the loss=47426.238154181665\n",
      "Gamma:  2.25546494866e-06\n",
      "Current iteration=1800, the loss=46825.69607965084\n",
      "Gamma:  2.25546494866e-06\n",
      "Current iteration=1900, the loss=46293.941666049905\n",
      "Gamma:  2.25546494866e-06\n",
      "Current iteration=2000, the loss=45825.57166293054\n",
      "Gamma:  2.25546494866e-06\n",
      "Current iteration=2100, the loss=45398.68401735692\n",
      "Gamma:  2.25546494866e-06\n",
      "Current iteration=2200, the loss=44998.8644640043\n",
      "Gamma:  2.25546494866e-06\n",
      "Current iteration=2300, the loss=44620.67926223101\n",
      "Gamma:  2.25546494866e-06\n",
      "Current iteration=2400, the loss=44260.09113468442\n",
      "Gamma:  2.25546494866e-06\n",
      "Current iteration=2500, the loss=43914.22702506385\n",
      "Gamma:  2.25546494866e-06\n",
      "Current iteration=2600, the loss=43581.205899826375\n",
      "Gamma:  2.25546494866e-06\n",
      "Current iteration=2700, the loss=43259.8237497058\n",
      "Gamma:  2.25546494866e-06\n",
      "Current iteration=2800, the loss=42949.29255711321\n",
      "Gamma:  2.25546494866e-06\n",
      "Current iteration=2900, the loss=42649.07004066987\n",
      "Gamma:  2.25546494866e-06\n",
      "Current iteration=3000, the loss=42358.75757399846\n",
      "Gamma:  2.25546494866e-06\n",
      "Current iteration=3100, the loss=42078.040667759866\n",
      "Gamma:  2.25546494866e-06\n",
      "Current iteration=3200, the loss=41806.65460822771\n",
      "Gamma:  2.25546494866e-06\n",
      "Current iteration=3300, the loss=41544.36512369478\n",
      "Gamma:  2.25546494866e-06\n",
      "Current iteration=3400, the loss=41290.95821461191\n",
      "Gamma:  2.25546494866e-06\n",
      "Current iteration=3500, the loss=41046.23544742107\n",
      "Gamma:  2.25546494866e-06\n",
      "Current iteration=3600, the loss=40810.01286995049\n",
      "Gamma:  2.25546494866e-06\n",
      "Current iteration=3700, the loss=40582.124791850154\n",
      "Gamma:  2.25546494866e-06\n",
      "Current iteration=3800, the loss=40362.43633134999\n",
      "Gamma:  2.25546494866e-06\n",
      "Current iteration=3900, the loss=40150.853082289024\n",
      "Gamma:  2.25546494866e-06\n",
      "Current iteration=4000, the loss=39947.28182580517\n",
      "Gamma:  2.25546494866e-06\n",
      "Current iteration=4100, the loss=39751.579128892656\n",
      "Gamma:  2.25546494866e-06\n",
      "Current iteration=4200, the loss=39563.57026124204\n",
      "Gamma:  2.25546494866e-06\n",
      "Current iteration=4300, the loss=39383.07316311726\n",
      "Gamma:  2.25546494866e-06\n",
      "Current iteration=4400, the loss=39209.90101289393\n",
      "Gamma:  2.25546494866e-06\n",
      "Current iteration=4500, the loss=39043.86075343061\n",
      "Gamma:  2.25546494866e-06\n",
      "Current iteration=4600, the loss=38884.75228656089\n",
      "Gamma:  2.25546494866e-06\n",
      "Current iteration=4700, the loss=38732.369328618304\n",
      "Gamma:  2.25546494866e-06\n",
      "Current iteration=4800, the loss=38586.502980896556\n",
      "Gamma:  2.25546494866e-06\n",
      "Current iteration=4900, the loss=38446.94758399573\n",
      "Gamma:  2.25546494866e-06\n",
      "Current iteration=5000, the loss=38313.50585356498\n",
      "Gamma:  2.25546494866e-06\n",
      "Current iteration=5100, the loss=38185.99030762226\n",
      "Gamma:  2.25546494866e-06\n",
      "Current iteration=5200, the loss=38064.22124308949\n",
      "Gamma:  2.25546494866e-06\n",
      "Current iteration=5300, the loss=37948.02368485495\n",
      "Gamma:  2.25546494866e-06\n",
      "Current iteration=5400, the loss=37837.22507660065\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=5500, the loss=37743.019925310684\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=5600, the loss=37675.09548341267\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=5700, the loss=37609.38062417268\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=5800, the loss=37545.83782060726\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=5900, the loss=37484.45603347041\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=6000, the loss=37425.25715941556\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=6100, the loss=37368.2376235327\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=6200, the loss=37313.34145757269\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=6300, the loss=37260.50556136029\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=6400, the loss=37209.6718143287\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=6500, the loss=37160.783830524146\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=6600, the loss=37113.785614655564\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=6700, the loss=37068.62124965153\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=6800, the loss=37025.2348157403\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=6900, the loss=36983.570340017555\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=7000, the loss=36943.57171541599\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=7100, the loss=36905.182564528965\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=7200, the loss=36868.346044558915\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=7300, the loss=36833.00462624646\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=7400, the loss=36799.09995689595\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=7500, the loss=36766.5730388164\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=7600, the loss=36735.36503998135\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=7700, the loss=36705.4189273089\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=7800, the loss=36676.68177714989\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=7900, the loss=36649.10763996375\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=8000, the loss=36622.66160571993\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=8100, the loss=36597.32436575634\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=8200, the loss=36573.08748712405\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=8300, the loss=36549.93382397341\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=8400, the loss=36527.830708145375\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=8500, the loss=36506.74014586236\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=8600, the loss=36486.6242073349\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=8700, the loss=36467.44563140536\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=8800, the loss=36449.16781057244\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=8900, the loss=36431.754833347804\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=9000, the loss=36415.17154718701\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=9100, the loss=36399.383613201586\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=9200, the loss=36384.357546546955\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=9300, the loss=36370.06074303026\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=9400, the loss=36356.461493802\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=9500, the loss=36343.528989855666\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=9600, the loss=36331.23331758791\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=9700, the loss=36319.545446205746\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=9800, the loss=36308.43720742167\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=9900, the loss=36297.88126774158\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=10000, the loss=36287.85109387129\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=10100, the loss=36278.32091263408\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=10200, the loss=36269.26566876239\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=10300, the loss=36260.66098757964\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=10400, the loss=36252.48315516653\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=10500, the loss=36244.709134913006\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=10600, the loss=36237.316642029466\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=10700, the loss=36230.28428880378\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=10800, the loss=36223.59178712442\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=10900, the loss=36217.22015829709\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=11000, the loss=36211.15187870385\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=11100, the loss=36205.37090734464\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=11200, the loss=36199.86259094781\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=11300, the loss=36194.613488572206\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=11400, the loss=36189.61117244295\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=11500, the loss=36184.84404751716\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=11600, the loss=36180.3012086413\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=11700, the loss=36175.97233627214\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=11800, the loss=36171.847623353744\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=11900, the loss=36167.91772432099\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=12000, the loss=36164.17371865546\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=12100, the loss=36160.607083593735\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=12200, the loss=36157.20967247149\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=12300, the loss=36153.97369653532\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=12400, the loss=36150.891708929215\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=12500, the loss=36147.9565901001\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=12600, the loss=36145.16153418495\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=12700, the loss=36142.50003612714\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=12800, the loss=36139.965879375486\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=12900, the loss=36137.55312408002\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=13000, the loss=36135.25609573259\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=13100, the loss=36133.06937422016\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=13200, the loss=36130.98778326914\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=13300, the loss=36129.00638026573\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=13400, the loss=36127.12044644012\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=13500, the loss=36125.32547740497\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=13600, the loss=36123.61717403863\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=13700, the loss=36121.99143370491\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=13800, the loss=36120.44434180084\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=13900, the loss=36118.97216362405\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=14000, the loss=36117.57133655158\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=14100, the loss=36116.23846252139\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=14200, the loss=36114.97030080836\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=14300, the loss=36113.76376108613\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=14400, the loss=36112.61589676628\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=14500, the loss=36111.52389860616\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=14600, the loss=36110.48508857718\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=14700, the loss=36109.49691398466\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=14800, the loss=36108.556941831186\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=14900, the loss=36107.66285341509\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=15000, the loss=36106.81243915572\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=15100, the loss=36106.00359363766\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=15200, the loss=36105.234310865875\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=15300, the loss=36104.50267972398\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=15400, the loss=36103.80687962817\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=15500, the loss=36103.145176369384\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=15600, the loss=36102.5159181363\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=15700, the loss=36101.91753171244\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=15800, the loss=36101.348518840234\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=15900, the loss=36100.80745274564\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=16000, the loss=36100.29297481663\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=16100, the loss=36099.803791429644\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=16200, the loss=36099.33867091753\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=16300, the loss=36098.89644067353\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=16400, the loss=36098.47598438542\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=16500, the loss=36098.07623939433\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=16600, the loss=36097.69619417329\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=16700, the loss=36097.33488591987\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=16800, the loss=36096.99139825875\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=16900, the loss=36096.6648590488\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=17000, the loss=36096.354438290626\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=17100, the loss=36096.05934613012\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=17200, the loss=36095.77883095366\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=17300, the loss=36095.5121775712\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=17400, the loss=36095.258705483124\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=17500, the loss=36095.0177672274\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=17600, the loss=36094.78874680343\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=17700, the loss=36094.57105816895\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=17800, the loss=36094.364143807084\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=17900, the loss=36094.167473360154\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=18000, the loss=36093.98054232734\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=18100, the loss=36093.802870823325\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=18200, the loss=36093.63400239514\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=18300, the loss=36093.473502894565\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=18400, the loss=36093.32095940366\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=18500, the loss=36093.17597921084\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=18600, the loss=36093.03818883539\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=18700, the loss=36092.90723309794\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=18800, the loss=36092.7827742352\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=18900, the loss=36092.66449105644\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=19000, the loss=36092.55207814016\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=19100, the loss=36092.445245068935\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=19200, the loss=36092.34371570069\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=19300, the loss=36092.24722747476\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=19400, the loss=36092.15553075107\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=19500, the loss=36092.06838818095\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=19600, the loss=36091.985574108\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=19700, the loss=36091.9068739978\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=19800, the loss=36091.83208389488\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=19900, the loss=36091.761009905866\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=20000, the loss=36091.693467707504\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=20100, the loss=36091.62928207832\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=20200, the loss=36091.56828645294\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=20300, the loss=36091.51032249788\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=20400, the loss=36091.45523970792\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=20500, the loss=36091.402895021885\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=20600, the loss=36091.353152457195\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=20700, the loss=36091.30588276201\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=20800, the loss=36091.260963084314\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=20900, the loss=36091.218276657135\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=21000, the loss=36091.17771249892\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=21100, the loss=36091.139165128676\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=21200, the loss=36091.10253429491\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=21300, the loss=36091.06772471773\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=21400, the loss=36091.03464584365\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=21500, the loss=36091.00321161233\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=21600, the loss=36090.97334023468\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=21700, the loss=36090.94495398187\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=21800, the loss=36090.91797898464\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=21900, the loss=36090.892345042535\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=22000, the loss=36090.86798544232\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=22100, the loss=36090.84483678551\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=22200, the loss=36090.822838824235\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=22300, the loss=36090.80193430519\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=22400, the loss=36090.78206882134\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=22500, the loss=36090.7631906708\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=22600, the loss=36090.745250722764\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=22700, the loss=36090.72820229002\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=22800, the loss=36090.71200100767\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=22900, the loss=36090.696604717996\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=23000, the loss=36090.68197336079\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=23100, the loss=36090.668068869316\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=23200, the loss=36090.65485507126\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=23300, the loss=36090.64229759463\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=23400, the loss=36090.630363778335\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=23500, the loss=36090.61902258711\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=23600, the loss=36090.608244530784\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=23700, the loss=36090.59800158737\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=23800, the loss=36090.5882671301\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=23900, the loss=36090.57901585806\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=24000, the loss=36090.57022373018\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=24100, the loss=36090.56186790257\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=24200, the loss=36090.55392666899\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=24300, the loss=36090.54637940415\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=24400, the loss=36090.539206509995\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=24500, the loss=36090.5323893645\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=24600, the loss=36090.5259102731\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=24700, the loss=36090.519752422515\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=24800, the loss=36090.51389983686\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=24900, the loss=36090.50833733589\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=25000, the loss=36090.503050495456\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=25100, the loss=36090.498025609806\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=25200, the loss=36090.49324965575\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=25300, the loss=36090.48871025872\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=25400, the loss=36090.48439566042\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=25500, the loss=36090.48029468813\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=25600, the loss=36090.4763967255\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=25700, the loss=36090.47269168488\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=25800, the loss=36090.4691699809\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=25900, the loss=36090.465822505495\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=26000, the loss=36090.46264060412\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=26100, the loss=36090.4596160531\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=26200, the loss=36090.45674103818\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=26300, the loss=36090.45400813414\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=26400, the loss=36090.45141028538\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=26500, the loss=36090.44894078747\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=26600, the loss=36090.446593269764\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=26700, the loss=36090.44436167858\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=26800, the loss=36090.442240261575\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=26900, the loss=36090.44022355262\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=27000, the loss=36090.43830635758\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=27100, the loss=36090.43648374075\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=27200, the loss=36090.434751011955\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=27300, the loss=36090.43310371436\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=27400, the loss=36090.43153761279\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=27500, the loss=36090.43004868273\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=27600, the loss=36090.42863309978\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=27700, the loss=36090.42728722972\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=27800, the loss=36090.426007619026\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=27900, the loss=36090.42479098588\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=28000, the loss=36090.42363421158\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=28100, the loss=36090.422534332414\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=28200, the loss=36090.42148853201\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=28300, the loss=36090.420494133905\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=28400, the loss=36090.41954859464\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=28500, the loss=36090.4186494971\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=28600, the loss=36090.41779454423\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=28700, the loss=36090.41698155309\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=28800, the loss=36090.4162084491\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=28900, the loss=36090.41547326074\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=29000, the loss=36090.41477411433\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=29100, the loss=36090.414109229205\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=29200, the loss=36090.413476913076\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=29300, the loss=36090.41287555765\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=29400, the loss=36090.4123036344\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=29500, the loss=36090.41175969068\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=29600, the loss=36090.41124234582\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=29700, the loss=36090.41075028771\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=29800, the loss=36090.410282269215\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=29900, the loss=36090.4098371051\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=30000, the loss=36090.40941366884\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=30100, the loss=36090.409010889765\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=30200, the loss=36090.40862775022\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=30300, the loss=36090.40826328301\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=30400, the loss=36090.40791656884\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=30500, the loss=36090.407586733905\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=30600, the loss=36090.40727294772\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=30700, the loss=36090.406974420875\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=30800, the loss=36090.40669040305\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=30900, the loss=36090.40642018105\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=31000, the loss=36090.406163076965\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=31100, the loss=36090.40591844642\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=31200, the loss=36090.40568567691\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=31300, the loss=36090.4054641862\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=31400, the loss=36090.40525342088\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=31500, the loss=36090.40505285484\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=31600, the loss=36090.40486198798\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=31700, the loss=36090.40468034494\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=31800, the loss=36090.40450747378\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=31900, the loss=36090.404342944916\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=32000, the loss=36090.40418634997\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=32100, the loss=36090.4040373007\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=32200, the loss=36090.40389542804\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=32300, the loss=36090.4037603811\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=32400, the loss=36090.40363182634\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=32500, the loss=36090.40350944661\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=32600, the loss=36090.403392940425\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=32700, the loss=36090.40328202116\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=32800, the loss=36090.4031764163\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=32900, the loss=36090.403075866736\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=33000, the loss=36090.40298012615\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=33100, the loss=36090.40288896031\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=33200, the loss=36090.40280214654\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=33300, the loss=36090.4027194731\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=33400, the loss=36090.40264073866\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=33500, the loss=36090.4025657518\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=33600, the loss=36090.402494330505\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=33700, the loss=36090.402426301676\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=33800, the loss=36090.40236150075\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=33900, the loss=36090.40229977123\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=34000, the loss=36090.402240964286\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=34100, the loss=36090.4021849384\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=34200, the loss=36090.40213155901\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=34300, the loss=36090.402080698106\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=34400, the loss=36090.402032234\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=34500, the loss=36090.401986050936\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=34600, the loss=36090.40194203883\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=34700, the loss=36090.40190009299\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=34800, the loss=36090.401860113845\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=34900, the loss=36090.4018220067\n"
     ]
    }
   ],
   "source": [
    "weights_1 = reg_logistic_regression(y_1, matrix_std_tx_1, lambdas[0], gammas[0], max_iter)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gamma:  0.00333333333333\n",
      "Current iteration=0, the loss=50282.97591936011\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chenfs/anaconda/lib/python3.5/site-packages/ipykernel/__main__.py:18: RuntimeWarning: overflow encountered in exp\n",
      "/Users/chenfs/anaconda/lib/python3.5/site-packages/ipykernel/__main__.py:7: RuntimeWarning: overflow encountered in exp\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gamma:  0.000130061474369\n",
      "Current iteration=100, the loss=1299813.3253270006\n",
      "Gamma:  5.78050997194e-05\n",
      "Current iteration=200, the loss=821260.9207168942\n",
      "Gamma:  5.78050997194e-05\n",
      "Current iteration=300, the loss=616401.3876636017\n",
      "Gamma:  5.78050997194e-05\n",
      "Current iteration=400, the loss=420942.4316300268\n",
      "Gamma:  3.85367331463e-05\n",
      "Current iteration=500, the loss=286249.6861910225\n",
      "Gamma:  1.71274369539e-05\n",
      "Current iteration=600, the loss=206946.38208725894\n",
      "Gamma:  1.71274369539e-05\n",
      "Current iteration=700, the loss=157597.89235992503\n",
      "Gamma:  1.71274369539e-05\n",
      "Current iteration=800, the loss=113620.56760026883\n",
      "Gamma:  7.61219420174e-06\n",
      "Current iteration=900, the loss=85422.16557873185\n",
      "Gamma:  7.61219420174e-06\n",
      "Current iteration=1000, the loss=73176.41080177324\n",
      "Gamma:  7.61219420174e-06\n",
      "Current iteration=1100, the loss=63774.83872950742\n",
      "Gamma:  7.61219420174e-06\n",
      "Current iteration=1200, the loss=56729.12816050771\n",
      "Gamma:  7.61219420174e-06\n",
      "Current iteration=1300, the loss=51293.49741852834\n",
      "Gamma:  3.38319742299e-06\n",
      "Current iteration=1400, the loss=49109.23301273872\n",
      "Gamma:  3.38319742299e-06\n",
      "Current iteration=1500, the loss=47248.57507621507\n",
      "Gamma:  3.38319742299e-06\n",
      "Current iteration=1600, the loss=45569.537310308166\n",
      "Gamma:  3.38319742299e-06\n",
      "Current iteration=1700, the loss=44062.022920128664\n",
      "Gamma:  3.38319742299e-06\n",
      "Current iteration=1800, the loss=42716.388307849375\n",
      "Gamma:  3.38319742299e-06\n",
      "Current iteration=1900, the loss=41521.78043637182\n",
      "Gamma:  3.38319742299e-06\n",
      "Current iteration=2000, the loss=40465.933006595675\n",
      "Gamma:  3.38319742299e-06\n",
      "Current iteration=2100, the loss=39553.07332390531\n",
      "Gamma:  3.38319742299e-06\n",
      "Current iteration=2200, the loss=38789.68198913543\n",
      "Gamma:  3.38319742299e-06\n",
      "Current iteration=2300, the loss=38118.18550464419\n",
      "Gamma:  3.38319742299e-06\n",
      "Current iteration=2400, the loss=37524.45676838455\n",
      "Gamma:  3.38319742299e-06\n",
      "Current iteration=2500, the loss=36998.342307944564\n",
      "Gamma:  3.38319742299e-06\n",
      "Current iteration=2600, the loss=36531.17975667957\n",
      "Gamma:  3.38319742299e-06\n",
      "Current iteration=2700, the loss=36115.37479329673\n",
      "Gamma:  3.38319742299e-06\n",
      "Current iteration=2800, the loss=35744.308262246326\n",
      "Gamma:  3.38319742299e-06\n",
      "Current iteration=2900, the loss=35412.25693319279\n",
      "Gamma:  3.38319742299e-06\n",
      "Current iteration=3000, the loss=35114.30082721614\n",
      "Gamma:  3.38319742299e-06\n",
      "Current iteration=3100, the loss=34846.22409284855\n",
      "Gamma:  3.38319742299e-06\n",
      "Current iteration=3200, the loss=34604.41798340645\n",
      "Gamma:  3.38319742299e-06\n",
      "Current iteration=3300, the loss=34385.79146859835\n",
      "Gamma:  3.38319742299e-06\n",
      "Current iteration=3400, the loss=34187.69205504357\n",
      "Gamma:  3.38319742299e-06\n",
      "Current iteration=3500, the loss=34007.83738330592\n",
      "Gamma:  3.38319742299e-06\n",
      "Current iteration=3600, the loss=33844.25706221113\n",
      "Gamma:  3.38319742299e-06\n",
      "Current iteration=3700, the loss=33695.243701486404\n",
      "Gamma:  3.38319742299e-06\n",
      "Current iteration=3800, the loss=33559.311940792715\n",
      "Gamma:  3.38319742299e-06\n",
      "Current iteration=3900, the loss=33435.16428056208\n",
      "Gamma:  3.38319742299e-06\n",
      "Current iteration=4000, the loss=33321.66261386397\n",
      "Gamma:  3.38319742299e-06\n",
      "Current iteration=4100, the loss=33217.80449360042\n",
      "Gamma:  3.38319742299e-06\n",
      "Current iteration=4200, the loss=33122.70331585436\n",
      "Gamma:  3.38319742299e-06\n",
      "Current iteration=4300, the loss=33035.57173891668\n",
      "Gamma:  3.38319742299e-06\n",
      "Current iteration=4400, the loss=32955.70777876487\n",
      "Gamma:  3.38319742299e-06\n",
      "Current iteration=4500, the loss=32882.48312303732\n",
      "Gamma:  3.38319742299e-06\n",
      "Current iteration=4600, the loss=32815.333288212154\n",
      "Gamma:  3.38319742299e-06\n",
      "Current iteration=4700, the loss=32753.74931160785\n",
      "Gamma:  3.38319742299e-06\n",
      "Current iteration=4800, the loss=32697.270723975686\n",
      "Gamma:  3.38319742299e-06\n",
      "Current iteration=4900, the loss=32645.479592433574\n",
      "Gamma:  3.38319742299e-06\n",
      "Current iteration=5000, the loss=32597.99552992245\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=5100, the loss=32558.096121580053\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=5200, the loss=32539.80280690363\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=5300, the loss=32522.20722038179\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=5400, the loss=32505.283968516564\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=5500, the loss=32489.00860850243\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=5600, the loss=32473.357593047203\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=5700, the loss=32458.30822964253\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=5800, the loss=32443.838642438717\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=5900, the loss=32429.927736458663\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=6000, the loss=32416.555163962796\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=6100, the loss=32403.701292802274\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=6200, the loss=32391.347176613323\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=6300, the loss=32379.474526717473\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=6400, the loss=32368.065685601854\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=6500, the loss=32357.103601858915\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=6600, the loss=32346.57180646794\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=6700, the loss=32336.454390299677\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=6800, the loss=32326.7359827201\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=6900, the loss=32317.40173115985\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=7000, the loss=32308.437281499504\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=7100, the loss=32299.828759095763\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=7200, the loss=32291.562750235185\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=7300, the loss=32283.626283741913\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=7400, the loss=32276.006812369425\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=7500, the loss=32268.69219344974\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=7600, the loss=32261.670668008035\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=7700, the loss=32254.930837092423\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=7800, the loss=32248.4616332478\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=7900, the loss=32242.25228355246\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=8000, the loss=32236.29225778589\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=8100, the loss=32230.571189889208\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=8200, the loss=32225.078751091416\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=8300, the loss=32219.804438879844\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=8400, the loss=32214.737243959236\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=8500, the loss=32209.865247387206\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=8600, the loss=32205.175523262467\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=8700, the loss=32200.654989244253\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=8800, the loss=32196.29197912804\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=8900, the loss=32192.077108997804\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=9000, the loss=32188.00296224044\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=9100, the loss=32184.063421081733\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=9200, the loss=32180.25318470564\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=9300, the loss=32176.567492531343\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=9400, the loss=32173.001965014984\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=9500, the loss=32169.55250723191\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=9600, the loss=32166.215247961543\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=9700, the loss=32162.986499932584\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=9800, the loss=32159.862733098376\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=9900, the loss=32156.840556112293\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=10000, the loss=32153.916703049937\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=10100, the loss=32151.088023537668\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=10200, the loss=32148.351475125302\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=10300, the loss=32145.70411716158\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=10400, the loss=32143.143105695883\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=10500, the loss=32140.665689097492\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=10600, the loss=32138.269204190346\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=10700, the loss=32135.951072769996\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=10800, the loss=32133.708798413423\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=10900, the loss=32131.539963520787\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=11000, the loss=32129.442226547137\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=11100, the loss=32127.413319393676\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=11200, the loss=32125.451044936963\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=11300, the loss=32123.553274679387\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=11400, the loss=32121.717946508114\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=11500, the loss=32119.943062552436\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=11600, the loss=32118.226687131068\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=11700, the loss=32116.566944782444\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=11800, the loss=32114.962018372127\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=11900, the loss=32113.410147271992\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=12000, the loss=32111.90962560679\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=12100, the loss=32110.458800563913\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=12200, the loss=32109.056070762796\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=12300, the loss=32107.699884680696\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=12400, the loss=32106.388739131788\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=12500, the loss=32105.121177796977\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=12600, the loss=32103.895789801925\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=12700, the loss=32102.711208340894\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=12800, the loss=32101.56610934455\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=12900, the loss=32100.45921018963\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=13000, the loss=32099.3892684487\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=13100, the loss=32098.355080678397\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=13200, the loss=32097.35548124469\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=13300, the loss=32096.389341183494\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=13400, the loss=32095.455567095614\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=13500, the loss=32094.553100074478\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=13600, the loss=32093.680914665714\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=13700, the loss=32092.8380178573\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=13800, the loss=32092.023448099375\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=13900, the loss=32091.23627435264\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=14000, the loss=32090.47559516449\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=14100, the loss=32089.740537771937\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=14200, the loss=32089.03025723051\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=14300, the loss=32088.34393556842\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=14400, the loss=32087.680780965038\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=14500, the loss=32087.04002695322\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=14600, the loss=32086.42093164448\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=14700, the loss=32085.82277697667\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=14800, the loss=32085.24486798329\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=14900, the loss=32084.686532083862\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=15000, the loss=32084.14711839492\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=15100, the loss=32083.62599706079\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=15200, the loss=32083.122558603856\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=15300, the loss=32082.636213293656\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=15400, the loss=32082.166390534236\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=15500, the loss=32081.71253826947\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=15600, the loss=32081.274122405597\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=15700, the loss=32080.850626250693\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=15800, the loss=32080.44154997055\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=15900, the loss=32080.04641006056\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=16000, the loss=32079.6647388331\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=16100, the loss=32079.29608392005\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=16200, the loss=32078.940007790025\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=16300, the loss=32078.596087279933\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=16400, the loss=32078.26391314046\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=16500, the loss=32077.943089595054\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=16600, the loss=32077.633233912158\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=16700, the loss=32077.333975990205\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=16800, the loss=32077.044957955102\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=16900, the loss=32076.76583376984\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=17000, the loss=32076.4962688559\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=17100, the loss=32076.2359397261\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=17200, the loss=32075.984533628613\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=17300, the loss=32075.741748201803\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=17400, the loss=32075.507291139613\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=17500, the loss=32075.280879867216\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=17600, the loss=32075.062241226515\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=17700, the loss=32074.85111117146\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=17800, the loss=32074.64723447266\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=17900, the loss=32074.45036443114\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=18000, the loss=32074.26026260101\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=18100, the loss=32074.076698520716\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=18200, the loss=32073.899449452638\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=18300, the loss=32073.728300130893\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=18400, the loss=32073.56304251693\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=18500, the loss=32073.403475562856\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=18600, the loss=32073.2494049822\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=18700, the loss=32073.100643027832\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=18800, the loss=32072.95700827698\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=18900, the loss=32072.81832542297\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=19000, the loss=32072.684425073632\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=19100, the loss=32072.555143556052\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=19200, the loss=32072.43032272759\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=19300, the loss=32072.30980979287\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=19400, the loss=32072.193457126697\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=19500, the loss=32072.081122102543\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=19600, the loss=32071.972666926602\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=19700, the loss=32071.867958477185\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=19800, the loss=32071.766868149236\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=19900, the loss=32071.6692717039\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=20000, the loss=32071.575049122934\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=20100, the loss=32071.48408446784\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=20200, the loss=32071.396265743548\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=20300, the loss=32071.31148476656\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=20400, the loss=32071.22963703732\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=20500, the loss=32071.150621616824\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=20600, the loss=32071.074341007166\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=20700, the loss=32071.00070103606\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=20800, the loss=32070.929610745094\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=20900, the loss=32070.860982281672\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=21000, the loss=32070.794730794492\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=21100, the loss=32070.73077433245\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=21200, the loss=32070.669033746875\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=21300, the loss=32070.609432597015\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=21400, the loss=32070.551897058565\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=21500, the loss=32070.496355835297\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=21600, the loss=32070.442740073617\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=21700, the loss=32070.390983279805\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=21800, the loss=32070.341021240227\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=21900, the loss=32070.292791944008\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=22000, the loss=32070.24623550835\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=22100, the loss=32070.201294106366\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=22200, the loss=32070.157911897197\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=22300, the loss=32070.116034958617\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=22400, the loss=32070.075611221797\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=22500, the loss=32070.036590408214\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=22600, the loss=32069.99892396875\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=22700, the loss=32069.962565024794\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=22800, the loss=32069.927468311318\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=22900, the loss=32069.893590121857\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=23000, the loss=32069.860888255338\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=23100, the loss=32069.82932196469\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=23200, the loss=32069.79885190724\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=23300, the loss=32069.769440096632\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=23400, the loss=32069.741049856584\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=23500, the loss=32069.713645775973\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=23600, the loss=32069.687193665595\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=23700, the loss=32069.66166051634\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=23800, the loss=32069.637014458753\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=23900, the loss=32069.61322472399\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=24000, the loss=32069.590261606092\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=24100, the loss=32069.56809642551\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=24200, the loss=32069.54670149395\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=24300, the loss=32069.52605008027\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=24400, the loss=32069.506116377663\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=24500, the loss=32069.486875471892\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=24600, the loss=32069.468303310612\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=24700, the loss=32069.450376673765\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=24800, the loss=32069.433073144904\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=24900, the loss=32069.416371083615\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=25000, the loss=32069.400249598733\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=25100, the loss=32069.384688522623\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=25200, the loss=32069.3696683862\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=25300, the loss=32069.355170394894\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=25400, the loss=32069.34117640542\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=25500, the loss=32069.327668903297\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=25600, the loss=32069.31463098116\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=25700, the loss=32069.30204631784\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=25800, the loss=32069.289899158135\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=25900, the loss=32069.278174293264\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=26000, the loss=32069.266857042025\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=26100, the loss=32069.255933232533\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=26200, the loss=32069.245389184674\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=26300, the loss=32069.235211693085\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=26400, the loss=32069.225388010753\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=26500, the loss=32069.21590583317\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=26600, the loss=32069.206753283037\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=26700, the loss=32069.197918895457\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=26800, the loss=32069.18939160373\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=26900, the loss=32069.181160725515\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=27000, the loss=32069.173215949548\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=27100, the loss=32069.165547322806\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=27200, the loss=32069.15814523809\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=27300, the loss=32069.15100042203\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=27400, the loss=32069.144103923587\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=27500, the loss=32069.13744710278\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=27600, the loss=32069.13102162002\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=27700, the loss=32069.124819425608\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=27800, the loss=32069.11883274973\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=27900, the loss=32069.11305409274\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=28000, the loss=32069.1074762158\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=28100, the loss=32069.102092131823\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=28200, the loss=32069.096895096725\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=28300, the loss=32069.09187860103\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=28400, the loss=32069.087036361714\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=28500, the loss=32069.082362314308\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=28600, the loss=32069.077850605372\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=28700, the loss=32069.07349558512\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=28800, the loss=32069.069291800373\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=28900, the loss=32069.065233987712\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=29000, the loss=32069.061317066913\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=29100, the loss=32069.057536134555\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=29200, the loss=32069.053886457896\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=29300, the loss=32069.050363468934\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=29400, the loss=32069.046962758686\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=29500, the loss=32069.04368007164\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=29600, the loss=32069.040511300467\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=29700, the loss=32069.03745248081\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=29800, the loss=32069.034499786365\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=29900, the loss=32069.031649524048\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=30000, the loss=32069.028898129352\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=30100, the loss=32069.026242161945\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=30200, the loss=32069.02367830124\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=30300, the loss=32069.021203342345\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=30400, the loss=32069.018814191924\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=30500, the loss=32069.016507864424\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=30600, the loss=32069.01428147825\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=30700, the loss=32069.01213225217\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=30800, the loss=32069.010057501822\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=30900, the loss=32069.008054636346\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=31000, the loss=32069.00612115512\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=31100, the loss=32069.00425464465\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=31200, the loss=32069.00245277546\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=31300, the loss=32069.00071329927\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=31400, the loss=32068.999034046068\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=31500, the loss=32068.99741292148\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=31600, the loss=32068.99584790405\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=31700, the loss=32068.994337042765\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=31800, the loss=32068.99287845455\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=31900, the loss=32068.99147032194\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=32000, the loss=32068.990110890787\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=32100, the loss=32068.98879846804\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=32200, the loss=32068.987531419625\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=32300, the loss=32068.986308168398\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=32400, the loss=32068.98512719214\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=32500, the loss=32068.98398702166\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=32600, the loss=32068.982886238955\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=32700, the loss=32068.981823475406\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=32800, the loss=32068.980797410077\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=32900, the loss=32068.979806768017\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=33000, the loss=32068.978850318716\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=33100, the loss=32068.977926874508\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=33200, the loss=32068.977035289096\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=33300, the loss=32068.97617445608\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=33400, the loss=32068.975343307644\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=33500, the loss=32068.974540813128\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=33600, the loss=32068.97376597777\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=33700, the loss=32068.97301784146\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=33800, the loss=32068.972295477506\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=33900, the loss=32068.971597991494\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=34000, the loss=32068.970924520178\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=34100, the loss=32068.970274230323\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=34200, the loss=32068.969646317728\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=34300, the loss=32068.969040006217\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=34400, the loss=32068.968454546597\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=34500, the loss=32068.967889215775\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=34600, the loss=32068.967343315828\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=34700, the loss=32068.96681617314\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=34800, the loss=32068.96630713751\n",
      "Gamma:  1.50364329911e-06\n",
      "Current iteration=34900, the loss=32068.9658155814\n"
     ]
    }
   ],
   "source": [
    "weights_23 = reg_logistic_regression(y_23, matrix_std_tx_23, lambdas[0], gammas[0], max_iter)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0  Size:  99913 \tPerformance:  0.8369781710087776\n",
      "1  Size:  77544 \tPerformance:  0.7879268544310327\n",
      "23 Size:  72543 \tPerformance:  0.8077002605351309\n"
     ]
    }
   ],
   "source": [
    "print(\"0  Size: \", len(y_0), \"\\tPerformance: \", performance(weights_0, y_0, matrix_std_tx_0))\n",
    "print(\"1  Size: \", len(y_1), \"\\tPerformance: \", performance(weights_1, y_1, matrix_std_tx_1))\n",
    "print(\"23 Size: \", len(y_23), \"\\tPerformance: \", performance(weights_23, y_23, matrix_std_tx_23))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate predictions and save ouput in csv format for submission:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "DATA_TEST_PATH = '../../data/test.csv' \n",
    "_, tX_test, ids_test = load_csv_data(DATA_TEST_PATH)\n",
    "i_0_test, i_1_test, i_23_test = split_dataset_wrt22(tX_test)\n",
    "\n",
    "tx_0_test = tX_test[i_0_test]\n",
    "tx_1_test = tX_test[i_1_test]\n",
    "tx_23_test = tX_test[i_23_test]\n",
    "\n",
    "std_tx_0_test = standardize_0(tx_0_test)\n",
    "std_tx_1_test = standardize_1(tx_1_test)\n",
    "std_tx_23_test = standardize_23(tx_23_test)\n",
    "\n",
    "ids_0_test = ids_test[i_0_test]\n",
    "ids_1_test = ids_test[i_1_test]\n",
    "ids_23_test = ids_test[i_23_test]\n",
    "\n",
    "output_path = '../../data/output0.csv'\n",
    "y_pred_0 = predict_labels(weights_0, build_poly(std_tx_0_test, degree))\n",
    "create_csv_submission(ids_0_test, y_pred_0, output_path)\n",
    "\n",
    "\n",
    "output_path = '../../data/output1.csv'\n",
    "y_pred_1 = predict_labels(weights_1, build_poly(std_tx_1_test, degree))\n",
    "create_csv_submission(ids_1_test, y_pred_1, output_path)\n",
    "\n",
    "\n",
    "output_path = '../../data/output23.csv'\n",
    "y_pred_23 = predict_labels(weights_23, build_poly(std_tx_23_test, degree))\n",
    "create_csv_submission(ids_23_test, y_pred_23, output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "OUTPUT_PATH = '../../data/output.csv' # TODO: fill in desired name of output file for submission\n",
    "y_pred = predict_labels(weights, build_poly(standardize(tX_test), degree))\n",
    "create_csv_submission(ids_test, y_pred, OUTPUT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
